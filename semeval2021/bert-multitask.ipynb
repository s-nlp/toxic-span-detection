{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* basic roberta ft: 0.6589791487657798 (thr 0.3)\n",
    "* basic roberta ft (head first): 0.6768011808573329 (thr 0.42)\n",
    "* fine tune roberta on weird clf, then only head on spans, then whole: 0.6853127403287083 (thr 0.32)\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForTokenClassification\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta-base'  #roberta-base\n",
    "saved_name = 'models/roberta_single'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([1] * inputs[\"input_ids\"].size(1)).unsqueeze(0)  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=tensor(3.5819, grad_fn=<NllLossBackward>), logits=tensor([[[ 4.4480, -3.6460],\n",
       "         [ 3.0508, -2.3449],\n",
       "         [ 3.4750, -2.5897],\n",
       "         [ 1.7331, -0.5390],\n",
       "         [-0.1971,  1.1977],\n",
       "         [-0.9398,  1.8006],\n",
       "         [-2.1010,  2.9615],\n",
       "         [ 3.6444, -2.7873]]], grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create labels for tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((690, 2), (7939, 2), (2000, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial = pd.read_csv(path + 'tsd_trial.csv')\n",
    "train = pd.read_csv(path + 'tsd_train.csv')\n",
    "final_test = pd.read_csv(path + 'tsd_test.csv')\n",
    "\n",
    "train['spans'] = train.spans.apply(literal_eval)\n",
    "trial['spans'] = trial.spans.apply(literal_eval)\n",
    "trial.shape, train.shape, final_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(set(trial.text).intersection(set(train.text))))\n",
    "print(len(set(final_test.text).intersection(set(train.text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06109081748331024\n",
      "0.06231884057971015\n"
     ]
    }
   ],
   "source": [
    "print((train.spans.apply(len) == 0).mean())\n",
    "print((trial.spans.apply(len) == 0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Because he's a <b>moron</b> and a <b>bigot</b>. It's not any more complicated than that."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Because he's a <b>moron</b> and a <b>bigot</b>. It's not any more complicated than that."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spans_utils\n",
    "from importlib import reload\n",
    "reload(spans_utils)\n",
    "from spans_utils import display_spans, spans2labels, labels2spans\n",
    "\n",
    "display_spans(trial.spans[0], trial.text[0])\n",
    "display_spans(trial.spans[0], trial.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df09d9c607924523b0abdf0e1294250e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=690.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Thanks+a+lot+douchebag.+You're+the+reason+the+Portland+buttfuckers+are+getting+pushed+out+and+moving+down+south+to+the+Wilsonville+area+and+beyond.+Because+of+you+all+the+stupid+fucking+assholes+that+made+Portland+a+place+full+of+stupid+fucking+assholes+are+going+t<b>o+try+to+turn+Salem+into+some+kind+of+new-stupid-fucking-asshole-topia.+This+will+drive+up+the+price+of+housing+and+displace++people+down+here.+Having+lived+in+San+Clemente+for+a+time+myself+I+think+you're+dumb+as+shit+to+have+moved+up+here.+The+fuck</b>s+wrong+with+you+outsider.+I+hope+bums+shit+in+your+yard+everyday+and+never+let+you+walk+by+without+demanding+spare++change."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Thanks+a+lot+douchebag.+You're+the+reason+the+Portland+buttfuckers+are+getting+pushed+out+and+moving+down+south+to+the+Wilsonville+area+and+beyond.+Because+of+you+all+the+stupid+fucking+assholes+that+made+Portland+a+place+full+of+stupid+fucking+assholes+are+going+<b>to+try+to+turn+Salem+into+some+kind+of+new-stupid-fucking-asshole-topia.+This+will+drive+up+the+price+of+housing+and+displace++people+down+here.+Having+lived+in+San+Clemente+for+a+time+myself+I+think+you're+dumb+as+shit+to+have+moved+up+here.+The+fucks</b>+wrong+with+you+outsider.+I+hope+bums+shit+in+your+yard+everyday+and+never+let+you+walk+by+without+demanding+spare++change."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Clinton+signed+that+Joint+Fundraising+agreement+in+August+2015.+It+was+reported+on+then;+why+is+it+an+issue+now?+And+there+wasn't+anything+illegal+about+it,+so+it's+not+a+crime.+Oh,+and+the+Sanders+campaign+signed+the+same+agreement+with+the+DNC+in+Oct+2015.+Somehow+that's+overlooked.+Though+granted,+Sanders+late<b>r+cho</b>se+to+raise+money+on+his+own,+not+thru+the+Fund.+No,+Clinton+hasn't+gotten+away+with+\"so+many+crimes,\"+because+she+hasn't+committed+them.+\"It+boggles+the+mind+that+after+all+these+revelations,+people+would+want+a+liar+and+a+crook+to+be+our+president,\"+clearly+refers+to+Trump;+a+liar+and+a+crook."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Clinton+signed+that+Joint+Fundraising+agreement+in+August+2015.+It+was+reported+on+then;+why+is+it+an+issue+now?+And+there+wasn't+anything+illegal+about+it,+so+it's+not+a+crime.+Oh,+and+the+Sanders+campaign+signed+the+same+agreement+with+the+DNC+in+Oct+2015.+Somehow+that's+overlooked.+Though+granted,+Sanders+<b>later+chose</b>+to+raise+money+on+his+own,+not+thru+the+Fund.+No,+Clinton+hasn't+gotten+away+with+\"so+many+crimes,\"+because+she+hasn't+committed+them.+\"It+boggles+the+mind+that+after+all+these+revelations,+people+would+want+a+liar+and+a+crook+to+be+our+president,\"+clearly+refers+to+Trump;+a+liar+and+a+crook."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Beijing+millionaires+and+wanna+be+Beijing+millionairesses+who+squeeze+money+from+the+rest+of+us!+And+<b>stupid+blind</b>+<b>white</b>+men+with+blinders!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Beijing+millionaires+and+wanna+be+Beijing+millionairesses+who+squeeze+money+from+the+rest+of+us!+And+<b>stupid+blind+white</b>+men+with+blinders!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Section+160+should+also+be+amended+to+include+<b>sexual</b>+<b>acts+with+animals</b>+not+involving+penetration."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Section+160+should+also+be+amended+to+include+<b>sexual+acts+with+animals</b>+not+involving+penetration."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\"Wythe+thinks+Republican+members+of+the+House+should+settle+their+differences+in+caucus+meetings+—+which+are+private.\"++That+statement+should+scare+the+h*ll+out+of++any+voter.++Another+advocate+for+a+state+government+functioning++behind+closed+doors+is+exactly+who+we+do+not+need+in+the+Alaska+Legislature.++The++system+she+apparently+adores+(the+secretive+standing+caucus+system)has+just+about+pounded+this+state+into+the+ground.++Also,+does+she+understand+caucus+membership+is+all+about+crushing+any+minority+view+and+has+little+to+do+with+party+platforms+and+everything+to+do+with+the+quid+pro+quo+of+trading+toady+behavior+for+spending+in+deals+that+are+done+behind+closed+doors?++<b>Stupid+is+as+stupid+does</b>.........."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\"Wythe+thinks+Republican+members+of+the+House+should+settle+their+differences+in+caucus+meetings+—+which+are+private.\"++That+statement+should+scare+the+h*ll+out+of++any+voter.++Another+advocate+for+a+state+government+functioning++behind+closed+doors+is+exactly+who+we+do+not+need+in+the+Alaska+Legislature.++The++system+she+apparently+adores+(the+secretive+standing+caucus+system)has+just+about+pounded+this+state+into+the+ground.++Also,+does+she+understand+caucus+membership+is+all+about+crushing+any+minority+view+and+has+little+to+do+with+party+platforms+and+everything+to+do+with+the+quid+pro+quo+of+trading+toady+behavior+for+spending+in+deals+that+are+done+behind+closed+doors?++S<b>tupid+is+as+stupid+does..</b>........"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "CBC+doesn't+worry+about+competition+that's+why+the+are+so+<b>mediocr</b>e."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "CBC+doesn't+worry+about+competition+that's+why+the+are+so+<b>mediocre</b>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Muslims+wou<b>ld+beg</b>+to+disagree"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Muslims+<b>would+beg</b>+to+disagree"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Go+awa<b>y.++Bigo</b>ted+<b>jerk</b>!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Go+<b>away.++Bigoted+jerk</b>!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "This+is+<b>either</b>+<b>dumb</b>+or+<b>stupid</b>.\n",
       "\n",
       "The+issue+most+folks+have+is+not+with+the+size+of+the+PFD,+but+the+fact+that+we+are+giving+up+any+of+our+PFD+while+we+throw+money+at+wealthy+oil+companies+in+the+form+of+tax+deductions.\n",
       "\n",
       "If+the+Senate+will+pass+the+Wilson-Seaton+oil+tax+bill,+am+sure+the+house+will+get+behind+the+senate+PFD+bill.+It+is+called+compromise."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "This+is+<b>either+dumb</b>+or+<b>stupid</b>.\n",
       "\n",
       "The+issue+most+folks+have+is+not+with+the+size+of+the+PFD,+but+the+fact+that+we+are+giving+up+any+of+our+PFD+while+we+throw+money+at+wealthy+oil+companies+in+the+form+of+tax+deductions.\n",
       "\n",
       "If+the+Senate+will+pass+the+Wilson-Seaton+oil+tax+bill,+am+sure+the+house+will+get+behind+the+senate+PFD+bill.+It+is+called+compromise."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Y<b>ou+hit+th</b>e+nail+on+the+head.++Republicans+should+be+denouncing+<b>idiot</b>+Trump+en+masse.++Instead+they+are+defending+him+(at+worst)+or+looking+the+other+way+and+humming."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>You+hit+the</b>+nail+on+the+head.++Republicans+should+be+denouncing+<b>idiot</b>+Trump+en+masse.++Instead+they+are+defending+him+(at+worst)+or+looking+the+other+way+and+humming."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.politicususa.com/2016/09/02/trump-craven-maniacal-david-bossie-clinton-character-assassin.html\n",
      "\n",
      "New+Trump+Hire+Proves+Hillary+Clinton’s+Vast+Right+Wing+Conspiracy+is+Real\n",
      "Trump's+new+Deputy+Campaign+Manager,+Citizens+United+President+David+Bossie,+\"has+devoted+his+career+ever+since+to+trying+to+tear+down+Hillary+Clinton\"\n",
      "\n",
      "There+is+no+disguising+the+truth:+Bossie+will+be+assisting+Kellyanne+Conway+as+an+anti-Clinton+character+assassin,+and+Hillary+for+America+Chair+John+Podesta+released+the+following+statement:\n",
      "\n",
      "“David+Bossie+is+so+craven+and+maniacal+that+in+the+heyday+of+the+overreaching,+Gingrich-era+Congress,+the+top+Whitewater+conspiracy+theorist+in+the+House+had+to+fire+him+for+doctoring+evidence.+He+has+devoted+his+career+ever+since+to+trying+to+tear+down+Hillary+Clinton\n",
      "\n",
      "Another+liar+just+like+Trump.+I+think+only+<b>idiots</b>..............\n",
      "http://www.politicususa.com/2016/09/02/trump-craven-maniacal-david-bossie-clinton-character-assassin.html\n",
      "\n",
      "New+Trump+Hire+Proves+Hillary+Clinton’s+Vast+Right+Wing+Conspiracy+is+Real\n",
      "Trump's+new+Deputy+Campaign+Manager,+Citizens+United+President+David+Bossie,+\"has+devoted+his+career+ever+since+to+trying+to+tear+down+Hillary+Clinton\"\n",
      "\n",
      "There+is+no+disguising+the+truth:+Bossie+will+be+assisting+Kellyanne+Conway+as+an+anti-Clinton+character+assassin,+and+Hillary+for+America+Chair+John+Podesta+released+the+following+statement:\n",
      "\n",
      "“David+Bossie+is+so+craven+and+maniacal+that+in+the+heyday+of+the+overreaching,+Gingrich-era+Congress,+the+top+Whitewater+conspiracy+theorist+in+the+House+had+to+fire+him+for+doctoring+evidence.+He+has+devoted+his+career+ever+since+to+trying+to+tear+down+Hillary+Clinton\n",
      "\n",
      "Another+liar+just+like+Trump.+I+think+only<b>+idiots....</b>..........\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "what+would+you+know+about+sport+or++scienceÉ+stay+in+your+room+doing+what+you+do+best..+wash+your+hands+after,+<b>wanker</b>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "what+would+you+know+about+sport+or++scienceÉ+stay+in+your+room+doing+what+you+do+best..+wash+your+hands+after,+w<b>anker."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Lebrun+James+is+disgusting!+I+used+to+like+him+but+he+stumped+for+HRC+the+biggest+most+corrupt+politician+to+EVER+run+and+called+blacks+SUPER+OREDATORS+and+he+trashes+Trump??+I+quit+watching+NBA+because+of+him+and+the+Seattle+Seahawks+version+of+the+NBA-GS+Warriors!+They're+<b>friggn</b>+<b>racists</b>+and+I'm+DONE!+Only+college+football+for+me+until......let's+unite+and+boycott+the+NFL/NBA!!!+ESPN+is+dying+too+thank+the+lord.....<b>RACIST+LIBTARDS"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Lebrun+James+is+disgusting!+I+used+to+like+him+but+he+stumped+for+HRC+the+biggest+most+corrupt+politician+to+EVER+run+and+called+blacks+SUPER+OREDATORS+and+he+trashes+Trump??+I+quit+watching+NBA+because+of+him+and+the+Seattle+Seahawks+version+of+the+NBA-GS+Warriors!+They're+<b>friggn+racists</b>+and+I'm+DONE!+Only+college+football+for+me+until......let's+unite+and+boycott+the+NFL/NBA!!!+ESPN+is+dying+too+thank+the+lord.....<b>RACIST+LIBTARDS"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Less+than+six+months+into+his+presidency,+it's+clear+that+Trump+is+unfit+for+office.\n",
       "\n",
       "“There+is+a+<b>sucker</b>+born+every+minute\n",
       "Each+time+the+second+hand+sweeps+to+the+top\n",
       "Like+dandelions+up+they+pop,\n",
       "Their+ears+so+big,+their+eyes+so+wide.\n",
       "And+though+I+feed+‘em+bona+fide+baloney\n",
       "With+no+truth+in+it\n",
       "Why+you+can+bet+I’ll+find+some+rube+to+buy+my+corn.\n",
       "‘Cause+there’s+a+sure-as-<b>shooting+sucker</b>+born+a+minute,\n",
       "And+I’m+referrin’+to+the+minute+you+were+born.”"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Less+than+six+months+into+his+presidency,+it's+clear+that+Trump+is+unfit+for+office.\n",
       "\n",
       "“There+is+a+<b>sucker+b</b>orn+every+minute\n",
       "Each+time+the+second+hand+sweeps+to+the+top\n",
       "Like+dandelions+up+they+pop,\n",
       "Their+ears+so+big,+their+eyes+so+wide.\n",
       "And+though+I+feed+‘em+bona+fide+baloney\n",
       "With+no+truth+in+it\n",
       "Why+you+can+bet+I’ll+find+some+rube+to+buy+my+corn.\n",
       "‘Cause+there’s+a+sure-as-<b>shooting+sucker+bo</b>rn+a+minute,\n",
       "And+I’m+referrin’+to+the+minute+you+were+born.”"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You+<b>TRUMPTARDS</b>+are+such+COMPLETE+<b>HYPOCRITES</b>!!!!\n",
       "\n",
       "Every+time+I+post+\"TRUMP+DEATH+SQUADS™,\"+you+completely+FREAK+OUT!!!++\n",
       "\n",
       "But+every+time+Palin+spewed+\"OBAMA+DEATH+SQUADS,\"+you+chanted+it+like+ZOMBIES!!!\n",
       "\n",
       "LMAO!!!+WHAT+FRAUDS!+WHAT+TOTAL+<b>HYPOCRITES</b>!!!!+#<b>LOSERS"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You+<b>TRUMPTARDS</b>+are+such+COMPLETE+<b>HYPOCRITES</b>!!!!\n",
       "\n",
       "Every+time+I+post+\"TRUMP+DEATH+SQUADS™,\"+you+completely+FREAK+OUT!!!++\n",
       "\n",
       "But+every+time+Palin+spewed+\"OBAMA+DEATH+SQUADS,\"+you+chanted+it+like+ZOMBIES!!!\n",
       "\n",
       "LMAO!!!+WHAT+FRAUDS!+WHAT+TO<b>TAL+HYPOCRITES</b>!!!!+#L<b>OSERS"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Fuck+off</b>+<b>mandydewaal</b>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Fuck+off+mandydewaal</b>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Do+what+most+police+departments+are+willing+to+do+when+someone+fails+to+obey+orders:+<b>shoot+him+to+deat</b>h.\n",
       "\n",
       "#<b>crazyasscrackalivesmatter"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Do+what+most+police+departments+are+willing+to+do+when+someone+fails+to+obey+orders:+<b>shoot+him+to+death</b>.\n",
       "\n",
       "#<b>crazyasscrackalivesmatter"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "I+never+t<b>hought</b>+I'd+say+this,+but+I'd+vote+for+Sarah+Palin+over+Dan+Sullivan.+She's+brain+damaged+but+at+least+she+has+a+pair."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "I+never+<b>thought</b>+I'd+say+this,+but+I'd+vote+for+Sarah+Palin+over+Dan+Sullivan.+She's+brain+damaged+but+at+least+she+has+a+pair."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "And+was+the+Oklahoma+bombing,+which+was+our+worst+albeit+I+neglected+to+mention+it+as+killing+168+people,+including+little+children,+also+wasn't+the+work+of+<b>Muslim+terrorist</b>s,+but+of+our+own+homespun+crazies."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "And+was+the+Oklahoma+bombing,+which+was+our+worst+albeit+I+neglected+to+mention+it+as+killing+168+people,+including+little+children,+also+wasn't+the+work+of+<b>Muslim+terrorists</b>,+but+of+our+own+homespun+crazies."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'<b>'The+governme</b>nt+has+already+sold+about+30+per+cent+of+the+shares+in+Hydro+One,+and+plans+to+sell+another+30+per+cent+to+raise+billions+of+dollars+to+pay+down+debt+and+to+fund+public+transit+and+infrastructure+projects.''+\n",
       "Sure.+That's+what+we+want.+Wynne+and+her+<b>incompetent+band+of+crooks</b>+getting+access+to+MORE+money.+They've+already+driven+the+Ontario+economy+over+a+cliff.+Of+course,+they'll+spend+it+on+paying+down+the+debt+(which+is+Everest-sized,+and+costs+1+billion+dollars+each+month+to+service+-+thanks,+Kathy,)+and+funding+infrastructure+and+transit.+Bullfeathers.+That+cash+will+disappear+into+the+same+black+hole+as+all+the+rest.They'll+burn+through+that+money+faster+than+a+wind-driven+grass+fire.+Then,+they'll+come+looking+for+more.+It's+what+they+do,+and+it's+what+they've+done.+A+whole+lot+of+their+pals+will+get+rich+in+the+process,+and+the+rest+of+us+will+be+left+holding+an+empty+bag.+How+anyone+could+believe+one+word+that+these+lying+bunch+of+thieves+says,+is+astounding."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>''The+government</b>+has+already+sold+about+30+per+cent+of+the+shares+in+Hydro+One,+and+plans+to+sell+another+30+per+cent+to+raise+billions+of+dollars+to+pay+down+debt+and+to+fund+public+transit+and+infrastructure+projects.''+\n",
       "Sure.+That's+what+we+want.+Wynne+and+her+<b>incompetent+band+of+crooks</b>+getting+access+to+MORE+money.+They've+already+driven+the+Ontario+economy+over+a+cliff.+Of+course,+they'll+spend+it+on+paying+down+the+debt+(which+is+Everest-sized,+and+costs+1+billion+dollars+each+month+to+service+-+thanks,+Kathy,)+and+funding+infrastructure+and+transit.+Bullfeathers.+That+cash+will+disappear+into+the+same+black+hole+as+all+the+rest.They'll+burn+through+that+money+faster+than+a+wind-driven+grass+fire.+Then,+they'll+come+looking+for+more.+It's+what+they+do,+and+it's+what+they've+done.+A+whole+lot+of+their+pals+will+get+rich+in+the+process,+and+the+rest+of+us+will+be+left+holding+an+empty+bag.+How+anyone+could+believe+one+word+that+these+lying+bunch+of+thieves+says,+is+astounding."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\"Salazar+said+there+was+no+“bad+intent”+on+the+part+of+DiStefano,+George+or+MacIntyre,+but+rather+“mistakes”+were+made.\"\n",
       "\n",
       "Huh!+I+learned+a+long+time+ago+in+the+principle+of+CYA.+When+something+like+this+happened+in+my+area+of+responsibility,+I+was+reaching+out+to+legal+and+HR+to+make+sure+things+were+handled+by+the+book+.+.++no+looking+away.\n",
       "\n",
       "No+bad+intent?+Don't+give+me+that+.+.+.+at+best+it+was+<b>gross+stupidity</b>+coupled+with+a+severe+case+of+willful+blindness."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\"Salazar+said+there+was+no+“bad+intent”+on+the+part+of+DiStefano,+George+or+MacIntyre,+but+rather+“mistakes”+were+made.\"\n",
       "\n",
       "Huh!+I+learned+a+long+time+ago+in+the+principle+of+CYA.+When+something+like+this+happened+in+my+area+of+responsibility,+I+was+reaching+out+to+legal+and+HR+to+make+sure+things+were+handled+by+the+book+.+.++no+looking+away.\n",
       "\n",
       "No+bad+intent?+Don't+give+me+that+.+.+.+at+best+it+was+g<b>ross+stupidity+coupled</b>+with+a+severe+case+of+willful+blindness."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "I+don't+think+<b>the+writer+of+this+piece+of+shit</b>+knows+what+a+\"white+savior+complex\"+is+if+<b>the+jackass</b>+did+know+what+it+meant+he+would+realize+he+used+it+incorrectly,+also+asians+are+pretty+damn+white+so+using+a+<b>pathetic+virtue+signal</b>+like+that+is+doesn't+apply+here,+stop+cowtowing+to+the+Far+Left+<b>you+little+bitch+you're+white</b>+stop+self+hating+just+because+people+have+told+you+to.+If+you+review+a+film+review+<b>the+damn+film+don't+review+society</b>+<b>you+virtue+signaling+cunt</b>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "I+don't+think+<b>the+writer+of+this+piece+of+shit</b>+knows+what+a+\"white+savior+complex\"+is+if+<b>the+jackass</b>+did+know+what+it+meant+he+would+realize+he+used+it+incorrectly,+also+asians+are+pretty+damn+white+so+using+a+<b>pathetic+virtue+signal</b>+like+that+is+doesn't+apply+here,+stop+cowtowing+to+the+Far+Left+<b>you+little+bitch+you're+white</b>+stop+self+hating+just+because+people+have+told+you+to.+If+you+review+a+film+review+<b>the+damn+film+don't+review+society+you+virtue+signaling+cunt</b>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "oh!+wait!+you+already+burned+down+to+a+crisp+in+fort+mac+by+your+own+<b>stupidity</b>,+already.+Stay+the+hell+away+from+BC,+you+<b>f*cking</b>+<b>idiots</b>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "oh!+wait!+you+already+burned+down+to+a+crisp+in+fort+mac+by+your+own+<b>stupidity</b>,+already.+Stay+the+hell+away+from+BC,+you+<b>f*cking+idiots</b>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Trump+makes+it+clear:+Trumpcare+is+all+about+the+tax+cuts+to+the+rich.\n",
       "\n",
       "\n",
       "That+part+he's+crystal+clear+on,+he's+not+confused+or+crazy-sounding.+\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "+++++He's+got+this+part+down+pat—taking+health+care+away+from+millions+is+going+to+be+a+huge+boon+to+the+wealthy,+\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "++++++\"tremendous+savings\"+in+taxes.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "++++++That's+all+this+has+ever+been+about—now+we've+got+it+straight+from+the+<b>ass's+mouth</b>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Trump+makes+it+clear:+Trumpcare+is+all+about+the+tax+cuts+to+the+rich.\n",
       "\n",
       "\n",
       "That+part+he's+crystal+clear+on,+he's+not+confused+or+crazy-sounding.+\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "+++++He's+got+this+part+down+pat—taking+health+care+away+from+millions+is+going+to+be+a+huge+boon+to+the+wealthy,+\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "++++++\"tremendous+savings\"+in+taxes.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "++++++That's+all+this+has+ever+been+about—now+we've+got+it+straight+from+the+<b>ass's+mouth."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Wes+Bascom.++Only+the+weak+and+<b>stupid</b>+<b>choose</b>+to+ingest+substances+that+will+<b>kill+them</b>.+The+weak+and+the+stupid+do+not+get+to+evolve.++They+are+eventually+replaced+by+organisms+not+as+weak+and+not+as+<b>stupid</b>.++It's+nature's+way+of+protecting+the+<b>herd</b>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Wes+Bascom.++Only+the+weak+and+<b>stupid+choose</b>+to+ingest+substances+that+will+<b>kill+them</b>.+The+weak+and+the+stupid+do+not+get+to+evolve.++They+are+eventually+replaced+by+organisms+not+as+weak+and+not+as+<b>stupid</b>.++It's+nature's+way+of+protecting+the+<b>herd</b>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Don’t+be+<b>ridiculous</b>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Don’t+be<b>+ridiculous."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "And+he+is+so+surprisingly+<b>stupid</b>.++Trump+is+both+i<b>gnorant+and+unintelligent</b>.++But+he+continues+to+think+and+act+as+though+he+is+the+smartest+guy+in+the+room."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "And+he+is+so+surprisingly+<b>stupid</b>.++Trump+is+both+<b>ignorant+and+unintelligent</b>.++But+he+continues+to+think+and+act+as+though+he+is+the+smartest+guy+in+the+room."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Lol+really.+No+offence+but+that’s+an+<b>idiotic</b>+statement.+I’m+not+looking+for+likes+from+the+douches+on+the+left/+right+political+side+of+things.+You+can’t+honestly+think+this+though.+Our+government+federally+and+provincially+across+Canada+works+exactly+like+this.+The+more+money+in+the+more+money+spent.+It+is+always,+and+will+always+be+spent+with+little+oversight,+common+sense,+little+efficiency.+That’s+it.+You+see+provinces+with+hst+taxing+more+that+have+massive+debts.+Obviously+more+money+didn’t+make+them+better+money+managers,+quite+the+opposite+actually.+They+post+phony++surpluses+but+debts+are+ever+increasing.+The+problem+is+the+way+money+is+spent.+That’s+it.+Bloated+public+sectors+that’s+sole+obligation+is+to+become+more+bloated.+What+bloats+more+then+more+money.+Don’t+be+naive+to+think+less+equalization+would+magically+turn+these+inept+politicians+to+manage+money+any+better.+That+includes+cons,+libs,+and+dippers.+They+have+zero+sense+of+money+management+and+probably+never+will."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Lol+really.+No+offence+but+that’s+an<b>+idiotic+s</b>tatement.+I’m+not+looking+for+likes+from+the+douches+on+the+left/+right+political+side+of+things.+You+can’t+honestly+think+this+though.+Our+government+federally+and+provincially+across+Canada+works+exactly+like+this.+The+more+money+in+the+more+money+spent.+It+is+always,+and+will+always+be+spent+with+little+oversight,+common+sense,+little+efficiency.+That’s+it.+You+see+provinces+with+hst+taxing+more+that+have+massive+debts.+Obviously+more+money+didn’t+make+them+better+money+managers,+quite+the+opposite+actually.+They+post+phony++surpluses+but+debts+are+ever+increasing.+The+problem+is+the+way+money+is+spent.+That’s+it.+Bloated+public+sectors+that’s+sole+obligation+is+to+become+more+bloated.+What+bloats+more+then+more+money.+Don’t+be+naive+to+think+less+equalization+would+magically+turn+these+inept+politicians+to+manage+money+any+better.+That+includes+cons,+libs,+and+dippers.+They+have+zero+sense+of+money+management+and+probably+never+will."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No+it+doesn’t.++Talking+about+superstition+<b>RP</b>,+you+still+believe+in+Hillary+Clinton.++Sorry,+<b>I+didn’t+mean+to+mock+your+false+god</b>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No+it+doesn’t.++Talking+about+superstit<b>ion+RP,+</b>you+still+believe+in+Hillary+Clinton.++Sorry,+<b>I+didn’t+mean+to+mock+your+false+god</b>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "We+have+people+who+use+the+handle+\"TRUMP+IS+SIMPLY+A+<b>VACUOUS+BUFFOON</b>+And+a+petty+con+man.+And+a+traitor\",+and+THIS+is+considered+acceptable+and+meeting+the+\"civility\"+standard+but+the+following+is+not????\n",
       "\n",
       "\"This+is+another+Trump+game+...+sort+of+like+tweeting+\"COFEFE\".+He+was+playing+with+Comey+so+that+Comey+w<b>asn't+sur</b>e+how+far+he+could+carry+any+lies.+Trump+has+probably+been+enjoying+watching+the+liberal+media+twisting+themselves+in+knots+over+yet+another+innocent+comment+(he+never+said+any+tapes+were+made,+only+that+Comey+better+hope+there+weren't+any)!!+Good+on+ya'+Mr.+President+...+keep+playing+with+these+liberal+media+<b>morons</b>+until+the+world+finally+sees+the+<b>buffoons</b>+they+are!!!\"\n",
       "\n",
       "Really??????????\n",
       "\n",
       "The+\"civility\"+standards+are+a+joke!!!!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "We+have+people+who+use+the+handle+\"TRUMP+IS+SIMPLY+A+<b>VACUOUS+BUFFOON</b>+And+a+petty+con+man.+And+a+traitor\",+and+THIS+is+considered+acceptable+and+meeting+the+\"civility\"+standard+but+the+following+is+not????\n",
       "\n",
       "\"This+is+another+Trump+game+...+sort+of+like+tweeting+\"COFEFE\".+He+was+playing+with+Comey+so+that+Comey+<b>wasn't+sure</b>+how+far+he+could+carry+any+lies.+Trump+has+probably+been+enjoying+watching+the+liberal+media+twisting+themselves+in+knots+over+yet+another+innocent+comment+(he+never+said+any+tapes+were+made,+only+that+Comey+better+hope+there+weren't+any)!!+Good+on+ya'+Mr.+President+...+keep+playing+with+these+liberal+media+<b>morons</b>+until+the+world+finally+sees+the+<b>buffoons</b>+they+are!!!\"\n",
       "\n",
       "Really??????????\n",
       "\n",
       "The+\"civility\"+standards+are+a+joke!!!!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Fool</b>s+and+their+money+are+easily+parted;+<b>but</b>+fools+with+other+people's+money+are+even+easier+to+be+parted!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Fools</b>+and+their+money+are+easily+parted;+<b>but</b>+fools+with+other+people's+money+are+even+easier+to+be+parted!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Instead+of+notifying+the+public+through+the+Star+Advertiser+with+a+days+notice+of+upcoming+hearings,+why+not+include+in+the+<b>ridiculous</b>++news+letter+that+is+part+of+the+monthly+bill.++Wouldn't+that+be+more+Fair?++Nah,+that's++too+easy!+<b>+Idiots</b>!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Instead+of+notifying+the+public+through+the+Star+Advertiser+with+a+days+notice+of+upcoming+hearings,+why+not+include+in+the+<b>ridiculous</b>++news+letter+that+is+part+of+the+monthly+bill.++Wouldn't+that+be+more+Fair?++Nah,+that's++too+easy!++<b>Idiots</b>!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You+are+well+aware+that+90%+of+Catalans+voted+for+Independence.\n",
       "\n",
       "If+you+didn’t+vote,+it’s+your+own+fault+and+no+one+cares+what+you+think+because+you+are+a+<b>coward</b>.\n",
       "\n",
       "Suspect+if+99.9%+had+voted+for+Independence,+you+would+still+be+asking,+“What+about+the+0.1%?”\n",
       "\n",
       "Congrats+to+the+Free+and+Independent+Republic+of+Catalonia"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You+are+well+aware+that+90%+of+Catalans+voted+for+Independence.\n",
       "\n",
       "If+you+didn’t+vote,+it’s+your+own+fault+and+no+one+cares+what+you+think+because+you+are+<b>a+coward.\n",
       "\n",
       "S</b>uspect+if+99.9%+had+voted+for+Independence,+you+would+still+be+asking,+“What+about+the+0.1%?”\n",
       "\n",
       "Congrats+to+the+Free+and+Independent+Republic+of+Catalonia"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Which+of+the+two+is+it+that+you+do+not+know+to+make+such+a+<b>ridiculous</b>+<b>statement</b>?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Which+of+the+two+is+it+that+you+do+not+know+to+make+such+a+<b>ridiculous+statement</b>?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "In+realty,+<b>Jeff+Stober++is++very+cruel,+self+serving,+and+nasty+to+say+the+lea</b>st.+He+just+acts+like+he's+a+good+person+because+it+benefits+his+company's+profit+margin.+If+someone+wrote+the+actual+truth+about+him,+everyone+would+be+appalled."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "In+realty,+<b>Jeff+Stober++is++very+cruel,+self+serving,+and+nasty+to+say+the+least</b>.+He+just+acts+like+he's+a+good+person+because+it+benefits+his+company's+profit+margin.+If+someone+wrote+the+actual+truth+about+him,+everyone+would+be+appalled."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The+s<b>cience+m</b>akes+Manitoba's+action+<b>stupid</b>!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The+<b>science+makes</b>+Manitoba's+action+<b>stupid</b>!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Becau<b>se+Trudeau+knows</b>+that+his+federal+Liberal+Party+survives+on+voters+with+brain+damage+from+recreational+pot+use."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Because+Trudeau+knows</b>+that+his+federal+Liberal+Party+survives+on+voters+with+brain+damage+from+recreational+pot+use."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\"Trump’s+tax+returns:+What+did+we+learn?\"\n",
       "\n",
       "----------\n",
       "\n",
       "<b>That+Rachel+Maddow+is+an</b>+<b>idiot</b>.\n",
       "\n",
       "<b>This+entire+episode+has+blown+up+in+her+face.\n",
       "\n",
       "She+deserves+i</b>t."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\"Trump’s+tax+returns:+What+did+we+learn?\"\n",
       "\n",
       "----------\n",
       "\n",
       "<b>That+Rachel+Maddow+is+an+idiot.\n",
       "</b>\n",
       "<b>This+entire+episode+has+blown+up+in+her+face.\n",
       "\n",
       "She+deserves+i</b>t."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Wha<b>t+a+stupid+question.++How+about+the+taxpayers+right+to+enjoy+our+facilities?++The+state+and+city+are+more+concerned+with+the+non+taxpaying+vendors,+how+much</b>+you+want+to+bet+that+they+don't+pay+any+taxes!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>What+a+stupid+question.++How+about+the+taxpayers+right+to+enjoy+our+facilities?++The+state+and+city+are+more+concerned+with+the+non+taxpaying+vendors,+how+much</b>+you+want+to+bet+that+they+don't+pay+any+taxes!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for row in tqdm([row for i, row in trial.iterrows()]):\n",
    "    labels = spans2labels(row.text, row.spans, tokenizer)\n",
    "    spans2 = labels2spans(row.text, labels, tokenizer)\n",
    "    if row.spans != spans2:\n",
    "        t = row.text.replace(' ', '+')\n",
    "        display_spans(row.spans, t)\n",
    "        display_spans(spans2, t)\n",
    "        n += 1\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc81094a37c4764bbbce81078e32745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_labels = [spans2labels(row.text, row.spans, tokenizer) for i, row in tqdm(train.iterrows())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486c72ba43b64cb9bcd838430d0df4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trial_labels = [spans2labels(row.text, row.spans, tokenizer) for i, row in tqdm(trial.iterrows())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['labels'] = train_labels\n",
    "trial['labels'] = trial_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpansDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpansDataset(tokenizer(train.text.tolist()), train_labels)\n",
    "eval_dataset = SpansDataset(tokenizer(trial.text.tolist()), trial_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_dataset = SpansDataset(tokenizer(final_test.text.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from semeval2021 import f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a single-task model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb\n",
    "https://huggingface.co/transformers/custom_datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.file_utils import cached_property\n",
    "from typing import Tuple\n",
    "\n",
    "device = torch.device('cuda:3')\n",
    "\n",
    "class TrAr(TrainingArguments):\n",
    "    @cached_property\n",
    "    def _setup_devices(self) -> Tuple[\"torch.device\", int]:\n",
    "        return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-61af721bb882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='./models/roberta_single_v2',   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,            # total # of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=3000,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-8,              # strength of weight decay\n",
    "    learning_rate=1e-5,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=eval_dataset,           # evaluation dataset\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The minimal loss of a single-task model (full) was about 28% on validation with 0.04 on train. \n",
    "* If we first train only head (batch 8, lr 1e-3 with 3K warmup and 1e-8 decline), we get minimal loss of 0.185 on validation with 0.23 on train\n",
    "* Training then the whole model (batch 8, lr 1e-5 with 3K warmup and 1e-8 decline) we get minimal loss of 0.175 on validation with 0.21 on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='4674' max='9930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4674/9930 08:26 < 09:30, 9.22 it/s, Epoch 4.71/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.200961</td>\n",
       "      <td>0.177887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.220103</td>\n",
       "      <td>0.177245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.215706</td>\n",
       "      <td>0.177792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.210133</td>\n",
       "      <td>0.175849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.208366</td>\n",
       "      <td>0.187829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.207653</td>\n",
       "      <td>0.193692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.194796</td>\n",
       "      <td>0.189296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.184825</td>\n",
       "      <td>0.206708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.157858</td>\n",
       "      <td>0.200036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4673</td>\n",
       "      <td>0.157858</td>\n",
       "      <td>0.216965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4673</td>\n",
       "      <td>0.157858</td>\n",
       "      <td>0.216965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/p3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    818\u001b[0m                             torch.nn.utils.clip_grad_norm_(\n\u001b[1;32m    819\u001b[0m                                 \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_apex\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m                             )\n\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p3/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.from_pretrained('models/roberta_single_v2/checkpoint-2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.save_pretrained('models/roberta_single_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='174' max='87' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [87/87 00:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.17584852874279022}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.21345926240649193\n",
      "0.01 0.25205437406740794\n",
      "0.03 0.37667972240631\n",
      "0.1 0.5772878732277155\n",
      "0.3 0.66359735406299\n",
      "0.4 0.6731999549591532\n",
      "0.5 0.6702516698368454\n",
      "0.6 0.6618129018990354\n",
      "0.7 0.6474321946754974\n",
      "1 0.06231884057971015\n"
     ]
    }
   ],
   "source": [
    "for threshold in [0, 0.01, 0.03, 0.1, 0.3, 0.4, 0.5, 0.6, 0.7, 1]:\n",
    "    preds = []\n",
    "    for text, pr in zip(trial.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    print(threshold, np.mean([f1(p, y) for p, y in zip(preds, trial.spans)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 0.66359735406299\n",
      "0.32 0.6680702150590394\n",
      "0.35 0.6700635517847642\n",
      "0.38 0.6735559287333039\n",
      "0.4 0.6731999549591532\n",
      "0.42 0.6768011808573329\n",
      "0.45 0.6729071697382535\n",
      "0.5 0.6702516698368454\n",
      "0.55 0.6634497922801099\n",
      "0.6 0.6618129018990354\n"
     ]
    }
   ],
   "source": [
    "for threshold in [0.3, 0.32, 0.35, 0.38, 0.4, 0.42, 0.45, 0.5, 0.55, 0.6]:\n",
    "    preds = []\n",
    "    for text, pr in zip(trial.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    print(threshold, np.mean([f1(p, y) for p, y in zip(preds, trial.spans)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(final_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.35\n",
    "preds = []\n",
    "for text, pr in zip(final_test.text, pred.predictions):\n",
    "    proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "    proba /= proba.sum(axis=1, keepdims=True)\n",
    "    labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "    preds.append(labels2spans(text, labels, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Why do people go to an area during hurricane season or to a war torn country and expect the government to bail them out. I would hope the individuals get billed for the cost. Being a Canadian should not mean you get a free pass to being <b>stupid</b>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = final_test.sample(1).iloc[0]\n",
    "display_spans(preds[row.name], row.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/roberta_th03.txt\", \"w\") as out:\n",
    "    for i in range(len(preds)):\n",
    "        out.write(f\"{str(i)}\\t{str(preds[i])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# M\n",
    "# Mult\n",
    "# Multitasking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForTokenClassification, RobertaForSequenceClassification\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "class WMean(nn.Module):\n",
    "    def __init__(self, dim=-2):\n",
    "        super(WMean, self).__init__()\n",
    "        self.pow = torch.nn.Parameter(data=torch.Tensor([1.0]), requires_grad=True)\n",
    "        self.coef = torch.nn.Parameter(data=torch.Tensor([0.0, 1.0]), requires_grad=True)\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        result = x ** self.pow[0]\n",
    "        if mask is None:\n",
    "            mp = result.mean(dim=-1)\n",
    "        else:\n",
    "            mp = (result * mask).sum(dim=self.dim) / mask.sum(dim=self.dim)\n",
    "        return torch.log(mp) * self.coef[1] + self.coef[0]\n",
    "\n",
    "\n",
    "class RobertaTaggerClassifier(RobertaForTokenClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.wmean = WMean()\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        token_logits = self.classifier(sequence_output)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            masks = attention_mask.unsqueeze(-1).repeat(1, 1, 2)\n",
    "        else:\n",
    "            masks = None\n",
    "\n",
    "        logits = self.wmean(torch.softmax(token_logits, dim=-1), mask=masks)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaTaggerClassifier: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaTaggerClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaTaggerClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaTaggerClassifier were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'wmean.pow', 'wmean.coef']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "rtt_model = RobertaTaggerClassifier.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.7042, -0.6822]], grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rtt_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358984, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>272706</th>\n",
       "      <td>Those aren't anti fascists those are national ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108545</th>\n",
       "      <td>Sherman is a traitor. A running dog lackey to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343175</th>\n",
       "      <td>\"\\n\\nGuy Chapman Slips up\\n\\n\"\"Actually Wessel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  label\n",
       "272706  Those aren't anti fascists those are national ...      0\n",
       "108545  Sherman is a traitor. A running dog lackey to ...      1\n",
       "343175  \"\\n\\nGuy Chapman Slips up\\n\\n\"\"Actually Wessel...      0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('../data/train/train.1.tsv', sep='\\t')\n",
    "df0 = pd.read_csv('../data/train/train_small.0.tsv', sep='\\t')\n",
    "df01 = pd.concat([df1, df0], ignore_index=True)\n",
    "df01.label = df01.label.astype(int)\n",
    "print(df01.shape)\n",
    "df01.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df01, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>214726</th>\n",
       "      <td>My Dad had a saying, if you can't play, play l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275434</th>\n",
       "      <td>Awesome! I realize players cannot / will not s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315281</th>\n",
       "      <td>If it were a school, I'd fire the principal fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225573</th>\n",
       "      <td>Do you believe POTUS should stop having he and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214259</th>\n",
       "      <td>The Liberals better wake up.  \\n\\nRemember wha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274177</th>\n",
       "      <td>This legislature is violating the Alaska State...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20450</th>\n",
       "      <td>if a trip wire or sensors could be hooked up t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182019</th>\n",
       "      <td>See what Dr. Fred Baughman had to say about dr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317139</th>\n",
       "      <td>Impatient millinial. Is that something to adve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23614</th>\n",
       "      <td>So ridiculous people still call this \"Our oil\"...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  label\n",
       "214726  My Dad had a saying, if you can't play, play l...      0\n",
       "275434  Awesome! I realize players cannot / will not s...      0\n",
       "315281  If it were a school, I'd fire the principal fo...      0\n",
       "225573  Do you believe POTUS should stop having he and...      0\n",
       "214259  The Liberals better wake up.  \\n\\nRemember wha...      0\n",
       "274177  This legislature is violating the Alaska State...      0\n",
       "20450   if a trip wire or sensors could be hooked up t...      1\n",
       "182019  See what Dr. Fred Baughman had to say about dr...      0\n",
       "317139  Impatient millinial. Is that something to adve...      0\n",
       "23614   So ridiculous people still call this \"Our oil\"...      1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpansDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_train_dataset = SpansDataset(\n",
    "    tokenizer(df_train.comment_text.tolist(), truncation=True), \n",
    "    df_train.label.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_test_dataset = SpansDataset(\n",
    "    tokenizer(df_test.comment_text.tolist(), truncation=True), \n",
    "    df_test.label.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_test_small_dataset = SpansDataset(\n",
    "    tokenizer(df_test.comment_text.iloc[:3000].tolist(), truncation=True), \n",
    "    df_test.label[:3000].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:3')\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.file_utils import cached_property\n",
    "from typing import Tuple\n",
    "\n",
    "class TrAr(TrainingArguments):\n",
    "    @cached_property\n",
    "    def _setup_devices(self) -> Tuple[\"torch.device\", int]:\n",
    "        return device, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy: first tune the head only with large batches and LR, then tune the whole model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in rtt_model.roberta.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='./models/roberta_clf',   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,             # total # of training epochs\n",
    "    per_device_train_batch_size=4,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    warmup_steps=3000,              # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-7,              # strength of weight decay\n",
    "    learning_rate=1e-6,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=100,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=rtt_model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                      # training arguments, defined above\n",
    "    train_dataset=clf_train_dataset,         # training dataset\n",
    "    eval_dataset=clf_test_small_dataset,           # evaluation dataset\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='21' max='242316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    21/242316 00:01 < 5:46:47, 11.64 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/p3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    797\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([1.6784], device='cuda:3', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.4055, 1.6780], device='cuda:3', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(rtt_model.wmean.pow)\n",
    "print(rtt_model.wmean.coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtt_model.save_pretrained('models/roberta_clf_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtt_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune the averager classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/roberta_clf_proba were not used when initializing RobertaForTokenClassification: ['wmean.pow', 'wmean.coef']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('models/roberta_clf_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='./models/roberta_clf_ft',   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=30,            # total # of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=3000,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-8,              # strength of weight decay\n",
    "    learning_rate=1e-6,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=eval_dataset,           # evaluation dataset\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the raw quasi-classifier: no use in the model at all\n",
    "* fine tuned head: still no use, the best score is 0.2138\n",
    "* fine tune whole model: 0.3 0.6849391042415774"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.21345926240649193\n",
      "0.01 0.33754217278281196\n",
      "0.03 0.5097796783203596\n",
      "0.1 0.6443839746429902\n",
      "0.25 0.6813249614759649\n",
      "0.3 0.6849391042415774\n",
      "0.35 0.684180917165571\n",
      "0.4 0.6787335180780203\n",
      "0.5 0.6640287939316257\n",
      "0.6 0.6514593408593448\n",
      "0.7 0.6172986482655661\n",
      "1 0.06231884057971015\n"
     ]
    }
   ],
   "source": [
    "pred = trainer.predict(eval_dataset)\n",
    "for threshold in [0, 0.01, 0.03, 0.1, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6, 0.7, 1]:\n",
    "    preds = []\n",
    "    for text, pr in zip(trial.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    print(threshold, np.mean([f1(p, y) for p, y in zip(preds, trial.spans)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25 0.6813249614759649\n",
      "0.28 0.6825281648777168\n",
      "0.3 0.6849391042415774\n",
      "0.32 0.6853127403287083\n",
      "0.35 0.684180917165571\n"
     ]
    }
   ],
   "source": [
    "for threshold in [ 0.25, 0.28, 0.3, 0.32, 0.35]:\n",
    "    preds = []\n",
    "    for text, pr in zip(trial.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    print(threshold, np.mean([f1(p, y) for p, y in zip(preds, trial.spans)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we fine tune only the classification head, we achieve the same validation loss (.29) as when fine tuning the whole model, without overfitting at all! However, at this value the model just stops learning. \n",
    "\n",
    "If we start tuning the whole model, validation loss goes to 0.2 quickly, and the learning stops with validation loss around 0.17 and train loss around 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='6904' max='29790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6904/29790 14:56 < 49:31, 7.70 it/s, Epoch 6.95/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.304273</td>\n",
       "      <td>0.249575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.296361</td>\n",
       "      <td>0.221189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.270973</td>\n",
       "      <td>0.194118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.237798</td>\n",
       "      <td>0.182866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.231155</td>\n",
       "      <td>0.177148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.246954</td>\n",
       "      <td>0.175022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.225380</td>\n",
       "      <td>0.173276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.233015</td>\n",
       "      <td>0.175608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.229856</td>\n",
       "      <td>0.174285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.232127</td>\n",
       "      <td>0.166275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.214516</td>\n",
       "      <td>0.172791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.225865</td>\n",
       "      <td>0.167609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.227861</td>\n",
       "      <td>0.174700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='903' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44/44 2:57:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/p3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    828\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p3/lib/python3.7/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0;31m# In-place operations to update the averages at the same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('models/roberta_clf_ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(final_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.6\n",
    "preds = []\n",
    "for text, pr in zip(final_test.text, pred.predictions):\n",
    "    proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "    proba /= proba.sum(axis=1, keepdims=True)\n",
    "    labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "    preds.append(labels2spans(text, labels, tokenizer))\n",
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/roberta_ft_th06.txt\", \"w\") as out:\n",
    "    for i in range(len(preds)):\n",
    "        out.write(f\"{str(i)}\\t{str(preds[i])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "We don't need to know how we are getting ripped off day after day month after month and year after year with this Rail. Write something else. You know the cost will not remain at 8.2 billion especially when the deadline is 2025. We don't need depressing news every morning when we wake up. Just let us know when we can hop on the <b>damn</b> thing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = final_test.sample(1).iloc[0]\n",
    "display_spans(preds[row.name], row.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try to improve the decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = pred.predictions[row.name]\n",
    "proba = np.exp(logits[logits[:, 0]!=-100])\n",
    "proba /= proba.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.truncate_sequences(list(range(1000)), truncation_strategy='longest_first')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_spans(text, target_proba, threshold, tokenizer, agg=max, space='Ġ', punct='.,?!()', return_spans=True, truncate=True):\n",
    "    # try to label a whole multitoken word consistently\n",
    "    punct = set(punct)\n",
    "    result = []\n",
    "    token_labels = [0]\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    if truncate:\n",
    "        token_ids = token_ids[:tokenizer.model_max_length]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    assert len(tokens) <= len(target_proba), '{} > {}'.format(len(tokens), len(target_proba))\n",
    "    left = 0\n",
    "    prev_label = 0\n",
    "    word_start = 0\n",
    "    word_scores = []\n",
    "    for i, (tok, tp) in enumerate(zip(tokens[1:], target_proba[1:])):\n",
    "        right = left + len(tok)\n",
    "        # start of word (except the first one)\n",
    "        if tok[0] == space or tok == '</s>' or tok in punct:\n",
    "            word_score = agg(word_scores)\n",
    "            if word_score > threshold:\n",
    "                if not prev_label:\n",
    "                    word_start += 1\n",
    "                prev_label = 1\n",
    "                result.extend(range(word_start, left))\n",
    "                token_labels.extend([1] * len(word_scores))\n",
    "            else:\n",
    "                prev_label = 0\n",
    "                token_labels.extend([0] * len(word_scores))\n",
    "            word_scores = []\n",
    "            word_start = left\n",
    "        word_scores.append(tp)\n",
    "        left = right\n",
    "    token_labels.extend([0] * len(word_scores))\n",
    "    if return_spans:\n",
    "        return result\n",
    "    else:\n",
    "        return token_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7628563] Ġthing Ġdamn 0.7628563\n",
      "[330, 331, 332, 333]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "We don't need to know how we are getting ripped off day after day month after month and year after year with this Rail. Write something else. You know the cost will not remain at 8.2 billion especially when the deadline is 2025. We don't need depressing news every morning when we wake up. Just let us know when we can hop on the <b>damn</b> thing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = decode_spans(row.text, proba[:, 1], 0.4, tokenizer)\n",
    "print(result)\n",
    "display_spans(result, row.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "agg     max score\n",
    "no agg: 0.6849391042415774\n",
    "max:    0.6733625076759546\n",
    "min:    0.640940791436199 \n",
    "mean:   0.6702291605163007\n",
    "pindep: 0.6529080645860461\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pindep(arr):\n",
    "    pos = np.prod(arr)\n",
    "    neg = np.prod([1 - x for x in arr])\n",
    "    return pos / (pos + neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.21345926240649193 0.2129121719490078\n",
      "0.01 0.33754217278281196 0.3577009491893737\n",
      "0.03 0.5097796783203596 0.5219355909995085\n",
      "0.1 0.6443839746429902 0.6220979241154265\n",
      "0.25 0.6813249614759649 0.6506613818412771\n",
      "0.3 0.6849391042415774 0.6529080645860461\n",
      "0.35 0.684180917165571 0.6496544187202091\n",
      "0.4 0.6787335180780203 0.6423737170064413\n",
      "0.5 0.6640287939316257 0.6289225858545294\n",
      "0.6 0.6514593408593448 0.6138640904113093\n",
      "0.7 0.6172986482655661 0.5890367232017543\n",
      "1 0.06231884057971015 0.06231884057971015\n"
     ]
    }
   ],
   "source": [
    "pred = trainer.predict(eval_dataset)\n",
    "\n",
    "for threshold in [0, 0.01, 0.03, 0.1, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6, 0.7, 1]:\n",
    "    preds = []\n",
    "    for text, pr in zip(trial.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    preds2 = []\n",
    "    for text, pr in zip(trial.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        spans = decode_spans(text, proba[:, 1], threshold, tokenizer, agg=pindep)\n",
    "        preds2.append(spans)\n",
    "    print(\n",
    "        threshold, \n",
    "        np.mean([f1(p, y) for p, y in zip(preds, trial.spans)]),\n",
    "        np.mean([f1(p, y) for p, y in zip(preds2, trial.spans)]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-label the toxic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_toxic = pd.read_csv('../data/train/train.1.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_dataset = SpansDataset(\n",
    "    tokenizer(kaggle_toxic.comment_text.tolist(), truncation=True), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158984\n"
     ]
    }
   ],
   "source": [
    "print(len(kaggle_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('models/roberta_clf_ft')\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='./models/roberta_clf_ft',   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=30,            # total # of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=4,   # smaller batch sizes are actually better, because seq len per batch gets shorter\n",
    "    warmup_steps=3000,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-8,              # strength of weight decay\n",
    "    learning_rate=1e-6,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy='steps'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=eval_dataset,           # evaluation dataset\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='39746' max='39746' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39746/39746 09:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kaggle_preds = trainer.predict(kaggle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158984, 512, 2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_preds.predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spans_utils\n",
    "reload(spans_utils)\n",
    "from spans_utils import decode_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3058f12463a4518a7946ffc4271e9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=158984.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (737 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "kaggle_proba = []\n",
    "kaggle_labels = []\n",
    "for i in trange(len(kaggle_dataset)):\n",
    "    n = len(kaggle_dataset[i]['input_ids'])\n",
    "    logits = kaggle_preds.predictions[i][:n].T\n",
    "    p = np.exp(logits)\n",
    "    p /= p.sum(axis=0)\n",
    "    kaggle_proba.append(p[1])\n",
    "    text = kaggle_toxic.comment_text[i]\n",
    "    token_labels = decode_spans(text, p[1], 0.33, tokenizer, return_spans=False, agg=max)\n",
    "    kaggle_labels.append(token_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    158984.000000\n",
       "mean          0.753992\n",
       "std           0.190560\n",
       "min           0.022448\n",
       "25%           0.647929\n",
       "50%           0.798888\n",
       "75%           0.904961\n",
       "max           0.998292\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxes = pd.Series(max(p) for p in kaggle_proba)\n",
    "maxes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9608451164897096\n",
      "0.8875295627232929\n"
     ]
    }
   ],
   "source": [
    "print((maxes>0.33).mean())\n",
    "print((maxes>0.50).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_dataset.labels = kaggle_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "urges?? thats the best this <b>idiot</b> pm can do? how about stop them and ship them back!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "urges?? thats the best this <b>idiot</b> pm can do? how about stop them and ship them back!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = kaggle_toxic.sample(1).iloc[0]\n",
    "spans = labels2spans(text=row.comment_text, labels=kaggle_dataset[row.name]['labels'], tokenizer=tokenizer)\n",
    "display_spans(spans, row.comment_text)\n",
    "\n",
    "i = row.name\n",
    "n = len(kaggle_dataset[i]['input_ids'])\n",
    "logits = kaggle_preds.predictions[i][:n].T\n",
    "p = np.exp(logits)\n",
    "p /= p.sum(axis=0)\n",
    "\n",
    "token_labels = decode_spans(row.comment_text, p[1], 0.33, tokenizer, return_spans=False, agg=max)\n",
    "spans = labels2spans(labels=token_labels, text=row.comment_text, tokenizer=tokenizer)\n",
    "display_spans(spans, row.comment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='./models/roberta_selflabel',   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,            # total # of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=4,   # smaller batch sizes are actually better, because seq len per batch gets shorter\n",
    "    warmup_steps=3000,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-8,              # strength of weight decay\n",
    "    learning_rate=1e-7,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    "    label_smoothing_factor=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=kaggle_dataset,         # training dataset\n",
    "    eval_dataset=eval_dataset,           # evaluation dataset\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='20049' max='596190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 20049/596190 41:33 < 19:54:27, 8.04 it/s, Epoch 1.01/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.276700</td>\n",
       "      <td>0.321415</td>\n",
       "      <td>2.031800</td>\n",
       "      <td>339.598000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.270700</td>\n",
       "      <td>0.315323</td>\n",
       "      <td>2.017400</td>\n",
       "      <td>342.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.312616</td>\n",
       "      <td>2.025300</td>\n",
       "      <td>340.692000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.264700</td>\n",
       "      <td>0.315503</td>\n",
       "      <td>2.010000</td>\n",
       "      <td>343.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.319900</td>\n",
       "      <td>2.010300</td>\n",
       "      <td>343.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.257800</td>\n",
       "      <td>0.324110</td>\n",
       "      <td>2.010800</td>\n",
       "      <td>343.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.255300</td>\n",
       "      <td>0.326413</td>\n",
       "      <td>2.005600</td>\n",
       "      <td>344.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.254600</td>\n",
       "      <td>0.326755</td>\n",
       "      <td>2.010300</td>\n",
       "      <td>343.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.252100</td>\n",
       "      <td>0.327561</td>\n",
       "      <td>2.026000</td>\n",
       "      <td>340.568000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.250200</td>\n",
       "      <td>0.329431</td>\n",
       "      <td>2.007100</td>\n",
       "      <td>343.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.249100</td>\n",
       "      <td>0.331667</td>\n",
       "      <td>2.007600</td>\n",
       "      <td>343.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.246600</td>\n",
       "      <td>0.329653</td>\n",
       "      <td>2.006100</td>\n",
       "      <td>343.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.248500</td>\n",
       "      <td>0.328217</td>\n",
       "      <td>2.009100</td>\n",
       "      <td>343.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.248700</td>\n",
       "      <td>0.329008</td>\n",
       "      <td>2.007200</td>\n",
       "      <td>343.767000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.249200</td>\n",
       "      <td>0.328215</td>\n",
       "      <td>2.010800</td>\n",
       "      <td>343.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.246800</td>\n",
       "      <td>0.328846</td>\n",
       "      <td>2.015300</td>\n",
       "      <td>342.389000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.248400</td>\n",
       "      <td>0.326551</td>\n",
       "      <td>2.041700</td>\n",
       "      <td>337.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.244700</td>\n",
       "      <td>0.326237</td>\n",
       "      <td>2.036300</td>\n",
       "      <td>338.843000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.247600</td>\n",
       "      <td>0.328023</td>\n",
       "      <td>2.029900</td>\n",
       "      <td>339.913000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.246500</td>\n",
       "      <td>0.326920</td>\n",
       "      <td>2.008800</td>\n",
       "      <td>343.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.246300</td>\n",
       "      <td>0.328234</td>\n",
       "      <td>2.046100</td>\n",
       "      <td>337.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.245700</td>\n",
       "      <td>0.326202</td>\n",
       "      <td>2.030800</td>\n",
       "      <td>339.773000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.245100</td>\n",
       "      <td>0.325267</td>\n",
       "      <td>2.008900</td>\n",
       "      <td>343.479000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.245600</td>\n",
       "      <td>0.325399</td>\n",
       "      <td>2.069100</td>\n",
       "      <td>333.483000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.243400</td>\n",
       "      <td>0.324633</td>\n",
       "      <td>2.025000</td>\n",
       "      <td>340.737000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.244000</td>\n",
       "      <td>0.324832</td>\n",
       "      <td>2.023100</td>\n",
       "      <td>341.066000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.244700</td>\n",
       "      <td>0.325862</td>\n",
       "      <td>2.049600</td>\n",
       "      <td>336.655000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.244600</td>\n",
       "      <td>0.325462</td>\n",
       "      <td>2.030300</td>\n",
       "      <td>339.847000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>0.325977</td>\n",
       "      <td>2.039500</td>\n",
       "      <td>338.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.244100</td>\n",
       "      <td>0.325133</td>\n",
       "      <td>2.033100</td>\n",
       "      <td>339.378000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.324905</td>\n",
       "      <td>2.010400</td>\n",
       "      <td>343.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.242700</td>\n",
       "      <td>0.326329</td>\n",
       "      <td>2.010900</td>\n",
       "      <td>343.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.244100</td>\n",
       "      <td>0.325999</td>\n",
       "      <td>2.013900</td>\n",
       "      <td>342.616000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>0.324516</td>\n",
       "      <td>2.014600</td>\n",
       "      <td>342.508000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.245000</td>\n",
       "      <td>0.327334</td>\n",
       "      <td>2.010400</td>\n",
       "      <td>343.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.241700</td>\n",
       "      <td>0.326330</td>\n",
       "      <td>2.070000</td>\n",
       "      <td>333.338000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.241700</td>\n",
       "      <td>0.326634</td>\n",
       "      <td>2.019700</td>\n",
       "      <td>341.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.242100</td>\n",
       "      <td>0.327076</td>\n",
       "      <td>2.124300</td>\n",
       "      <td>324.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.241500</td>\n",
       "      <td>0.327673</td>\n",
       "      <td>2.028600</td>\n",
       "      <td>340.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.242000</td>\n",
       "      <td>0.326858</td>\n",
       "      <td>2.021600</td>\n",
       "      <td>341.308000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='346' max='173' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [173/173 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/p3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    886\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best score is **0.6865**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.21345926240649193\n",
      "0.01 0.21345926240649193\n",
      "0.03 0.21447329483589261\n",
      "0.1 0.6487386870668558\n",
      "0.25 0.6793734526472192\n",
      "0.3 0.6826507661661871\n",
      "0.35 0.686369410349317\n",
      "0.37 0.6865370507610563\n",
      "0.4 0.6857396940615527\n",
      "0.5 0.6865664238391493\n",
      "0.6 0.6841674313771393\n",
      "0.7 0.6857623157818409\n",
      "1 0.06231884057971015\n"
     ]
    }
   ],
   "source": [
    "pred = trainer.predict(eval_dataset)\n",
    "for threshold in [0, 0.01, 0.03, 0.1, 0.25, 0.3, 0.35, 0.37, 0.4, 0.5, 0.6, 0.7, 1]:\n",
    "    preds = []\n",
    "    for text, pr in zip(trial.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    print(threshold, np.mean([f1(p, y) for p, y in zip(preds, trial.spans)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning on the pseudo-labeled set, tune once more on the original set to calibrate the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:4')\n",
    "\n",
    "class TrAr(TrainingArguments):\n",
    "    @cached_property\n",
    "    def _setup_devices(self) -> Tuple[\"torch.device\", int]:\n",
    "        return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='./models/roberta_selflabel_final',   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,            # total # of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=4,   # smaller batch sizes are actually better, because seq len per batch gets shorter\n",
    "    warmup_steps=500,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-8,              # strength of weight decay\n",
    "    learning_rate=1e-6,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=100,\n",
    "    eval_steps=100,\n",
    "    evaluation_strategy='steps',\n",
    "    label_smoothing_factor=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=eval_dataset,           # evaluation dataset\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1986' max='1986' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1986/1986 03:55, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.165277</td>\n",
       "      <td>2.082800</td>\n",
       "      <td>331.284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.211100</td>\n",
       "      <td>0.165285</td>\n",
       "      <td>2.030000</td>\n",
       "      <td>339.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.189600</td>\n",
       "      <td>0.169712</td>\n",
       "      <td>2.057100</td>\n",
       "      <td>335.431000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.200900</td>\n",
       "      <td>0.170866</td>\n",
       "      <td>1.971600</td>\n",
       "      <td>349.972000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.173400</td>\n",
       "      <td>0.165180</td>\n",
       "      <td>1.974800</td>\n",
       "      <td>349.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>0.163509</td>\n",
       "      <td>1.973600</td>\n",
       "      <td>349.617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.245400</td>\n",
       "      <td>0.160556</td>\n",
       "      <td>1.990600</td>\n",
       "      <td>346.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.207200</td>\n",
       "      <td>0.164278</td>\n",
       "      <td>1.979400</td>\n",
       "      <td>348.588000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.188100</td>\n",
       "      <td>0.170596</td>\n",
       "      <td>1.980000</td>\n",
       "      <td>348.491000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.210900</td>\n",
       "      <td>0.163308</td>\n",
       "      <td>1.974600</td>\n",
       "      <td>349.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.212600</td>\n",
       "      <td>0.166779</td>\n",
       "      <td>1.989600</td>\n",
       "      <td>346.806000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.210600</td>\n",
       "      <td>0.167101</td>\n",
       "      <td>1.980800</td>\n",
       "      <td>348.344000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.213400</td>\n",
       "      <td>0.164756</td>\n",
       "      <td>1.984800</td>\n",
       "      <td>347.649000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.212600</td>\n",
       "      <td>0.163347</td>\n",
       "      <td>1.980200</td>\n",
       "      <td>348.452000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.164393</td>\n",
       "      <td>2.014700</td>\n",
       "      <td>342.485000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.204400</td>\n",
       "      <td>0.166792</td>\n",
       "      <td>1.992100</td>\n",
       "      <td>346.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.226900</td>\n",
       "      <td>0.169341</td>\n",
       "      <td>1.980600</td>\n",
       "      <td>348.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.165640</td>\n",
       "      <td>1.979500</td>\n",
       "      <td>348.567000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.165620</td>\n",
       "      <td>1.980800</td>\n",
       "      <td>348.351000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1986, training_loss=0.20756035917114996, metrics={'train_runtime': 235.7622, 'train_samples_per_second': 8.424, 'total_flos': 1529497023746184, 'epoch': 2.0})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max: **0.6887** (случайно потренировал на основной выборке с label smoothing 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.21345926240649193\n",
      "0.01 0.3012953761616438\n",
      "0.03 0.4746953451131265\n",
      "0.1 0.6348240644894162\n",
      "0.25 0.6784224446048986\n",
      "0.3 0.6827629205595998\n",
      "0.35 0.6855501867465028\n",
      "0.37 0.6879845625829613\n",
      "0.4 0.6880259983225367\n",
      "0.43 0.683851102696139\n",
      "0.5 0.6797056139144931\n",
      "0.6 0.6688018820701156\n",
      "0.7 0.6511178167749042\n",
      "1 0.06231884057971015\n"
     ]
    }
   ],
   "source": [
    "pred = trainer.predict(eval_dataset)\n",
    "for threshold in [0, 0.01, 0.03, 0.1, 0.25, 0.3, 0.35, 0.37, 0.4, 0.43, 0.5, 0.6, 0.7, 1]:\n",
    "    preds = []\n",
    "    for text, pr in zip(trial.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    print(threshold, np.mean([f1(p, y) for p, y in zip(preds, trial.spans)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('models/roberta_clf_ft_plus_pseudolabels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(final_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.6\n",
    "preds = []\n",
    "for text, pr in zip(final_test.text, pred.predictions):\n",
    "    proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "    proba /= proba.sum(axis=1, keepdims=True)\n",
    "    labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "    preds.append(labels2spans(text, labels, tokenizer))\n",
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/roberta_ft_pseudol_th06.txt\", \"w\") as out:\n",
    "    for i in range(len(preds)):\n",
    "        out.write(f\"{str(i)}\\t{str(preds[i])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "And you were doing so well explaining your side and than, <b>damn</b> you had to bring out the race card and with that you LOSE!! Better luck next time!"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = final_test.sample(1).iloc[0]\n",
    "display_spans(preds[row.name], row.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train to fine-tune after training a classic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "clf_model = RobertaForSequenceClassification.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in clf_model.roberta.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='./models/roberta_clf_2',   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,             # total # of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    warmup_steps=3000,              # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-7,              # strength of weight decay\n",
    "    learning_rate=1e-3,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=clf_model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                      # training arguments, defined above\n",
    "    train_dataset=clf_train_dataset,         # training dataset\n",
    "    eval_dataset=clf_test_small_dataset,           # evaluation dataset\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='20193' max='20193' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20193/20193 37:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.550800</td>\n",
       "      <td>0.429195</td>\n",
       "      <td>16.024700</td>\n",
       "      <td>187.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.515600</td>\n",
       "      <td>0.431466</td>\n",
       "      <td>16.049300</td>\n",
       "      <td>186.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.516900</td>\n",
       "      <td>0.429980</td>\n",
       "      <td>16.057200</td>\n",
       "      <td>186.832000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.537600</td>\n",
       "      <td>0.438840</td>\n",
       "      <td>16.056800</td>\n",
       "      <td>186.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.518100</td>\n",
       "      <td>0.447410</td>\n",
       "      <td>16.050800</td>\n",
       "      <td>186.906000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.528600</td>\n",
       "      <td>0.428593</td>\n",
       "      <td>16.056100</td>\n",
       "      <td>186.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.540700</td>\n",
       "      <td>0.445084</td>\n",
       "      <td>16.056400</td>\n",
       "      <td>186.841000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.512600</td>\n",
       "      <td>0.479808</td>\n",
       "      <td>16.064800</td>\n",
       "      <td>186.744000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.520200</td>\n",
       "      <td>0.467433</td>\n",
       "      <td>16.056800</td>\n",
       "      <td>186.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.528900</td>\n",
       "      <td>0.421904</td>\n",
       "      <td>16.055200</td>\n",
       "      <td>186.855000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.514600</td>\n",
       "      <td>0.423427</td>\n",
       "      <td>16.054900</td>\n",
       "      <td>186.858000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.523600</td>\n",
       "      <td>0.426291</td>\n",
       "      <td>16.062400</td>\n",
       "      <td>186.772000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.510200</td>\n",
       "      <td>0.468615</td>\n",
       "      <td>16.055100</td>\n",
       "      <td>186.857000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.524200</td>\n",
       "      <td>0.474307</td>\n",
       "      <td>16.059200</td>\n",
       "      <td>186.809000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.517700</td>\n",
       "      <td>0.430597</td>\n",
       "      <td>16.058600</td>\n",
       "      <td>186.816000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.502100</td>\n",
       "      <td>0.424258</td>\n",
       "      <td>16.054800</td>\n",
       "      <td>186.861000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.518700</td>\n",
       "      <td>0.437283</td>\n",
       "      <td>16.050800</td>\n",
       "      <td>186.907000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.510700</td>\n",
       "      <td>0.425912</td>\n",
       "      <td>16.053700</td>\n",
       "      <td>186.873000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.510200</td>\n",
       "      <td>0.472426</td>\n",
       "      <td>16.054600</td>\n",
       "      <td>186.862000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.512500</td>\n",
       "      <td>0.419555</td>\n",
       "      <td>16.053700</td>\n",
       "      <td>186.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.520500</td>\n",
       "      <td>0.435337</td>\n",
       "      <td>16.053900</td>\n",
       "      <td>186.871000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.507400</td>\n",
       "      <td>0.414303</td>\n",
       "      <td>16.049800</td>\n",
       "      <td>186.918000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.507900</td>\n",
       "      <td>0.423115</td>\n",
       "      <td>16.049000</td>\n",
       "      <td>186.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.518700</td>\n",
       "      <td>0.430626</td>\n",
       "      <td>16.047800</td>\n",
       "      <td>186.941000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.493200</td>\n",
       "      <td>0.424967</td>\n",
       "      <td>16.053600</td>\n",
       "      <td>186.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.496600</td>\n",
       "      <td>0.424347</td>\n",
       "      <td>16.056200</td>\n",
       "      <td>186.843000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.496100</td>\n",
       "      <td>0.422870</td>\n",
       "      <td>16.052800</td>\n",
       "      <td>186.884000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.508400</td>\n",
       "      <td>0.425831</td>\n",
       "      <td>16.061900</td>\n",
       "      <td>186.777000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.499600</td>\n",
       "      <td>0.414448</td>\n",
       "      <td>16.061400</td>\n",
       "      <td>186.783000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.505400</td>\n",
       "      <td>0.414647</td>\n",
       "      <td>16.058500</td>\n",
       "      <td>186.817000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.495900</td>\n",
       "      <td>0.413661</td>\n",
       "      <td>16.070500</td>\n",
       "      <td>186.677000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.491900</td>\n",
       "      <td>0.412250</td>\n",
       "      <td>16.061100</td>\n",
       "      <td>186.787000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.499000</td>\n",
       "      <td>0.415643</td>\n",
       "      <td>16.058200</td>\n",
       "      <td>186.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.487100</td>\n",
       "      <td>0.412412</td>\n",
       "      <td>16.058400</td>\n",
       "      <td>186.818000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.493300</td>\n",
       "      <td>0.417384</td>\n",
       "      <td>16.060600</td>\n",
       "      <td>186.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.487000</td>\n",
       "      <td>0.410172</td>\n",
       "      <td>16.061600</td>\n",
       "      <td>186.781000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.502900</td>\n",
       "      <td>0.421606</td>\n",
       "      <td>16.061500</td>\n",
       "      <td>186.782000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.496200</td>\n",
       "      <td>0.414124</td>\n",
       "      <td>16.078100</td>\n",
       "      <td>186.589000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.494400</td>\n",
       "      <td>0.414143</td>\n",
       "      <td>16.058600</td>\n",
       "      <td>186.816000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.493300</td>\n",
       "      <td>0.414674</td>\n",
       "      <td>16.060900</td>\n",
       "      <td>186.789000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20193, training_loss=0.5100492591614214, metrics={'train_runtime': 2255.1964, 'train_samples_per_second': 8.954, 'total_flos': 52054189856424960, 'epoch': 1.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in clf_model.roberta.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='./models/roberta_clf_2',   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,             # total # of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    warmup_steps=5000,              # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-8,              # strength of weight decay\n",
    "    learning_rate=1e-6,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=clf_model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                      # training arguments, defined above\n",
    "    train_dataset=clf_train_dataset,         # training dataset\n",
    "    eval_dataset=clf_test_small_dataset,           # evaluation dataset\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='14501' max='40386' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14501/40386 37:37 < 1:07:10, 6.42 it/s, Epoch 0.36/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.275800</td>\n",
       "      <td>0.283414</td>\n",
       "      <td>16.231500</td>\n",
       "      <td>184.826000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.277800</td>\n",
       "      <td>0.286870</td>\n",
       "      <td>16.110000</td>\n",
       "      <td>186.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.247400</td>\n",
       "      <td>0.300099</td>\n",
       "      <td>16.116500</td>\n",
       "      <td>186.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.249300</td>\n",
       "      <td>0.315221</td>\n",
       "      <td>16.118100</td>\n",
       "      <td>186.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.238100</td>\n",
       "      <td>0.341812</td>\n",
       "      <td>16.116100</td>\n",
       "      <td>186.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.201500</td>\n",
       "      <td>0.361610</td>\n",
       "      <td>16.117700</td>\n",
       "      <td>186.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.211500</td>\n",
       "      <td>0.372231</td>\n",
       "      <td>16.119700</td>\n",
       "      <td>186.108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.190900</td>\n",
       "      <td>0.380558</td>\n",
       "      <td>16.115500</td>\n",
       "      <td>186.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.396305</td>\n",
       "      <td>16.115900</td>\n",
       "      <td>186.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.196700</td>\n",
       "      <td>0.411312</td>\n",
       "      <td>16.110900</td>\n",
       "      <td>186.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>0.416732</td>\n",
       "      <td>16.120400</td>\n",
       "      <td>186.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.176900</td>\n",
       "      <td>0.424274</td>\n",
       "      <td>16.121200</td>\n",
       "      <td>186.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.176400</td>\n",
       "      <td>0.443561</td>\n",
       "      <td>16.115100</td>\n",
       "      <td>186.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.393500</td>\n",
       "      <td>0.348970</td>\n",
       "      <td>16.109400</td>\n",
       "      <td>186.227000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.350800</td>\n",
       "      <td>0.317202</td>\n",
       "      <td>16.117200</td>\n",
       "      <td>186.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.299800</td>\n",
       "      <td>0.327594</td>\n",
       "      <td>16.127000</td>\n",
       "      <td>186.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.333100</td>\n",
       "      <td>0.308280</td>\n",
       "      <td>16.119200</td>\n",
       "      <td>186.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.303900</td>\n",
       "      <td>0.307834</td>\n",
       "      <td>16.113600</td>\n",
       "      <td>186.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.331100</td>\n",
       "      <td>0.300620</td>\n",
       "      <td>16.115200</td>\n",
       "      <td>186.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.302138</td>\n",
       "      <td>16.109200</td>\n",
       "      <td>186.229000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.308500</td>\n",
       "      <td>0.302929</td>\n",
       "      <td>16.118700</td>\n",
       "      <td>186.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.301100</td>\n",
       "      <td>0.299393</td>\n",
       "      <td>16.125100</td>\n",
       "      <td>186.045000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.325800</td>\n",
       "      <td>0.293886</td>\n",
       "      <td>16.117800</td>\n",
       "      <td>186.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.301300</td>\n",
       "      <td>0.297229</td>\n",
       "      <td>16.114600</td>\n",
       "      <td>186.167000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.301600</td>\n",
       "      <td>0.302187</td>\n",
       "      <td>16.114300</td>\n",
       "      <td>186.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.307700</td>\n",
       "      <td>0.299928</td>\n",
       "      <td>16.115400</td>\n",
       "      <td>186.158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.308000</td>\n",
       "      <td>0.295077</td>\n",
       "      <td>16.122600</td>\n",
       "      <td>186.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.307400</td>\n",
       "      <td>0.297908</td>\n",
       "      <td>16.120800</td>\n",
       "      <td>186.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>0.300415</td>\n",
       "      <td>16.122200</td>\n",
       "      <td>186.079000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "write(): fd 126 failed with No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/p3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    927\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_epoch_stop\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_training_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_world_process_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"optimizer.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcaught_warnings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"scheduler.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0mserialized_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_should_read_directly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: write(): fd 126 failed with No space left on device"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model.save_pretrained('models/roberta_clf_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/roberta_clf_2 were not used when initializing RobertaForTokenClassification: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at models/roberta_clf_2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('models/roberta_clf_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='./models/roberta_ft_v2',   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,            # total # of training epochs\n",
    "    per_device_train_batch_size=64,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=3000,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-8,              # strength of weight decay\n",
    "    learning_rate=1e-3,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    "    save_total_limit=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=eval_dataset,           # evaluation dataset\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 06:16, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.218200</td>\n",
       "      <td>0.181255</td>\n",
       "      <td>2.022100</td>\n",
       "      <td>341.236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.218100</td>\n",
       "      <td>0.183458</td>\n",
       "      <td>2.053600</td>\n",
       "      <td>335.999000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.21767515563964843, metrics={'train_runtime': 376.8704, 'train_samples_per_second': 3.317, 'total_flos': 13112992679461536, 'epoch': 10.0})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='./models/roberta_ft_v2',   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,            # total # of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=3000,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-8,              # strength of weight decay\n",
    "    learning_rate=1e-5,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    "    save_total_limit=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=eval_dataset,           # evaluation dataset\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='6804' max='9930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6804/9930 11:40 < 05:21, 9.71 it/s, Epoch 6.85/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.210100</td>\n",
       "      <td>0.182794</td>\n",
       "      <td>2.021700</td>\n",
       "      <td>341.304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.231000</td>\n",
       "      <td>0.175530</td>\n",
       "      <td>2.016800</td>\n",
       "      <td>342.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.220400</td>\n",
       "      <td>0.175073</td>\n",
       "      <td>2.015900</td>\n",
       "      <td>342.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>0.175506</td>\n",
       "      <td>2.011800</td>\n",
       "      <td>342.979000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.212400</td>\n",
       "      <td>0.179223</td>\n",
       "      <td>2.013500</td>\n",
       "      <td>342.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.214300</td>\n",
       "      <td>0.186044</td>\n",
       "      <td>2.017800</td>\n",
       "      <td>341.949000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.206600</td>\n",
       "      <td>0.170099</td>\n",
       "      <td>2.017900</td>\n",
       "      <td>341.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.198100</td>\n",
       "      <td>0.186814</td>\n",
       "      <td>2.020800</td>\n",
       "      <td>341.442000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.179500</td>\n",
       "      <td>0.180094</td>\n",
       "      <td>2.020600</td>\n",
       "      <td>341.478000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.179900</td>\n",
       "      <td>0.204089</td>\n",
       "      <td>2.018300</td>\n",
       "      <td>341.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.156800</td>\n",
       "      <td>0.203710</td>\n",
       "      <td>2.015600</td>\n",
       "      <td>342.335000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.159000</td>\n",
       "      <td>0.214134</td>\n",
       "      <td>2.019000</td>\n",
       "      <td>341.758000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.144100</td>\n",
       "      <td>0.211439</td>\n",
       "      <td>2.016300</td>\n",
       "      <td>342.219000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='87' max='87' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [87/87 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/p3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    919\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p3/lib/python3.7/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.21345926240649193\n",
      "0.01 0.4157790976472464\n",
      "0.03 0.5294728448610527\n",
      "0.1 0.6148151396215593\n",
      "0.25 0.6550136242080651\n",
      "0.3 0.6646029995260339\n",
      "0.35 0.6715131454982759\n",
      "0.37 0.6733516869972194\n",
      "0.4 0.6744209810749072\n",
      "0.43 0.6741804277228053\n",
      "0.5 0.673925228987182\n",
      "0.6 0.6717334585591548\n",
      "0.7 0.6682677734353392\n",
      "1 0.06231884057971015\n"
     ]
    }
   ],
   "source": [
    "pred = trainer.predict(eval_dataset)\n",
    "for threshold in [0, 0.01, 0.03, 0.1, 0.25, 0.3, 0.35, 0.37, 0.4, 0.43, 0.5, 0.6, 0.7, 1]:\n",
    "    preds = []\n",
    "    for text, pr in zip(trial.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    print(threshold, np.mean([f1(p, y) for p, y in zip(preds, trial.spans)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: лучше вроде бы не стало, хоть и сильно хуже - тоже. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('models/roberta_ft_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3k",
   "language": "python",
   "name": "p3k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
