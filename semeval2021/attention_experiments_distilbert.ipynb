{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 17 08:10:01 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 27%   37C    P8    26W / 260W |   7203MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 27%   31C    P8    10W / 260W |   6215MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 27%   35C    P8     5W / 260W |   9070MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 27%   33C    P8    10W / 260W |   5698MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce RTX 208...  Off  | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 27%   30C    P8     2W / 260W |   5706MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce RTX 208...  Off  | 00000000:3E:00.0 Off |                  N/A |\n",
      "| 27%   33C    P8    14W / 260W |   5216MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce RTX 208...  Off  | 00000000:40:00.0 Off |                  N/A |\n",
      "| 27%   32C    P8    12W / 260W |   5764MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce RTX 208...  Off  | 00000000:41:00.0 Off |                  N/A |\n",
      "| 27%   31C    P8    15W / 260W |   6282MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0      4694      C   /opt/.pyenv/versions/3.7.4/bin/python        885MiB |\n",
      "|    0      9727      C   /opt/.pyenv/versions/3.7.4/bin/python       1407MiB |\n",
      "|    0     15151      C   /opt/.pyenv/versions/3.7.4/bin/python       1081MiB |\n",
      "|    0     23791      C   /opt/.pyenv/versions/3.7.4/bin/python       3819MiB |\n",
      "|    1      4694      C   /opt/.pyenv/versions/3.7.4/bin/python       1029MiB |\n",
      "|    1     17743      C   python                                      5173MiB |\n",
      "|    2     24051      C   /opt/.pyenv/versions/3.7.4/bin/python       9059MiB |\n",
      "|    3     17742      C   python                                      5685MiB |\n",
      "|    4     17744      C   python                                      5693MiB |\n",
      "|    5     17745      C   python                                      5203MiB |\n",
      "|    6     17746      C   python                                      5751MiB |\n",
      "|    7      4009      C   /home/markov/anaconda3/bin/python           2791MiB |\n",
      "|    7      9443      C   /home/logacheva/anaconda3/bin/python        1329MiB |\n",
      "|    7     21307      C   /home/markov/anaconda3/bin/python           2151MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from box import Box\n",
    "import pandas as pd\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import datetime\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from fast_bert.modeling import BertForMultiLabelSequenceClassification\n",
    "from fast_bert.data_cls import BertDataBunch, InputExample, InputFeatures, MultiLabelTextProcessor, convert_examples_to_features\n",
    "from fast_bert.learner_cls import BertLearner\n",
    "\n",
    "import re\n",
    "MAX_SENTENCE_LEN = 82\n",
    "\n",
    "from utils import preprocess\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import torchtext.data as data\n",
    "from torchtext.data import Field\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "from transformers import BertTokenizer, BertForMaskedLM, AdamW, DistilBertTokenizer\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from functools import partial\n",
    "import time\n",
    "from IPython.core.debugger import set_trace\n",
    "from tqdm import tqdm, trange\n",
    "# from tqdm import tqdm, trange\n",
    "\n",
    "seed = 678\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(7)\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "DATA_PATH = '../../../semeval2021_task/data/'\n",
    "\n",
    "train = pd.read_csv(DATA_PATH + 'tsd_train.csv')\n",
    "trial = pd.read_csv(DATA_PATH + 'tsd_trial.csv')\n",
    "\n",
    "train['spans'] = train.spans.apply(literal_eval)\n",
    "trial['spans'] = trial.spans.apply(literal_eval)\n",
    "texts = list(train['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFICATION_MODEL_PATH = '../../models/distilbert/model_out'\n",
    "CLASSIFICATION_LABELS_PATH = '../../labels'\n",
    "\n",
    "from fast_bert.prediction import BertClassificationPredictor\n",
    "\n",
    "predictor = BertClassificationPredictor(CLASSIFICATION_MODEL_PATH, CLASSIFICATION_LABELS_PATH, \n",
    "                                        multi_label=True, model_type='distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "lern = predictor.get_learner()\n",
    "pretrained_weights = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(pretrained_weights)\n",
    "MASK_INDEX = tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
    "\n",
    "lern.model.eval();\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = {v: k for k, v in tokenizer.vocab.items()}\n",
    "\n",
    "def toks_to_words(token_ids):\n",
    "    \"\"\" Merge subword tokens into whole words \"\"\"\n",
    "    indices = []\n",
    "    for i, token_id in enumerate(token_ids):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        token_text = v[token_id]\n",
    "        if token_text.startswith('##'):\n",
    "            indices.append(i)\n",
    "        else:\n",
    "            if indices:\n",
    "                toks = [v[token_ids[t]] for t in indices]\n",
    "                word = ''.join([toks[0]] + [t[2:] for t in toks[1:]])\n",
    "                new_indices = [index - 1 for index in indices]\n",
    "                yield new_indices, word\n",
    "            indices = [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig\n",
    "config = DistilBertConfig.from_pretrained('distilbert-base-cased', output_attentions=True)\n",
    "lern.model.distilbert.config = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years ago he was blaming Western values, including respect for life and democracy, for his misfortunes.  What a total clown.\n"
     ]
    }
   ],
   "source": [
    "text = random.choice(texts)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([0], 'years'),\n",
       " ([1], 'ago'),\n",
       " ([2], 'he'),\n",
       " ([3], 'was'),\n",
       " ([4], 'blaming'),\n",
       " ([5], 'western'),\n",
       " ([6], 'values'),\n",
       " ([7], ','),\n",
       " ([8], 'including'),\n",
       " ([9], 'respect'),\n",
       " ([10], 'for'),\n",
       " ([11], 'life'),\n",
       " ([12], 'and'),\n",
       " ([13], 'democracy'),\n",
       " ([14], ','),\n",
       " ([15], 'for'),\n",
       " ([16], 'his'),\n",
       " ([17, 18, 19], 'misfortunes'),\n",
       " ([20], '.'),\n",
       " ([21], 'what'),\n",
       " ([22], 'a'),\n",
       " ([23], 'total'),\n",
       " ([24], 'clown'),\n",
       " ([25], '.')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(toks_to_words(toks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to use the attentions from all heads and all layers as features for words, then later train some classifier on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink(nums_arr, att_arr):\n",
    "    \"\"\"\n",
    "    When words are splitted by several tokens by BERT, each token has\n",
    "    its own feature vector. this function takes the mean of these vectors\n",
    "    and assign it to the original word.\n",
    "    \n",
    "    nums_arr: list, contains indices of tokens that need to be united back into\n",
    "    one word\n",
    "    \n",
    "    att_arr: np.array of attention, shape (num_heads) x (num_words) or\n",
    "    (num_layers) x (num_words)\n",
    "    \n",
    "    output: np.array, shape (num_words) x (num_heads) or \n",
    "    (num_words) x (num_layers)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    prev_i = 0\n",
    "    for arr in nums_arr:#for each splitted word\n",
    "        fig = att_arr[:, prev_i:arr[0]] #add all previous words\n",
    "        nafig = att_arr[:, arr].mean(axis=1)[:, np.newaxis] #add mean of token features\n",
    "        res.append(fig)\n",
    "        res.append(nafig)\n",
    "        prev_i = arr[-1] + 1 #make the current word \"previous\"\n",
    "    if prev_i < att_arr.shape[1]: #just to be safe and not overstep the array\n",
    "        res.append(att_arr[:, prev_i:])\n",
    "    return np.hstack(res).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_for_words(text: str, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Get attention values for all heads averaged over all layers and \n",
    "    attention values for all layers averaged over all heads.\n",
    "    \"\"\"\n",
    "    toks = tokenizer.encode(text) \n",
    "    emb_num = model.distilbert.embeddings.word_embeddings.num_embeddings\n",
    "    toks = [i if i < emb_num else tokenizer.unk_token_id for i in toks]\n",
    "    #these procedure is necessary because for some reason, \n",
    "    #tokenizer knows more words that BERT\n",
    "    nums = [arr for (arr, string) in list(toks_to_words(toks)) if len(arr) > 1]\n",
    "    sentence = torch.tensor(toks).unsqueeze(0)\n",
    "    sentence = sentence.cuda()\n",
    "    out = lern.model.distilbert(sentence, output_attentions=True,\n",
    "                    output_hidden_states=True)\n",
    "    #we cut the CLS and SEP tokens\n",
    "    attentions = torch.cat(out[2], dim=0).cpu()[:, :, 1:-1, 1:-1]\n",
    "    #take mean attention over all layers\n",
    "    means_heads =  attentions.mean(dim=(0, 2)).detach().numpy()\n",
    "    #take mean attention over all heads\n",
    "    means_layers = attentions.mean(dim=(1, 2)).detach().numpy()\n",
    "    means_heads = shrink(nums, means_heads)\n",
    "    means_layers = shrink(nums, means_layers)\n",
    "\n",
    "    return means_heads, means_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getword(arr, text):\n",
    "    \"\"\"\n",
    "    Using a span from a dataset, obtain a word\n",
    "    \"\"\"\n",
    "    ans = ''\n",
    "    for i in range(len(arr)):\n",
    "        elem = arr[i]\n",
    "        if  i != 0 and i != len(arr) - 1 and elem != arr[i-1] + 1:\n",
    "            ans += ' '\n",
    "            ans += text[elem]\n",
    "        else:\n",
    "            ans += text[elem]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(span, text, model, tokenizer):\n",
    "    target_toks = tokenizer.encode(getword(span, text))\n",
    "    toks = tokenizer.encode(text)\n",
    "    emb_num = model.distilbert.embeddings.word_embeddings.num_embeddings\n",
    "    target_toks = [i if i < emb_num else tokenizer.unk_token_id for i in target_toks]\n",
    "    toks = [i if i < emb_num else tokenizer.unk_token_id for i in toks]\n",
    "    toks = [string for (arr, string) in list(toks_to_words(toks))]\n",
    "    target_toks = [string for (arr, string) in list(toks_to_words(target_toks))]\n",
    "    target = []\n",
    "    #check if the word is in spanned words\n",
    "    for tok in toks:\n",
    "        if len(target_toks) > 0 and tok == target_toks[0]:\n",
    "            target.append(1)\n",
    "            target_toks = target_toks[1:]\n",
    "        else:\n",
    "            target.append(0)\n",
    "    return toks, target\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['another',\n",
       "  'violent',\n",
       "  'and',\n",
       "  'aggressive',\n",
       "  'immigrant',\n",
       "  'killing',\n",
       "  'a',\n",
       "  'innocent',\n",
       "  'and',\n",
       "  'intelligent',\n",
       "  'us',\n",
       "  'citizen',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  'sarcasm'],\n",
       " [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_target(train.spans.iloc[0], train.text.iloc[0], lern.model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7939it [01:46, 74.59it/s] \n"
     ]
    }
   ],
   "source": [
    "target = []\n",
    "data = np.array([])\n",
    "for span, text in tqdm(zip(train['spans'], train['text'])):\n",
    "    heads, layers = get_attention_for_words(text, lern.model, tokenizer)\n",
    "    headlay = np.hstack((heads, layers))\n",
    "    #print(headlay.shape)\n",
    "    if len(data) == 0:\n",
    "        data = headlay\n",
    "    else:\n",
    "        data = np.vstack((data, headlay))\n",
    "    words, new_target = make_target(span, text, lern.model, tokenizer)\n",
    "    #print(new_target)\n",
    "    try:\n",
    "        assert len(new_target) == headlay.shape[0]\n",
    "    except:\n",
    "        print(new_target)\n",
    "        print(words)\n",
    "        print(headlay.shape)\n",
    "        print(len(words))\n",
    "        print(text)\n",
    "        break\n",
    "    target += new_target\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352307, 18)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markov/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 690/690 [00:06<00:00, 113.46it/s]\n"
     ]
    }
   ],
   "source": [
    "trial_data = np.array([])\n",
    "for text in tqdm(trial['text']):\n",
    "    heads, layers = get_attention_for_words(text, lern.model, tokenizer)\n",
    "    headlay = np.hstack((heads, layers))\n",
    "    if len(trial_data) == 0:\n",
    "        trial_data = headlay\n",
    "    else:\n",
    "        trial_data = np.vstack((trial_data, headlay))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = logreg.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_probas = logreg.predict_proba(data)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352307 0\n",
      "32225 1\n",
      "5145 2\n",
      "1979 3\n",
      "1075 4\n",
      "632 5\n",
      "372 6\n",
      "210 7\n",
      "102 8\n",
      "45 9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    train_pred = np.where(train_probas > i/10, 1, 0)\n",
    "    print(train_pred.sum(), i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24942"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29652, 18)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_probas = logreg.predict_proba(trial_data)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_probas = logreg.predict_proba(trial_data)[:, 1]\n",
    "trial_preds = np.where(trial_probas > 0.15, 1, 0)\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print(f1_score(trial_preds, trial_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "690it [00:00, 996.27it/s]\n"
     ]
    }
   ],
   "source": [
    "trial_target = []\n",
    "for span, text in tqdm(zip(trial['spans'], trial['text'])):\n",
    "    words, new_target = make_target(span, text, lern.model, tokenizer)\n",
    "    trial_target += new_target\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11862068965517242\n"
     ]
    }
   ],
   "source": [
    "trial_probas = logreg.predict_proba(trial_data)[:, 1]\n",
    "trial_preds = np.where(trial_probas > 0.22, 1, 0)\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print(f1_score(trial_preds, trial_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
