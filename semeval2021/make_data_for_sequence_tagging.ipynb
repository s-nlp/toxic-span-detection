{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = pd.read_csv(path + 'tsd_trial.csv')\n",
    "train = pd.read_csv(path + 'tsd_train.csv')\n",
    "\n",
    "train['spans'] = train.spans.apply(literal_eval)\n",
    "trial['spans'] = trial.spans.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getword(text, arr):\n",
    "    ans = ''\n",
    "    for i in range(len(arr)):\n",
    "        elem = arr[i]\n",
    "        if  i != 0 and i != len(arr) - 1 and elem != arr[i-1] + 1:\n",
    "            ans += ' '\n",
    "            ans += text[elem]\n",
    "        else:\n",
    "            ans += text[elem]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_whitespaces(text):\n",
    "    ans = []\n",
    "    counter = 0\n",
    "    for char in text:\n",
    "        if char == ' ' or char == '\\n':\n",
    "            counter += 1\n",
    "        elif counter != 0:\n",
    "            ans.append(counter)\n",
    "            counter = 0\n",
    "    return ans\n",
    "\n",
    "\n",
    "def cut_left_word_from_span(span):\n",
    "    prev_i = 0\n",
    "    for i in range(1, len(span)):\n",
    "        if span[i] - span[prev_i] > 1:\n",
    "            return span[i:]\n",
    "        prev_i = i\n",
    "    return []\n",
    "\n",
    "\n",
    "def label_sentence(text, span_arr):\n",
    "    whites = get_all_whitespaces(text)\n",
    "    whites.append(1)\n",
    "    sentence = text.split()\n",
    "    new_sentence = []\n",
    "    curr_position = 0\n",
    "    in_span = False\n",
    "    for i, word in enumerate(sentence):\n",
    "        if len(span_arr) != 0:\n",
    "            if not in_span and curr_position == span_arr[0]:\n",
    "                in_span = True\n",
    "                new_word = (word, 'I-SPAN')\n",
    "            elif in_span:\n",
    "                if curr_position in span_arr:\n",
    "                    new_word = (word, 'I-SPAN')\n",
    "                else:\n",
    "                    in_span = False\n",
    "                    span_arr = cut_left_word_from_span(span_arr)\n",
    "                    new_word = (word, '0')\n",
    "            else:\n",
    "                new_word = (word, 'O')\n",
    "            curr_position += len(word) + whites[i]\n",
    "        else:\n",
    "            new_word = (word, 'O')\n",
    "        new_sentence.append(new_word)\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conll_sentence(labeled_sentence):\n",
    "    ok_sentence = []\n",
    "    for elem in labeled_sentence:\n",
    "        new_word = []\n",
    "        word, label = elem\n",
    "        word_elems = word_tokenize(word)\n",
    "        for word_elem in word_elems:\n",
    "            if any(char.isalpha() for char in word_elem):\n",
    "                new_word.append((word_elem, label))\n",
    "            else:\n",
    "                new_word.append((word_elem, 'O'))\n",
    "        ok_sentence += new_word\n",
    "    return ok_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obamacare is on it's last gasping breaths.   You idiots who don't want something else passed that's going to save your free healthcare are kind of stupid.   But if you block anything else that gives you socialized healthcare it's your own fault for being unbudging.  You can go back to nothing when nothing further is passed and Obamacare is just an ugly memory.\n",
      "[49, 50, 51, 52, 53, 54, 147, 148, 149, 150, 151, 152]\n",
      "idiots stupid\n",
      "[('Obamacare', 'O'), ('is', 'O'), ('on', 'O'), (\"it's\", 'O'), ('last', 'O'), ('gasping', 'O'), ('breaths.', 'O'), ('You', 'O'), ('idiots', 'I-SPAN'), ('who', '0'), (\"don't\", 'O'), ('want', 'O'), ('something', 'O'), ('else', 'O'), ('passed', 'O'), (\"that's\", 'O'), ('going', 'O'), ('to', 'O'), ('save', 'O'), ('your', 'O'), ('free', 'O'), ('healthcare', 'O'), ('are', 'O'), ('kind', 'O'), ('of', 'O'), ('stupid.', 'I-SPAN'), ('But', '0'), ('if', 'O'), ('you', 'O'), ('block', 'O'), ('anything', 'O'), ('else', 'O'), ('that', 'O'), ('gives', 'O'), ('you', 'O'), ('socialized', 'O'), ('healthcare', 'O'), (\"it's\", 'O'), ('your', 'O'), ('own', 'O'), ('fault', 'O'), ('for', 'O'), ('being', 'O'), ('unbudging.', 'O'), ('You', 'O'), ('can', 'O'), ('go', 'O'), ('back', 'O'), ('to', 'O'), ('nothing', 'O'), ('when', 'O'), ('nothing', 'O'), ('further', 'O'), ('is', 'O'), ('passed', 'O'), ('and', 'O'), ('Obamacare', 'O'), ('is', 'O'), ('just', 'O'), ('an', 'O'), ('ugly', 'O'), ('memory.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "text = train.iloc[8, 1]\n",
    "span = train.iloc[8, 0]\n",
    "print(text, span, sep = \"\\n\")\n",
    "print(getword(text, span))\n",
    "test = label_sentence(text, span)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw_conll = []\n",
    "for i, text, span in zip(range(len(train)), train['text'], train['spans']):\n",
    "    try:\n",
    "        train_raw_conll.append(label_sentence(text, span))\n",
    "    except:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_conll = []\n",
    "for sent in train_raw_conll:\n",
    "    try:\n",
    "        train_conll.append(conll_sentence(sent))\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'O'), ('suppose', 'O'), ('the', 'O'), ('Russians', 'I-SPAN'), ('were', '0'), ('punished', 'O'), ('for', 'O'), ('Stalin', 'O'), (\"'s\", 'O'), ('misdeeds', 'O'), ('?', 'O'), ('Gosh', 'O'), ('you', 'O'), ('are', 'O'), ('a', 'O'), ('stupid', 'I-SPAN'), ('Dork', '0'), ('!', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(train_conll[125])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_raw_conll = []\n",
    "for i, text, span in zip(range(len(trial)), trial['text'], trial['spans']):\n",
    "    try:\n",
    "        trial_raw_conll.append(label_sentence(text, span))\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_conll = []\n",
    "for sent in trial_raw_conll:\n",
    "    try:\n",
    "        trial_conll.append(conll_sentence(sent))\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690\n"
     ]
    }
   ],
   "source": [
    "print(len(trial_conll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/trial.conll', 'w') as f:\n",
    "    for sentence in trial_conll:\n",
    "        f.write('-DOCSTART- O\\n')\n",
    "        f.write('\\n')\n",
    "        for comb in sentence:\n",
    "            f.write(comb[0] + ' ' + comb[1] + '\\n')\n",
    "        f.write('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/train.conll', 'w') as f:\n",
    "    for sentence in train_conll:\n",
    "        f.write('-DOCSTART- O\\n')\n",
    "        f.write('\\n')\n",
    "        for comb in sentence:\n",
    "            f.write(comb[0] + ' ' + comb[1] + '\\n')\n",
    "        f.write('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7939\n"
     ]
    }
   ],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
