{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: comparing classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def add_sys_path(p):\n",
    "    p = os.path.abspath(p)\n",
    "    print(p)\n",
    "    if p not in sys.path:\n",
    "        sys.path.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForTokenClassification, RobertaForSequenceClassification\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.file_utils import cached_property\n",
    "from typing import Tuple\n",
    "\n",
    "class TrAr(TrainingArguments):\n",
    "    @cached_property\n",
    "    def _setup_devices(self):\n",
    "        return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpansDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = self.labels[idx]\n",
    "        return {k: torch.tensor(v, dtype=torch.long) for k, v in item.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358984, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>259013</th>\n",
       "      <td>A tax on Rogers and Bell's services to pay Rog...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133153</th>\n",
       "      <td>I consider anyone on the right who freely deny...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350914</th>\n",
       "      <td>And perhaps another: \"Blessed are those who se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  label\n",
       "259013  A tax on Rogers and Bell's services to pay Rog...      0\n",
       "133153  I consider anyone on the right who freely deny...      1\n",
       "350914  And perhaps another: \"Blessed are those who se...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('../data/train/train.1.tsv', sep='\\t')\n",
    "df0 = pd.read_csv('../data/train/train_small.0.tsv', sep='\\t')\n",
    "df01 = pd.concat([df1, df0], ignore_index=True)\n",
    "df01.label = df01.label.astype(int)\n",
    "print(df01.shape)\n",
    "df01.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df01, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word-level classifier almost does not overfit, and scores 93.2% on the test set. \n",
    "\n",
    "The character-ngram classifier has some more learning capacity, and scores 93.7%.\n",
    "\n",
    "If we increase its vocabulary size, we get even better results of 94.2%.\n",
    "\n",
    "If we fine-tune a RoBERTa, we get 88% by tuning a head only, and 95% if we tune its body. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pipe = make_pipeline(HashingVectorizer(n_features=32_000), LogisticRegression(C=1, penalty='l2', max_iter=1_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pipe.fit(df_train.comment_text, df_train.label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9397186371671808\n"
     ]
    }
   ],
   "source": [
    "preds = word_pipe.predict_proba(df_train.comment_text)[:, 1]\n",
    "print(roc_auc_score(df_train.label, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9324057891122254\n"
     ]
    }
   ],
   "source": [
    "preds = word_pipe.predict_proba(df_test.comment_text)[:, 1]\n",
    "print(roc_auc_score(df_test.label, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9451532647499767\n",
      "0.9369443803559506\n"
     ]
    }
   ],
   "source": [
    "char_pipe = make_pipeline(HashingVectorizer(n_features=32_000, ngram_range=(3,6), analyzer='char_wb'), LogisticRegression(C=1, penalty='l2', max_iter=1_000))\n",
    "char_pipe.fit(df_train.comment_text, df_train.label);\n",
    "\n",
    "for table in df_train, df_test:\n",
    "    preds = char_pipe.predict_proba(table.comment_text)[:, 1]\n",
    "    print(roc_auc_score(table.label, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9498664523286193\n",
      "0.9416081650940398\n"
     ]
    }
   ],
   "source": [
    "char_pipe2 = make_pipeline(HashingVectorizer(n_features=128_000, ngram_range=(3,6), analyzer='char_wb'), LogisticRegression(C=1, penalty='l2', max_iter=1_000, n_jobs=32))\n",
    "char_pipe2.fit(df_train.comment_text, df_train.label);\n",
    "\n",
    "for table in df_train, df_test:\n",
    "    preds = char_pipe2.predict_proba(table.comment_text)[:, 1]\n",
    "    print(roc_auc_score(table.label, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A neural baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForTokenClassification, RobertaForSequenceClassification\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta-base' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_train_dataset = SpansDataset(\n",
    "    tokenizer(df_train.comment_text.tolist(), truncation=True), \n",
    "    df_train.label.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_test_dataset = SpansDataset(\n",
    "    tokenizer(df_test.comment_text.tolist(), truncation=True), \n",
    "    df_test.label.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3000\n",
    "clf_test_small_dataset = SpansDataset(\n",
    "    tokenizer(df_test.comment_text.iloc[:N].tolist(), truncation=True), \n",
    "    df_test.label[:N].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "clf = RobertaForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in clf.roberta.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([2, 768])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for param in clf.parameters():\n",
    "    if param.requires_grad:\n",
    "        print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='./models/roberta_classifier',   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,            # total # of training epochs\n",
    "    per_device_train_batch_size=64,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=100,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-8,              # strength of weight decay\n",
    "    learning_rate=1e-3,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=100,\n",
    "    eval_steps=100,\n",
    "    evaluation_strategy='steps',\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_metric(out):\n",
    "    return {'roc_auc': roc_auc_score(out.label_ids, out.predictions[:, 1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "esc = EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=clf,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=clf_train_dataset,         # training dataset\n",
    "    eval_dataset=clf_test_small_dataset,           # evaluation dataset\n",
    "    #data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=hf_metric,\n",
    "    callbacks=[esc],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='94' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [47/47 01:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.569843734247404}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_metric(trainer.predict(clf_test_small_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='700' max='5049' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 700/5049 06:44 < 42:02, 1.72 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.667800</td>\n",
       "      <td>0.606639</td>\n",
       "      <td>0.857506</td>\n",
       "      <td>15.969400</td>\n",
       "      <td>187.859000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.602500</td>\n",
       "      <td>0.510367</td>\n",
       "      <td>0.871650</td>\n",
       "      <td>16.054700</td>\n",
       "      <td>186.862000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.554100</td>\n",
       "      <td>0.512327</td>\n",
       "      <td>0.874013</td>\n",
       "      <td>16.069600</td>\n",
       "      <td>186.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.536200</td>\n",
       "      <td>0.450778</td>\n",
       "      <td>0.878355</td>\n",
       "      <td>16.080600</td>\n",
       "      <td>186.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.517400</td>\n",
       "      <td>0.455767</td>\n",
       "      <td>0.883892</td>\n",
       "      <td>16.075700</td>\n",
       "      <td>186.617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.531800</td>\n",
       "      <td>0.484408</td>\n",
       "      <td>0.885007</td>\n",
       "      <td>16.060900</td>\n",
       "      <td>186.789000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.533700</td>\n",
       "      <td>0.467164</td>\n",
       "      <td>0.886813</td>\n",
       "      <td>16.071200</td>\n",
       "      <td>186.670000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=700, training_loss=0.5633628300258091, metrics={'train_runtime': 405.3533, 'train_samples_per_second': 12.456, 'total_flos': 9488996164220160, 'epoch': 0.14})"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second stage of the training: the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in clf.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='./models/roberta_classifier',   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,            # total # of training epochs\n",
    "    per_device_train_batch_size=4,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=300,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-8,              # strength of weight decay\n",
    "    learning_rate=1e-5,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=300,\n",
    "    eval_steps=300,\n",
    "    evaluation_strategy='steps',\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='roc_auc',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "esc = EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=clf,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=clf_train_dataset,         # training dataset\n",
    "    eval_dataset=clf_test_small_dataset,           # evaluation dataset\n",
    "    #data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=hf_metric,\n",
    "    callbacks=[esc],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2100' max='80772' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2100/80772 05:28 < 3:25:17, 6.39 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.440800</td>\n",
       "      <td>0.680429</td>\n",
       "      <td>0.952350</td>\n",
       "      <td>16.145500</td>\n",
       "      <td>185.811000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.310700</td>\n",
       "      <td>0.707914</td>\n",
       "      <td>0.953171</td>\n",
       "      <td>16.086500</td>\n",
       "      <td>186.491000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.299900</td>\n",
       "      <td>1.009980</td>\n",
       "      <td>0.942094</td>\n",
       "      <td>16.082400</td>\n",
       "      <td>186.539000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.240500</td>\n",
       "      <td>0.902290</td>\n",
       "      <td>0.944559</td>\n",
       "      <td>16.086300</td>\n",
       "      <td>186.494000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.241100</td>\n",
       "      <td>0.737430</td>\n",
       "      <td>0.952941</td>\n",
       "      <td>16.088300</td>\n",
       "      <td>186.471000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>0.767514</td>\n",
       "      <td>0.947816</td>\n",
       "      <td>16.086900</td>\n",
       "      <td>186.487000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.911359</td>\n",
       "      <td>0.945745</td>\n",
       "      <td>16.109500</td>\n",
       "      <td>186.225000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2100, training_loss=0.2901053474062965, metrics={'train_runtime': 328.7329, 'train_samples_per_second': 245.707, 'total_flos': 876494967183360, 'epoch': 0.03})"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='561' max='561' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [561/561 03:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.9519475224270993}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_metric(trainer.predict(clf_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: show that tagger-classifier is a good classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tagger-like architecture seems to be more efficient than standard RobertaForSequenceClassification: with a smaller number of parameters, it scores 90% ROC AUC (vs 88% of baseline) when only the head is fine-tuned. \n",
    "* When the body is fine-tuned, it converges to the same 95% of ROC AUC as an ordinary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForTokenClassification, RobertaForSequenceClassification\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "class WMean(nn.Module):\n",
    "    def __init__(self, dim=-2):\n",
    "        super(WMean, self).__init__()\n",
    "        self.pow = torch.nn.Parameter(data=torch.Tensor([1.0]), requires_grad=True)\n",
    "        self.coef = torch.nn.Parameter(data=torch.Tensor([0.0, 1.0]), requires_grad=True)\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        result = x ** self.pow[0]\n",
    "        if mask is None:\n",
    "            mp = result.mean(dim=-1)\n",
    "        else:\n",
    "            mp = (result * mask).sum(dim=self.dim) / mask.sum(dim=self.dim)\n",
    "        return torch.log(mp) * self.coef[1] + self.coef[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaTaggerClassifier(RobertaForTokenClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.wmean = WMean()\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        token_logits = self.classifier(sequence_output)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            masks = attention_mask.unsqueeze(-1).repeat(1, 1, 2)\n",
    "        else:\n",
    "            masks = None\n",
    "\n",
    "        logits = self.wmean(torch.softmax(token_logits, dim=-1), mask=masks)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaTaggerClassifier: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaTaggerClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaTaggerClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaTaggerClassifier were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'wmean.pow', 'wmean.coef']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "clf = RobertaTaggerClassifier.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in clf.roberta.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768])\n",
      "torch.Size([2])\n",
      "torch.Size([1])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for param in clf.parameters():\n",
    "    if param.requires_grad:\n",
    "        print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='./models/roberta_wm_classifier',   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,            # total # of training epochs\n",
    "    per_device_train_batch_size=64,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=100,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-8,              # strength of weight decay\n",
    "    learning_rate=1e-3,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=100,\n",
    "    eval_steps=100,\n",
    "    evaluation_strategy='steps',\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='roc_auc',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_metric(out):\n",
    "    return {'roc_auc': roc_auc_score(out.label_ids, out.predictions[:, 1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "esc = EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=clf,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=clf_train_dataset,         # training dataset\n",
    "    eval_dataset=clf_test_small_dataset,           # evaluation dataset\n",
    "    #data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=hf_metric,\n",
    "    callbacks=[esc],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='94' max='47' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [47/47 01:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.5387299346932374}"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_metric(trainer.predict(clf_test_small_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's surprising, but this simple trainer seems to be just unable to converge. Why? Is it going asymptotic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2800' max='5049' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2800/5049 27:25 < 22:02, 1.70 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.657000</td>\n",
       "      <td>0.594520</td>\n",
       "      <td>0.862524</td>\n",
       "      <td>15.926100</td>\n",
       "      <td>188.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.561300</td>\n",
       "      <td>0.505007</td>\n",
       "      <td>0.876173</td>\n",
       "      <td>16.024600</td>\n",
       "      <td>187.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.503700</td>\n",
       "      <td>0.469456</td>\n",
       "      <td>0.878648</td>\n",
       "      <td>16.079300</td>\n",
       "      <td>186.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.476200</td>\n",
       "      <td>0.461533</td>\n",
       "      <td>0.884187</td>\n",
       "      <td>16.091600</td>\n",
       "      <td>186.432000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.461700</td>\n",
       "      <td>0.457068</td>\n",
       "      <td>0.887382</td>\n",
       "      <td>16.086800</td>\n",
       "      <td>186.488000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.462000</td>\n",
       "      <td>0.435209</td>\n",
       "      <td>0.885204</td>\n",
       "      <td>16.093000</td>\n",
       "      <td>186.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.462700</td>\n",
       "      <td>0.429421</td>\n",
       "      <td>0.888364</td>\n",
       "      <td>16.091000</td>\n",
       "      <td>186.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.458700</td>\n",
       "      <td>0.426974</td>\n",
       "      <td>0.889986</td>\n",
       "      <td>16.092800</td>\n",
       "      <td>186.419000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.445800</td>\n",
       "      <td>0.431917</td>\n",
       "      <td>0.893396</td>\n",
       "      <td>16.079900</td>\n",
       "      <td>186.568000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.446400</td>\n",
       "      <td>0.420964</td>\n",
       "      <td>0.894024</td>\n",
       "      <td>16.084700</td>\n",
       "      <td>186.513000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.419407</td>\n",
       "      <td>0.895713</td>\n",
       "      <td>16.084200</td>\n",
       "      <td>186.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.444900</td>\n",
       "      <td>0.424258</td>\n",
       "      <td>0.897549</td>\n",
       "      <td>16.072800</td>\n",
       "      <td>186.651000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.438900</td>\n",
       "      <td>0.414858</td>\n",
       "      <td>0.896792</td>\n",
       "      <td>16.094600</td>\n",
       "      <td>186.398000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.444600</td>\n",
       "      <td>0.412617</td>\n",
       "      <td>0.896880</td>\n",
       "      <td>16.076700</td>\n",
       "      <td>186.605000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.434700</td>\n",
       "      <td>0.415597</td>\n",
       "      <td>0.898355</td>\n",
       "      <td>16.085000</td>\n",
       "      <td>186.509000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.440600</td>\n",
       "      <td>0.417389</td>\n",
       "      <td>0.900375</td>\n",
       "      <td>16.076400</td>\n",
       "      <td>186.609000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.444400</td>\n",
       "      <td>0.408979</td>\n",
       "      <td>0.899366</td>\n",
       "      <td>16.093400</td>\n",
       "      <td>186.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.446400</td>\n",
       "      <td>0.412802</td>\n",
       "      <td>0.900311</td>\n",
       "      <td>16.094700</td>\n",
       "      <td>186.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.442000</td>\n",
       "      <td>0.409321</td>\n",
       "      <td>0.899123</td>\n",
       "      <td>16.080100</td>\n",
       "      <td>186.566000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.439600</td>\n",
       "      <td>0.408950</td>\n",
       "      <td>0.899857</td>\n",
       "      <td>16.086200</td>\n",
       "      <td>186.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.439700</td>\n",
       "      <td>0.410299</td>\n",
       "      <td>0.900127</td>\n",
       "      <td>16.081100</td>\n",
       "      <td>186.555000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.439300</td>\n",
       "      <td>0.408199</td>\n",
       "      <td>0.899725</td>\n",
       "      <td>16.082100</td>\n",
       "      <td>186.543000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.438200</td>\n",
       "      <td>0.409382</td>\n",
       "      <td>0.901219</td>\n",
       "      <td>16.081600</td>\n",
       "      <td>186.549000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.434300</td>\n",
       "      <td>0.410466</td>\n",
       "      <td>0.901461</td>\n",
       "      <td>16.082700</td>\n",
       "      <td>186.536000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.431900</td>\n",
       "      <td>0.406704</td>\n",
       "      <td>0.901031</td>\n",
       "      <td>16.082800</td>\n",
       "      <td>186.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.435400</td>\n",
       "      <td>0.411532</td>\n",
       "      <td>0.901530</td>\n",
       "      <td>16.083700</td>\n",
       "      <td>186.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.441800</td>\n",
       "      <td>0.408845</td>\n",
       "      <td>0.901609</td>\n",
       "      <td>16.078800</td>\n",
       "      <td>186.581000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.452100</td>\n",
       "      <td>0.408111</td>\n",
       "      <td>0.901959</td>\n",
       "      <td>16.078400</td>\n",
       "      <td>186.586000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2800, training_loss=0.4597941248757499, metrics={'train_runtime': 1646.2604, 'train_samples_per_second': 3.067, 'total_flos': 38546066886201600, 'epoch': 0.55})"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second stage of the training: the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in clf.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='./models/roberta_wm_classifier',   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,            # total # of training epochs\n",
    "    per_device_train_batch_size=4,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=300,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-8,              # strength of weight decay\n",
    "    learning_rate=1e-5,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=300,\n",
    "    eval_steps=300,\n",
    "    evaluation_strategy='steps',\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='roc_auc',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "esc = EarlyStoppingCallback(early_stopping_patience=10, early_stopping_threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=clf,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=clf_train_dataset,         # training dataset\n",
    "    eval_dataset=clf_test_small_dataset,           # evaluation dataset\n",
    "    #data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=hf_metric,\n",
    "    callbacks=[esc],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='6000' max='80772' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6000/80772 15:45 < 3:16:29, 6.34 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.530700</td>\n",
       "      <td>0.440437</td>\n",
       "      <td>0.916096</td>\n",
       "      <td>16.182700</td>\n",
       "      <td>185.383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.629700</td>\n",
       "      <td>0.675921</td>\n",
       "      <td>0.936811</td>\n",
       "      <td>16.076600</td>\n",
       "      <td>186.607000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.617700</td>\n",
       "      <td>0.417694</td>\n",
       "      <td>0.941973</td>\n",
       "      <td>16.071100</td>\n",
       "      <td>186.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.566200</td>\n",
       "      <td>0.631325</td>\n",
       "      <td>0.932847</td>\n",
       "      <td>16.083100</td>\n",
       "      <td>186.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.625600</td>\n",
       "      <td>0.569090</td>\n",
       "      <td>0.938906</td>\n",
       "      <td>16.083500</td>\n",
       "      <td>186.527000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.571000</td>\n",
       "      <td>0.441794</td>\n",
       "      <td>0.944334</td>\n",
       "      <td>16.088000</td>\n",
       "      <td>186.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.568800</td>\n",
       "      <td>0.556021</td>\n",
       "      <td>0.930812</td>\n",
       "      <td>16.079300</td>\n",
       "      <td>186.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.607300</td>\n",
       "      <td>0.455331</td>\n",
       "      <td>0.939510</td>\n",
       "      <td>16.076000</td>\n",
       "      <td>186.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.645248</td>\n",
       "      <td>0.949999</td>\n",
       "      <td>16.089400</td>\n",
       "      <td>186.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.522400</td>\n",
       "      <td>0.425579</td>\n",
       "      <td>0.952864</td>\n",
       "      <td>16.099200</td>\n",
       "      <td>186.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.535800</td>\n",
       "      <td>0.413904</td>\n",
       "      <td>0.952372</td>\n",
       "      <td>16.086500</td>\n",
       "      <td>186.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.523000</td>\n",
       "      <td>0.421050</td>\n",
       "      <td>0.949880</td>\n",
       "      <td>16.081100</td>\n",
       "      <td>186.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.457500</td>\n",
       "      <td>0.573106</td>\n",
       "      <td>0.939232</td>\n",
       "      <td>16.089500</td>\n",
       "      <td>186.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.557200</td>\n",
       "      <td>0.522963</td>\n",
       "      <td>0.941861</td>\n",
       "      <td>16.099400</td>\n",
       "      <td>186.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.486200</td>\n",
       "      <td>0.466833</td>\n",
       "      <td>0.946912</td>\n",
       "      <td>16.090700</td>\n",
       "      <td>186.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.498200</td>\n",
       "      <td>0.483894</td>\n",
       "      <td>0.945516</td>\n",
       "      <td>16.084600</td>\n",
       "      <td>186.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.489000</td>\n",
       "      <td>0.653770</td>\n",
       "      <td>0.945232</td>\n",
       "      <td>16.108100</td>\n",
       "      <td>186.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.521700</td>\n",
       "      <td>0.488527</td>\n",
       "      <td>0.949903</td>\n",
       "      <td>16.101600</td>\n",
       "      <td>186.317000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.477400</td>\n",
       "      <td>0.499614</td>\n",
       "      <td>0.941146</td>\n",
       "      <td>16.094500</td>\n",
       "      <td>186.399000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.504700</td>\n",
       "      <td>0.527589</td>\n",
       "      <td>0.947760</td>\n",
       "      <td>16.093000</td>\n",
       "      <td>186.416000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6000, training_loss=0.5388713582356771, metrics={'train_runtime': 945.9142, 'train_samples_per_second': 85.39, 'total_flos': 2464073343886176, 'epoch': 0.07})"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='561' max='561' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [561/561 03:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.9516783318837393}"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_metric(trainer.predict(clf_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting scores are very, very strange.  \n",
    "\n",
    "It seems that we do really need multitasking, because without taging superivision at all, we get trash results with e.g punctuation being the most extremely rated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([1.8128], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.8515, 1.8189], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in clf.wmean.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spans_utils\n",
    "import matplotlib.cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: #4949ff\">The</span><span style=\"background-color: #6161ff\"> bears</span><span style=\"background-color: #4c4cff\"> are</span><span style=\"background-color: #4c4cff\"> losing</span><span style=\"background-color: #3e3eff\"> money</span><span style=\"background-color: #4646ff\"> trying</span><span style=\"background-color: #5151ff\"> to</span><span style=\"background-color: #8383ff\"> predict</span><span style=\"background-color: #9292ff\"> the</span><span style=\"background-color: #a3a3ff\"> next</span><span style=\"background-color: #a8a8ff\"> stock</span><span style=\"background-color: #c3c3ff\"> market</span><span style=\"background-color: #6161ff\"> crash</span><span style=\"background-color: #ff0000\">.</span><span style=\"background-color: #4040ff\"> And</span><span style=\"background-color: #6161ff\"> just</span><span style=\"background-color: #5959ff\"> because</span><span style=\"background-color: #8282ff\"> there</span><span style=\"background-color: #a2a2ff\"> was</span><span style=\"background-color: #bcbcff\"> a</span><span style=\"background-color: #9898ff\"> bad</span><span style=\"background-color: #8282ff\"> day</span><span style=\"background-color: #8e8eff\"> thirty</span><span style=\"background-color: #7c7cff\"> years</span><span style=\"background-color: #8c8cff\"> ago</span><span style=\"background-color: #5858ff\"> is</span><span style=\"background-color: #4848ff\"> meaningless</span><span style=\"background-color: #5e5eff\"> since</span><span style=\"background-color: #fffcfc\"> past</span><span style=\"background-color: #b8b8ff\"> behavior</span><span style=\"background-color: #8383ff\"> is</span><span style=\"background-color: #7e7eff\"> not</span><span style=\"background-color: #b0b0ff\"> a</span><span style=\"background-color: #8c8cff\"> reliable</span><span style=\"background-color: #9c9cff\"> figure</span><span style=\"background-color: #b6b6ff\"> to</span><span style=\"background-color: #ff6868\"> predict</span><span style=\"background-color: #ffcccc\"> existing</span><span style=\"background-color: #ffdada\"> and</span><span style=\"background-color: #ff4646\"> future</span><span style=\"background-color: #ffe8e8\"> behavior</span><span style=\"background-color: #ff0000\">.</span><span style=\"background-color: #3e3eff\"> </span><span style=\"background-color: #2424ff\">Ċ</span><span style=\"background-color: #2c2cff\">Ċ</span><span style=\"background-color: #d8d8ff\">With</span><span style=\"background-color: #fff6f6\"> the</span><span style=\"background-color: #a3a3ff\"> Federal</span><span style=\"background-color: #f2f2ff\"> Reserve</span><span style=\"background-color: #ff0c0c\"> raising</span><span style=\"background-color: #ff8888\"> interest</span><span style=\"background-color: #ffa2a2\"> rates</span><span style=\"background-color: #ffdcdc\">,</span><span style=\"background-color: #ffc3c3\"> bonds</span><span style=\"background-color: #ffc2c2\"> and</span><span style=\"background-color: #ffb3b3\"> cash</span><span style=\"background-color: #9090ff\"> generally</span><span style=\"background-color: #e3e3ff\"> do</span><span style=\"background-color: #d0d0ff\"> not</span><span style=\"background-color: #ffa6a6\"> perform</span><span style=\"background-color: #a6a6ff\"> as</span><span style=\"background-color: #babaff\"> well</span><span style=\"background-color: #ff5454\"> as</span><span style=\"background-color: #fff6f6\"> stocks</span><span style=\"background-color: #ff0000\">.</span><span style=\"background-color: #ffecec\"> And</span><span style=\"background-color: #ff0000\"> the</span><span style=\"background-color: #ff5656\"> markets</span><span style=\"background-color: #ffeeee\"> tend</span><span style=\"background-color: #ff6e6e\"> to</span><span style=\"background-color: #ff0000\"> do</span><span style=\"background-color: #ff6666\"> well</span><span style=\"background-color: #ff0000\">.</span><span style=\"background-color: #ff4949\"> It</span><span style=\"background-color: #ff3434\"> is</span><span style=\"background-color: #ff6666\"> not</span><span style=\"background-color: #ff7676\"> feasible</span><span style=\"background-color: #ff9898\"> to</span><span style=\"background-color: #ff0000\"> predict</span><span style=\"background-color: #ff0000\"> how</span><span style=\"background-color: #ff0000\"> well</span><span style=\"background-color: #ffcccc\">.</span><span style=\"background-color: #ff0000\"> Maybe</span><span style=\"background-color: #ff0000\"> the</span><span style=\"background-color: #ff0000\"> markets</span><span style=\"background-color: #ff0000\"> will</span><span style=\"background-color: #ff0000\"> slow</span><span style=\"background-color: #ff0000\"> down</span><span style=\"background-color: #e8e8ff\">.</span><span style=\"background-color: #fffcfc\"> Or</span><span style=\"background-color: #ff2c2c\"> maybe</span><span style=\"background-color: #ff4141\"> not</span><span style=\"background-color: #ff0000\">.</span><span style=\"background-color: #ff0000\"> It</span><span style=\"background-color: #ff0000\"> may</span><span style=\"background-color: #ff0000\"> take</span><span style=\"background-color: #ff0000\"> many</span><span style=\"background-color: #ff0000\"> years</span><span style=\"background-color: #ff0000\"> before</span><span style=\"background-color: #ff0000\"> interest</span><span style=\"background-color: #ff0000\"> rates</span><span style=\"background-color: #ff0000\"> rise</span><span style=\"background-color: #ff0c0c\"> quite</span><span style=\"background-color: #ff0000\"> significantly</span><span style=\"background-color: #ff0000\">.</span><span style=\"background-color: #acacff\"> </span><span style=\"background-color: #2424ff\">Ċ</span><span style=\"background-color: #2424ff\">Ċ</span><span style=\"background-color: #9a9aff\">And</span><span style=\"background-color: #e8e8ff\"> it</span><span style=\"background-color: #fffefe\"> is</span><span style=\"background-color: #bebeff\"> not</span><span style=\"background-color: #d2d2ff\"> clear</span><span style=\"background-color: #f2f2ff\"> that</span><span style=\"background-color: #ff6464\"> the</span><span style=\"background-color: #fff6f6\"> Federal</span><span style=\"background-color: #ff9898\"> Reserve</span><span style=\"background-color: #ffeaea\"> can</span><span style=\"background-color: #ff0000\"> raise</span><span style=\"background-color: #ff0000\"> rates</span><span style=\"background-color: #ff1010\"> a</span><span style=\"background-color: #fff8f8\"> huge</span><span style=\"background-color: #ffd2d2\"> amount</span><span style=\"background-color: #ffc8c8\"> without</span><span style=\"background-color: #ffdcdc\"> creating</span><span style=\"background-color: #ff9898\"> economic</span><span style=\"background-color: #ffe2e2\"> problems</span><span style=\"background-color: #ff0000\">.</span><span style=\"background-color: #9898ff\"> The</span><span style=\"background-color: #a0a0ff\"> Federal</span><span style=\"background-color: #eaeaff\"> Government</span><span style=\"background-color: #ff8282\"> has</span><span style=\"background-color: #ffaaaa\"> a</span><span style=\"background-color: #ffc6c6\"> large</span><span style=\"background-color: #ff0000\"> debt</span><span style=\"background-color: #c2c2ff\"> and</span><span style=\"background-color: #fffcfc\"> causing</span><span style=\"background-color: #ff8e8e\"> a</span><span style=\"background-color: #fffafa\"> collapse</span><span style=\"background-color: #ff9e9e\"> in</span><span style=\"background-color: #f2f2ff\"> people</span><span style=\"background-color: #ff6969\"> buying</span><span style=\"background-color: #ffaeae\"> housing</span><span style=\"background-color: #ffcece\">,</span><span style=\"background-color: #fff0f0\"> cars</span><span style=\"background-color: #7878ff\">,</span><span style=\"background-color: #d8d8ff\"> and</span><span style=\"background-color: #c0c0ff\"> other</span><span style=\"background-color: #ffe2e2\"> items</span><span style=\"background-color: #d0d0ff\"> will</span><span style=\"background-color: #a3a3ff\"> lead</span><span style=\"background-color: #f0f0ff\"> to</span><span style=\"background-color: #ffc2c2\"> a</span><span style=\"background-color: #ff3636\"> recession</span><span style=\"background-color: #1616ff\">.</span><span style=\"background-color: #2626ff\"> Trump</span><span style=\"background-color: #1616ff\"> and</span><span style=\"background-color: #1616ff\"> the</span><span style=\"background-color: #1212ff\"> other</span><span style=\"background-color: #1414ff\"> idiots</span><span style=\"background-color: #1616ff\"> in</span><span style=\"background-color: #2626ff\"> Washington</span><span style=\"background-color: #1e1eff\"> have</span><span style=\"background-color: #2020ff\"> failed</span><span style=\"background-color: #2c2cff\"> to</span><span style=\"background-color: #9898ff\"> invest</span><span style=\"background-color: #b0b0ff\"> in</span><span style=\"background-color: #babaff\"> infrastructure</span><span style=\"background-color: #4848ff\"> and</span><span style=\"background-color: #5e5eff\"> are</span><span style=\"background-color: #6666ff\"> proposing</span><span style=\"background-color: #f3f3ff\"> giving</span><span style=\"background-color: #5e5eff\"> rich</span><span style=\"background-color: #6969ff\"> people</span><span style=\"background-color: #7979ff\"> tax</span><span style=\"background-color: #8c8cff\"> cuts</span><span style=\"background-color: #d8d8ff\"> which</span><span style=\"background-color: #dcdcff\"> will</span><span style=\"background-color: #d8d8ff\"> significantly</span><span style=\"background-color: #ff1818\"> in</span><span style=\"background-color: #c0c0ff\">ce</span><span style=\"background-color: #ff0000\">ase</span><span style=\"background-color: #ff0000\"> the</span><span style=\"background-color: #ff0000\"> debt</span><span style=\"background-color: #ff0000\">.</span><span style=\"background-color: #0e0eff\"> They</span><span style=\"background-color: #0e0eff\"> are</span><span style=\"background-color: #1818ff\"> incompetent</span><span style=\"background-color: #ff0000\">.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = df_test[df_test.label==1].sample(1).iloc[0].comment_text    \n",
    "inputs = {k: v.to(device) for k, v in tokenizer(text, return_tensors='pt').items()}\n",
    "\n",
    "clf.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = clf.classifier(clf.dropout(clf.roberta(**inputs)[0]))\n",
    "    \n",
    "scores = preds[0].softmax(-1)[:, 0]\n",
    "scores = scores[1:-1].cpu().numpy()\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "spans_utils.display_token_scores(tokens, scores / np.median(scores) * 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: try using a tagger as a classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея: посмотреть, насколько теггер токсичных спанов (из задачи https://www.sites.google.com/view/toxicspans) способен решить задачу классификации комментариев. \n",
    "\n",
    "Мы уже пробовали трансферить классификацию в тегирование, и получилось так себе. Очень интересно, будет ли работать в другую сторону. \n",
    "\n",
    "Если тоже нет, это значит, что, скорее всего, определение токсичности в Jigsaw и в toxicspans всё-таки существенно различается. Потому что сам по себе теггер перформит неплохо. \n",
    "\n",
    "С другой стороны, теггер никогда не видел нетоксичных текстов, так что с чего бы ему вообще что-то знать про нетоксичность?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../semeval2021/models/roberta_clf_ft'\n",
    "spans_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "spans_tagger = AutoModelForTokenClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spans_tagger.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35899, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_test_dataset = SpansDataset(\n",
    "    spans_tokenizer(df_test.comment_text.tolist(), truncation=True), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,  8346,     6,    89,    21,     5, 23629, 12968,   631,     4,\n",
       "         12526,  1636,  6465,  5726,     6,    54,   393, 11184,    13,    24,\n",
       "            25,   444,    25,    38,   216,     4,  1437, 50140, 50118,  2409,\n",
       "         11110,     5, 23629, 12968,   631,    36,  1610, 32565,    10,   367,\n",
       "           728,     7,   356,    88,    24,     6,  2085,    43,    16,  7159,\n",
       "             7,     5,  2731,     4,     2]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='tmp',\n",
    "    per_device_eval_batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=spans_tagger,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=spans_tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='561' max='561' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [561/561 03:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(clf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = np.exp(preds.predictions)\n",
    "proba /= proba.sum(-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2702874274764a5ea2be775aa00f734e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35899.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_scores = np.array([pr[:obs['input_ids'].shape[0], 1].max() for obs, pr in zip(tqdm(clf_test_dataset), proba)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores = [pr[:obs['input_ids'].shape[0], 1].mean() for obs, pr in zip(clf_test_dataset, proba)]\n",
    "sum_scores = [pr[:obs['input_ids'].shape[0], 1].sum() for obs, pr in zip(clf_test_dataset, proba)]\n",
    "\n",
    "logmean_scores = [pr[:obs['input_ids'].shape[0], 1].mean() for obs, pr in zip(clf_test_dataset, np.log(proba))]\n",
    "logsum_scores = [pr[:obs['input_ids'].shape[0], 1].sum() for obs, pr in zip(clf_test_dataset, np.log(proba))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max\t 0.9284224310789024\n",
      "mean\t 0.6688997908411966\n",
      "sum\t 0.6664518174212648\n",
      "logmean\t 0.3314395081219059\n",
      "logsum\t 0.47393828428693896\n"
     ]
    }
   ],
   "source": [
    "print('max\\t', roc_auc_score(df_test.label, max_scores))\n",
    "print('mean\\t', roc_auc_score(df_test.label, mean_scores))\n",
    "print('sum\\t', roc_auc_score(df_test.label, sum_scores))\n",
    "print('logmean\\t', roc_auc_score(df_test.label, logmean_scores))\n",
    "print('logsum\\t', roc_auc_score(df_test.label, logsum_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оказывается, у теггера приличные скоры: 92.8%, против 95.2% у нейроночного классификатора. Оценивать самый токсичный токен - самое здравое. \n",
    "\n",
    "Если cкладывать скоры в очень высокой степени (типа 30), получаем тоже хорошие результаты, но число математически это уже практически максимум. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 \t 0.4655723488480239\n",
      "1 \t 0.6688997908411966\n",
      "2 \t 0.8806614007404695\n",
      "3 \t 0.907694938501369\n",
      "10 \t 0.9253033132240656\n",
      "20 \t 0.9273417204420338\n",
      "30 \t 0.9278425415934849\n",
      "50 \t 0.927552451397623\n",
      "100 \t 0.9130072398811575\n",
      "1000 \t 0.6214589697223271\n"
     ]
    }
   ],
   "source": [
    "for p in [0.5, 1, 2, 3, 10, 20, 30, 50, 100, 1000]:\n",
    "    pmean_scores = [(pr[:obs['input_ids'].shape[0], 1]**p).mean() for obs, pr in zip(clf_test_dataset, proba)]\n",
    "    print(p, '\\t', roc_auc_score(df_test.label, pmean_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 \t 0.44866954771466305\n",
      "1 \t 0.6664518174212648\n",
      "2 \t 0.9066887454803618\n",
      "3 \t 0.921574869310578\n",
      "10 \t 0.9282860526301903\n",
      "20 \t 0.9284209891271048\n",
      "30 \t 0.9284236874796511\n",
      "50 \t 0.9280216125081837\n",
      "100 \t 0.9146362051220178\n",
      "1000 \t 0.625298624757893\n"
     ]
    }
   ],
   "source": [
    "for p in [0.5, 1, 2, 3, 10, 20, 30, 50, 100, 1000]:\n",
    "    pmean_scores = [(pr[:obs['input_ids'].shape[0], 1]**p).sum() for obs, pr in zip(clf_test_dataset, proba)]\n",
    "    print(p, '\\t', roc_auc_score(df_test.label, pmean_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dale/p3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_test['pred_score'] = max_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test[df_test.label==0].pred_score.hist();\n",
    "df_test[df_test.label==1].pred_score.hist(alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>label</th>\n",
       "      <th>pred_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181880</th>\n",
       "      <td>This is insane.....</td>\n",
       "      <td>0</td>\n",
       "      <td>0.893508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244421</th>\n",
       "      <td>Yet you don't see a problem with murdering an American serviceman.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.863068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213228</th>\n",
       "      <td>sustaineugenedotorg.....Hey, quit being so damn negative.  Just wait...when the economic  \"trickle-down\" starts you will be a happy camper.  Don't be discouraged that it's never worked in the past.....don't be distracted by the idea that \"the best predictor or future behavior is past behavior\".......don't be upset the Donald's selecting other billionaires for his cabinet.....after all, being at the \"center\" of an Amway pyramid is indicative of having a real dedication to improving public edu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.863382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250723</th>\n",
       "      <td>Tell him that you and the kids will miss him. He has a right to carry the gun openly (assuming the state/county allows this) but your role is keeping your kids safe. Ask your husband to get on-board.  If he doesn't, when the BIL comes over, you and kids need to leave. This is non-negotiable. Every.damn.day a child is seriously hurt or killed by a gun.  I have friends who have guns in their home, and they know not to even ask if I want to come over to their house.   And tell him what I told m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.828627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216656</th>\n",
       "      <td>all the sources to eusebius and africanus and Septuagint are all here in wikipedia and apparently youre too stupid to link it as you read it or too lazy to go look thru Wikipedia to find it and see its there. You know-it-alls are like those who negligently built the dykes of new orleans, you should die with the victims who did in new orleans when you negligenlty take any structural matter into your hands, your wives and children should right before your eyes due to the evil in your atitude t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.893790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314826</th>\n",
       "      <td>That's how the media tries to make a fool of us sometimes. It doesn't even add up.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.839540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355966</th>\n",
       "      <td>\"Greatest traitor\"  This is the most appalling letter I've ever read in the RG. It's equal to the hateful tripe Booth wrote about Lincoln.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.811896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277855</th>\n",
       "      <td>Character develops over time. Can the shrinks change it? I doubt it. Fear of certain punishment prevents most crime.  Prisons should be more miserable and then life there should be shown per videos in schools.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.812131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191385</th>\n",
       "      <td>Not me. I don't want to get high. I don't want tobacco smoke or dope smoke, because I'm a cancer survivor. Enforce the by-laws and I don't have a problem. Other than certain BC bud stinks like a skunk.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.823896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206100</th>\n",
       "      <td>leave it to Kathy to spend more money, she has no idea how to stop spending money that's not hers. What does she care, certainly not about her tax paying constituents.\\nWhat a waste of skin.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.853846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               comment_text  \\\n",
       "181880                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This is insane.....   \n",
       "244421                                                                                                                                                                                                                                                                                                                                                                                                                                                   Yet you don't see a problem with murdering an American serviceman.   \n",
       "213228  sustaineugenedotorg.....Hey, quit being so damn negative.  Just wait...when the economic  \"trickle-down\" starts you will be a happy camper.  Don't be discouraged that it's never worked in the past.....don't be distracted by the idea that \"the best predictor or future behavior is past behavior\".......don't be upset the Donald's selecting other billionaires for his cabinet.....after all, being at the \"center\" of an Amway pyramid is indicative of having a real dedication to improving public edu...   \n",
       "250723  Tell him that you and the kids will miss him. He has a right to carry the gun openly (assuming the state/county allows this) but your role is keeping your kids safe. Ask your husband to get on-board.  If he doesn't, when the BIL comes over, you and kids need to leave. This is non-negotiable. Every.damn.day a child is seriously hurt or killed by a gun.  I have friends who have guns in their home, and they know not to even ask if I want to come over to their house.   And tell him what I told m...   \n",
       "216656  all the sources to eusebius and africanus and Septuagint are all here in wikipedia and apparently youre too stupid to link it as you read it or too lazy to go look thru Wikipedia to find it and see its there. You know-it-alls are like those who negligently built the dykes of new orleans, you should die with the victims who did in new orleans when you negligenlty take any structural matter into your hands, your wives and children should right before your eyes due to the evil in your atitude t...   \n",
       "314826                                                                                                                                                                                                                                                                                                                                                                                                                                   That's how the media tries to make a fool of us sometimes. It doesn't even add up.   \n",
       "355966                                                                                                                                                                                                                                                                                                                                                                           \"Greatest traitor\"  This is the most appalling letter I've ever read in the RG. It's equal to the hateful tripe Booth wrote about Lincoln.   \n",
       "277855                                                                                                                                                                                                                                                                                                    Character develops over time. Can the shrinks change it? I doubt it. Fear of certain punishment prevents most crime.  Prisons should be more miserable and then life there should be shown per videos in schools.   \n",
       "191385                                                                                                                                                                                                                                                                                                            Not me. I don't want to get high. I don't want tobacco smoke or dope smoke, because I'm a cancer survivor. Enforce the by-laws and I don't have a problem. Other than certain BC bud stinks like a skunk.   \n",
       "206100                                                                                                                                                                                                                                                                                                                       leave it to Kathy to spend more money, she has no idea how to stop spending money that's not hers. What does she care, certainly not about her tax paying constituents.\\nWhat a waste of skin.   \n",
       "\n",
       "        label  pred_score  \n",
       "181880      0    0.893508  \n",
       "244421      0    0.863068  \n",
       "213228      0    0.863382  \n",
       "250723      0    0.828627  \n",
       "216656      0    0.893790  \n",
       "314826      0    0.839540  \n",
       "355966      0    0.811896  \n",
       "277855      0    0.812131  \n",
       "191385      0    0.823896  \n",
       "206100      0    0.853846  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[(df_test.label==0) & (df_test.pred_score > 0.8)].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>label</th>\n",
       "      <th>pred_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95666</th>\n",
       "      <td>Do you have \"free speech\" on G%M? Which drug are you taking?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.120124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128576</th>\n",
       "      <td>You are attempting to use logic and reason on folks who possess neither.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95355</th>\n",
       "      <td>No, I don't suppose Trump would resign -- he's far too selfish for that; of course, selfishness could lead him to resign if he came to believe that doing so would be in his own best interest. As I've said, I have very mixed feelings about it: I think the country needs the transparency of the impeachment process. I hope there are some people who love or care for Trump who also love and care about the country; but I'm not sure who they are or whether they love the country enough to tell Trump ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.194154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118639</th>\n",
       "      <td>Sounds like the analysis of every pro sports team that has been in futility, toiling away for decades....they are just one tiny tweek,  one small change in the playbook, or just one small trade for an inconsequential player away from perfection, and eternal championships...\\n\\nNot gonna happen.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.161133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>u aint got no pancake mix lady so stopppp</td>\n",
       "      <td>1</td>\n",
       "      <td>0.140863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97750</th>\n",
       "      <td>Yes. Rationalizing your  jealousy ? Then become a doctor. Or Serve the public or stop your whining because others are doing better than you because they earned it.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.170173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129596</th>\n",
       "      <td>A 60s to 70s year-old person, participating in a 2.384 mile-race, has an empty coconut for a head.\\nLike the song goes:\\nPlenty pupule it's easy to see.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.123686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158826</th>\n",
       "      <td>Poor Steve.  Writing comments in the local paper where one lives  is not trolling, it's citizenship.\\n\\nBut writing the same few comments over and over again, hundreds of times a day, in fora all over the English-speaking world, but only on one topic  --  THAT's trolling!  Got a mirror?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.171609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30426</th>\n",
       "      <td>We need a fence taller than the Mexican fence because Canadians on average are taller</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9591</th>\n",
       "      <td>wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.184638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               comment_text  \\\n",
       "95666                                                                                                                                                                                                                                                                                                                                                                                                                                                          Do you have \"free speech\" on G%M? Which drug are you taking?   \n",
       "128576                                                                                                                                                                                                                                                                                                                                                                                                                                             You are attempting to use logic and reason on folks who possess neither.   \n",
       "95355   No, I don't suppose Trump would resign -- he's far too selfish for that; of course, selfishness could lead him to resign if he came to believe that doing so would be in his own best interest. As I've said, I have very mixed feelings about it: I think the country needs the transparency of the impeachment process. I hope there are some people who love or care for Trump who also love and care about the country; but I'm not sure who they are or whether they love the country enough to tell Trump ...   \n",
       "118639                                                                                                                                                                                                              Sounds like the analysis of every pro sports team that has been in futility, toiling away for decades....they are just one tiny tweek,  one small change in the playbook, or just one small trade for an inconsequential player away from perfection, and eternal championships...\\n\\nNot gonna happen.   \n",
       "398                                                                                                                                                                                                                                                                                                                                                                                                                                                                               u aint got no pancake mix lady so stopppp   \n",
       "97750                                                                                                                                                                                                                                                                                                                                                   Yes. Rationalizing your  jealousy ? Then become a doctor. Or Serve the public or stop your whining because others are doing better than you because they earned it.   \n",
       "129596                                                                                                                                                                                                                                                                                                                                                             A 60s to 70s year-old person, participating in a 2.384 mile-race, has an empty coconut for a head.\\nLike the song goes:\\nPlenty pupule it's easy to see.   \n",
       "158826                                                                                                                                                                                                                      Poor Steve.  Writing comments in the local paper where one lives  is not trolling, it's citizenship.\\n\\nBut writing the same few comments over and over again, hundreds of times a day, in fora all over the English-speaking world, but only on one topic  --  THAT's trolling!  Got a mirror?   \n",
       "30426                                                                                                                                                                                                                                                                                                                                                                                                                                 We need a fence taller than the Mexican fence because Canadians on average are taller   \n",
       "9591    wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS w...   \n",
       "\n",
       "        label  pred_score  \n",
       "95666       1    0.120124  \n",
       "128576      1    0.125693  \n",
       "95355       1    0.194154  \n",
       "118639      1    0.161133  \n",
       "398         1    0.140863  \n",
       "97750       1    0.170173  \n",
       "129596      1    0.123686  \n",
       "158826      1    0.171609  \n",
       "30426       1    0.125205  \n",
       "9591        1    0.184638  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[(df_test.label==1) & (df_test.pred_score < 0.2)].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "false negatives используют иногда довольно типичные обидные выражения, составленные из необидных слов, а иногда реально нужна логика для понимания обидности (типа *You are attempting to use logic and reason on folks who possess neither.*)\n",
    "\n",
    "false positives часто реально грубоватые, но иногда это цитаты, а иногда стремноватые слова используются в необидном контексте (*That's how the media tries to make a fool of us sometimes. It doesn't even add up.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3k",
   "language": "python",
   "name": "p3k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
