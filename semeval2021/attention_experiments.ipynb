{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 10 00:30:12 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 27%   31C    P8    26W / 260W |   7915MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 27%   25C    P8     9W / 260W |   1040MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 27%   28C    P8     3W / 260W |   9070MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 27%   27C    P8     9W / 260W |   2094MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce RTX 208...  Off  | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 27%   23C    P8     1W / 260W |     11MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce RTX 208...  Off  | 00000000:3E:00.0 Off |                  N/A |\n",
      "| 27%   26C    P8    14W / 260W |     11MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce RTX 208...  Off  | 00000000:40:00.0 Off |                  N/A |\n",
      "| 27%   25C    P8    10W / 260W |     11MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce RTX 208...  Off  | 00000000:41:00.0 Off |                  N/A |\n",
      "| 27%   25C    P8    14W / 260W |     11MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0      4694      C   /opt/.pyenv/versions/3.7.4/bin/python        885MiB |\n",
      "|    0     15151      C   /opt/.pyenv/versions/3.7.4/bin/python       1081MiB |\n",
      "|    0     23791      C   /opt/.pyenv/versions/3.7.4/bin/python       3819MiB |\n",
      "|    0     31167      C   /opt/.pyenv/versions/3.7.4/bin/python       2119MiB |\n",
      "|    1      4694      C   /opt/.pyenv/versions/3.7.4/bin/python       1029MiB |\n",
      "|    2     24051      C   /opt/.pyenv/versions/3.7.4/bin/python       9059MiB |\n",
      "|    3      3728      C   /home/markov/anaconda3/bin/python           2083MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from box import Box\n",
    "import pandas as pd\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import datetime\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from fast_bert.modeling import BertForMultiLabelSequenceClassification\n",
    "from fast_bert.data_cls import BertDataBunch, InputExample, InputFeatures, MultiLabelTextProcessor, convert_examples_to_features\n",
    "from fast_bert.learner_cls import BertLearner\n",
    "\n",
    "import re\n",
    "MAX_SENTENCE_LEN = 82\n",
    "\n",
    "from utils import preprocess\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import torchtext.data as data\n",
    "from torchtext.data import Field\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from functools import partial\n",
    "import time\n",
    "from IPython.core.debugger import set_trace\n",
    "from tqdm import tqdm, trange\n",
    "# from tqdm import tqdm, trange\n",
    "\n",
    "seed = 678\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(3)\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = {v: k for k, v in tokenizer.vocab.items()}\n",
    "\n",
    "def toks_to_words(token_ids):\n",
    "    \"\"\" Merge subword tokens into whole words \"\"\"\n",
    "    indices = []\n",
    "    for i, token_id in enumerate(token_ids):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        token_text = v[token_id]\n",
    "        if token_text.startswith('##'):\n",
    "            indices.append(i)\n",
    "        else:\n",
    "            if indices:\n",
    "                toks = [v[token_ids[t]] for t in indices]\n",
    "                word = ''.join([toks[0]] + [t[2:] for t in toks[1:]])\n",
    "                new_indices = [index - 1 for index in indices]\n",
    "                yield new_indices, word\n",
    "            indices = [i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "DATA_PATH = '../../../semeval2021_task/data/'\n",
    "\n",
    "train = pd.read_csv(DATA_PATH + 'tsd_train.csv')\n",
    "trial = pd.read_csv(DATA_PATH + 'tsd_trial.csv')\n",
    "\n",
    "train['spans'] = train.spans.apply(literal_eval)\n",
    "trial['spans'] = trial.spans.apply(literal_eval)\n",
    "texts = list(train['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFICATION_MODEL_PATH = '../../models/distilbert/model_out'\n",
    "CLASSIFICATION_LABELS_PATH = '../../labels'\n",
    "\n",
    "from fast_bert.prediction import BertClassificationPredictor\n",
    "\n",
    "predictor = BertClassificationPredictor(CLASSIFICATION_MODEL_PATH, CLASSIFICATION_LABELS_PATH, \n",
    "                                        multi_label=True, model_type='distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "lern = predictor.get_learner()\n",
    "pretrained_weights = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "MASK_INDEX = tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
    "\n",
    "lern.model.eval();\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig\n",
    "config = DistilBertConfig.from_pretrained('distilbert-base-cased', output_attentions=True)\n",
    "lern.model.distilbert.config = config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to use the attentions from all heads and all layers as features for words, then later train some classifier on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when words are splitted by several tokens by BERT, each token has\n",
    "#its own feature vector. this function takes the mean of these vectors\n",
    "#and assign it to the original word\n",
    "def shrink(nums_arr, att_arr):\n",
    "    \"\"\"\n",
    "    When words are splitted by several tokens by BERT, each token has\n",
    "    its own feature vector. this function takes the mean of these vectors\n",
    "    and assign it to the original word.\n",
    "    \n",
    "    nums_arr: list, contains indices of tokens that need to be united back into\n",
    "    one word\n",
    "    \n",
    "    att_arr: np.array of attention, shape (num_heads) x (num_words) or\n",
    "    (num_layers) x (num_words)\n",
    "    \n",
    "    output: np.array, shape (num_words) x (num_heads) or \n",
    "    (num_words) x (num_layers)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    prev_i = 0\n",
    "    for arr in nums_arr:#for each splitted word\n",
    "        fig = att_arr[:, prev_i:arr[0]] #add all previous words\n",
    "        nafig = att_arr[:, arr].mean(axis=1)[:, np.newaxis] #add mean of token features\n",
    "        res.append(fig)\n",
    "        res.append(nafig)\n",
    "        prev_i = arr[-1] + 1 #make the current word \"previous\"\n",
    "    if prev_i < att_arr.shape[1]: #just to be safe and not overstep the array\n",
    "        res.append(att_arr[:, prev_i:])\n",
    "    return np.hstack(res).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_for_words(text: str, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Get attention values for all heads averaged over all layers and \n",
    "    attention values for all layers averaged over all heads.\n",
    "    \"\"\"\n",
    "    toks = tokenizer.encode(text) \n",
    "    emb_num = model.distilbert.embeddings.word_embeddings.num_embeddings\n",
    "    toks = [i if i < emb_num else tokenizer.unk_token_id for i in toks]\n",
    "    #these procedure is necessary because for some reason, \n",
    "    #tokenizer knows more words that BERT\n",
    "    nums = [arr for (arr, string) in list(toks_to_words(toks)) if len(arr) > 1]\n",
    "    sentence = torch.tensor(toks).unsqueeze(0)\n",
    "    sentence = sentence.cuda()\n",
    "    out = lern.model.distilbert(sentence, output_attentions=True,\n",
    "                    output_hidden_states=True)\n",
    "    #we cut the CLS and SEP tokens\n",
    "    attentions = torch.cat(out[2], dim=0).cpu()[:, :, 1:-1, 1:-1]\n",
    "    #take mean attention over all layers\n",
    "    means_heads =  attentions.mean(dim=(0, 2)).detach().numpy()\n",
    "    #take mean attention over all heads\n",
    "    means_layers = attentions.mean(dim=(1, 2)).detach().numpy()\n",
    "    means_heads = shrink(nums, means_heads)\n",
    "    means_layers = shrink(nums, means_layers)\n",
    "\n",
    "    return means_heads, means_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getword(arr, text):\n",
    "    \"\"\"\n",
    "    Using a span from a dataset, obtain a word\n",
    "    \"\"\"\n",
    "    ans = ''\n",
    "    for i in range(len(arr)):\n",
    "        elem = arr[i]\n",
    "        if  i != 0 and i != len(arr) - 1 and elem != arr[i-1] + 1:\n",
    "            ans += ' '\n",
    "            ans += text[elem]\n",
    "        else:\n",
    "            ans += text[elem]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(span, text, model, tokenizer):\n",
    "    target_toks = tokenizer.encode(getword(span, text))\n",
    "    toks = tokenizer.encode(text)\n",
    "    emb_num = model.distilbert.embeddings.word_embeddings.num_embeddings\n",
    "    target_toks = [i if i < emb_num else tokenizer.unk_token_id for i in target_toks]\n",
    "    toks = [i if i < emb_num else tokenizer.unk_token_id for i in toks]\n",
    "    toks = [string for (arr, string) in list(toks_to_words(toks))]\n",
    "    target_toks = [string for (arr, string) in list(toks_to_words(target_toks))]\n",
    "    target = []\n",
    "    #check if the word is in spanned words\n",
    "    for tok in toks:\n",
    "        if len(target_toks) > 0 and tok == target_toks[0]:\n",
    "            target.append(1)\n",
    "            target_toks = target_toks[1:]\n",
    "        else:\n",
    "            target.append(0)\n",
    "    return toks, target\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7939it [01:38, 80.85it/s] \n"
     ]
    }
   ],
   "source": [
    "target = []\n",
    "data = np.array([])\n",
    "for span, text in tqdm(zip(train['spans'], train['text'])):\n",
    "    heads, layers = get_attention_for_words(text, lern.model, tokenizer)\n",
    "    headlay = np.hstack((heads, layers))\n",
    "    #print(headlay.shape)\n",
    "    if len(data) == 0:\n",
    "        data = headlay\n",
    "    else:\n",
    "        data = np.vstack((data, headlay))\n",
    "    words, new_target = make_target(span, text, lern.model, tokenizer)\n",
    "    #print(new_target)\n",
    "    try:\n",
    "        assert len(new_target) == headlay.shape[0]\n",
    "    except:\n",
    "        print(new_target)\n",
    "        print(words)\n",
    "        print(headlay.shape)\n",
    "        print(len(words))\n",
    "        print(text)\n",
    "        break\n",
    "    target += new_target\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352307, 18)"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markov/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 690/690 [00:05<00:00, 120.74it/s]\n"
     ]
    }
   ],
   "source": [
    "trial_data = np.array([])\n",
    "for text in tqdm(trial['text']):\n",
    "    heads, layers = get_attention_for_words(text, lern.model, tokenizer)\n",
    "    headlay = np.hstack((heads, layers))\n",
    "    if len(trial_data) == 0:\n",
    "        trial_data = headlay\n",
    "    else:\n",
    "        trial_data = np.vstack((trial_data, headlay))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = logreg.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "632"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24942"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29652, 18)"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_preds = logreg.predict_proba(trial_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89564029, 0.10435971],\n",
       "       [0.88706988, 0.11293012],\n",
       "       [0.90168331, 0.09831669],\n",
       "       [0.90979598, 0.09020402],\n",
       "       [0.91046697, 0.08953303],\n",
       "       [0.90476699, 0.09523301],\n",
       "       [0.89886462, 0.10113538],\n",
       "       [0.92500759, 0.07499241],\n",
       "       [0.89978916, 0.10021084],\n",
       "       [0.90337858, 0.09662142]])"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29652.0"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_preds.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
