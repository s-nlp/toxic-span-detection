{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec  4 01:11:25 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 27%   38C    P2    61W / 260W |   6276MiB / 11019MiB |      1%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 27%   35C    P0    78W / 260W |     11MiB / 11019MiB |     45%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 27%   34C    P8     4W / 260W |   9528MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 53%   55C    P2    63W / 260W |   2784MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce RTX 208...  Off  | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 44%   51C    P2    52W / 260W |   1148MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce RTX 208...  Off  | 00000000:3E:00.0 Off |                  N/A |\n",
      "| 27%   38C    P0    85W / 260W |     11MiB / 11019MiB |     48%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce RTX 208...  Off  | 00000000:40:00.0 Off |                  N/A |\n",
      "| 55%   55C    P0    94W / 260W |     11MiB / 11019MiB |     27%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce RTX 208...  Off  | 00000000:41:00.0 Off |                  N/A |\n",
      "| 27%   36C    P8    27W / 260W |   1340MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     15151      C   /opt/.pyenv/versions/3.7.4/bin/python       1081MiB |\n",
      "|    0     23791      C   /opt/.pyenv/versions/3.7.4/bin/python       3819MiB |\n",
      "|    0     26236      C   /opt/.pyenv/versions/3.7.4/bin/python       1365MiB |\n",
      "|    2      8833      C   /opt/.pyenv/versions/3.7.4/bin/python       9517MiB |\n",
      "|    3     26008      C   python                                      2773MiB |\n",
      "|    4     26008      C   python                                      1137MiB |\n",
      "|    7      9443      C   /home/logacheva/anaconda3/bin/python        1329MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n",
      "GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "cuda_device = torch.device('cuda:1')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "for i in range(n_gpu):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-04 01:11:32,194 Reading data from data\n",
      "2020-12-04 01:11:32,195 Train: data/train.conll2003\n",
      "2020-12-04 01:11:32,196 Dev: data/dev.conll2003\n",
      "2020-12-04 01:11:32,197 Test: data/trial.conll2003\n",
      "{\n",
      "    \"TRAIN\": {\n",
      "        \"dataset\": \"TRAIN\",\n",
      "        \"total_number_of_documents\": 6351,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 267161,\n",
      "            \"min\": 1,\n",
      "            \"max\": 236,\n",
      "            \"avg\": 42.06597386238388\n",
      "        }\n",
      "    },\n",
      "    \"TEST\": {\n",
      "        \"dataset\": \"TEST\",\n",
      "        \"total_number_of_documents\": 690,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 28240,\n",
      "            \"min\": 1,\n",
      "            \"max\": 216,\n",
      "            \"avg\": 40.927536231884055\n",
      "        }\n",
      "    },\n",
      "    \"DEV\": {\n",
      "        \"dataset\": \"DEV\",\n",
      "        \"total_number_of_documents\": 1588,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 68172,\n",
      "            \"min\": 1,\n",
      "            \"max\": 231,\n",
      "            \"avg\": 42.92947103274559\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "data_folder = './data'\n",
    "corpora = ColumnCorpus(data_folder,\n",
    "                      {0: 'text', 3 : 'toxic'},\n",
    "                      train_file='train.conll2003', \n",
    "                      test_file='trial.conll2003',\n",
    "                      dev_file='dev.conll2003')\n",
    "\n",
    "print(corpora.obtain_statistics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "def create_dataset(flair_dataset, preprocess=False):\n",
    "    dataset = []\n",
    "    \n",
    "    for sent in flair_dataset:\n",
    "        tokens = [w.text for w in sent]\n",
    "        labels = [w.get_tag('toxic').value for w in sent]\n",
    "        \n",
    "        if preprocess:\n",
    "            dataset.append((list(zip(tokens, [nltk.pos_tag([tok])[0][1] for tok in tokens])), labels))\n",
    "        else:\n",
    "            dataset.append((tokens, labels))\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(corpora.train, preprocess=True)\n",
    "dev_dataset = create_dataset(corpora.dev, preprocess=True)\n",
    "test_dataset= create_dataset(corpora.test, preprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([('Another', 'DT'),\n",
       "   ('violent', 'NN'),\n",
       "   ('and', 'CC'),\n",
       "   ('aggressive', 'JJ'),\n",
       "   ('immigrant', 'NN'),\n",
       "   ('killing', 'VBG'),\n",
       "   ('a', 'DT'),\n",
       "   ('innocent', 'NN'),\n",
       "   ('and', 'CC'),\n",
       "   ('intelligent', 'NN'),\n",
       "   ('US', 'NN'),\n",
       "   ('Citizen', 'NN'),\n",
       "   ('...', ':'),\n",
       "   ('.', '.'),\n",
       "   ('Sarcasm', 'NN')],\n",
       "  ['O',\n",
       "   'B-SPAN',\n",
       "   'I-SPAN',\n",
       "   'I-SPAN',\n",
       "   'I-SPAN',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O']),\n",
       " ([('I', 'PRP'),\n",
       "   ('am', 'VBP'),\n",
       "   ('56', 'CD'),\n",
       "   ('years', 'NNS'),\n",
       "   ('old', 'JJ'),\n",
       "   (',', ','),\n",
       "   ('I', 'PRP'),\n",
       "   ('am', 'VBP'),\n",
       "   ('not', 'RB'),\n",
       "   ('your', 'PRP$'),\n",
       "   ('fucking', 'VBG'),\n",
       "   ('junior', 'NN'),\n",
       "   ('pal', 'NN'),\n",
       "   ('.', '.'),\n",
       "   ('What', 'WP'),\n",
       "   ('you', 'PRP'),\n",
       "   ('are', 'VBP'),\n",
       "   ('saying', 'VBG'),\n",
       "   ('makes', 'VBZ'),\n",
       "   ('no', 'DT'),\n",
       "   ('sense', 'NN'),\n",
       "   ('.', '.'),\n",
       "   ('I', 'PRP'),\n",
       "   ('do', 'VB'),\n",
       "   (\"n't\", 'RB'),\n",
       "   ('know', 'VB'),\n",
       "   ('what', 'WP'),\n",
       "   ('you', 'PRP'),\n",
       "   ('are', 'VBP'),\n",
       "   ('basing', 'VBG'),\n",
       "   ('this', 'DT'),\n",
       "   ('on', 'IN'),\n",
       "   ('.', '.'),\n",
       "   ('The', 'DT'),\n",
       "   ('cheap', 'NN'),\n",
       "   ('black', 'JJ'),\n",
       "   ('market', 'NN'),\n",
       "   ('crap', 'NN'),\n",
       "   ('is', 'VBZ'),\n",
       "   ('still', 'RB'),\n",
       "   ('coming', 'VBG'),\n",
       "   ('up', 'RB'),\n",
       "   ('from', 'IN'),\n",
       "   ('Mexican', 'JJ'),\n",
       "   ('nationals', 'NNS'),\n",
       "   (',', ','),\n",
       "   ('and', 'CC'),\n",
       "   ('the', 'DT'),\n",
       "   ('good', 'JJ'),\n",
       "   ('stuff', 'NN'),\n",
       "   ('from', 'IN'),\n",
       "   ('BC', 'NN'),\n",
       "   ('and', 'CC'),\n",
       "   ('Humbolt', 'NN'),\n",
       "   ('.', '.'),\n",
       "   ('The', 'DT'),\n",
       "   ('I5', 'NN'),\n",
       "   ('corridor', 'NN'),\n",
       "   ('is', 'VBZ'),\n",
       "   ('still', 'RB'),\n",
       "   ('just', 'RB'),\n",
       "   ('as', 'IN'),\n",
       "   ('busy', 'JJ'),\n",
       "   (',', ','),\n",
       "   ('and', 'CC'),\n",
       "   ('they', 'PRP'),\n",
       "   ('are', 'VBP'),\n",
       "   ('still', 'RB'),\n",
       "   ('making', 'VBG'),\n",
       "   ('a', 'DT'),\n",
       "   ('ton', 'NN'),\n",
       "   ('of', 'IN'),\n",
       "   ('money', 'NN'),\n",
       "   ('.', '.'),\n",
       "   ('Nothing', 'NN'),\n",
       "   ('has', 'VBZ'),\n",
       "   ('really', 'RB'),\n",
       "   ('changed', 'VBN'),\n",
       "   ('except', 'IN'),\n",
       "   ('that', 'IN'),\n",
       "   ('the', 'DT'),\n",
       "   ('legal', 'JJ'),\n",
       "   ('market', 'NN'),\n",
       "   ('has', 'VBZ'),\n",
       "   ('made', 'VBN'),\n",
       "   ('it', 'PRP'),\n",
       "   ('easy', 'JJ'),\n",
       "   ('for', 'IN'),\n",
       "   ('anyone', 'NN'),\n",
       "   ('of', 'IN'),\n",
       "   ('legal', 'JJ'),\n",
       "   ('age', 'NN'),\n",
       "   ('to', 'TO'),\n",
       "   ('buy', 'VB'),\n",
       "   ('it', 'PRP'),\n",
       "   ('.', '.'),\n",
       "   ('And', 'CC'),\n",
       "   ('the', 'DT'),\n",
       "   ('legal', 'JJ'),\n",
       "   ('market', 'NN'),\n",
       "   ('is', 'VBZ'),\n",
       "   ('controlled', 'VBN'),\n",
       "   ('by', 'IN'),\n",
       "   ('corporate', 'JJ'),\n",
       "   ('growers', 'NNS'),\n",
       "   ('with', 'IN'),\n",
       "   ('million', 'CD'),\n",
       "   ('dollar', 'NN'),\n",
       "   ('facilities', 'NNS'),\n",
       "   (',', ','),\n",
       "   ('or', 'CC'),\n",
       "   ('a', 'DT'),\n",
       "   ('co-op', 'NN'),\n",
       "   ('of', 'IN'),\n",
       "   ('smaller', 'JJR'),\n",
       "   ('growers', 'NNS'),\n",
       "   ('.', '.'),\n",
       "   ('The', 'DT'),\n",
       "   ('Federal', 'JJ'),\n",
       "   ('government', 'NN'),\n",
       "   ('``', '``'),\n",
       "   ('war', 'NN'),\n",
       "   ('on', 'IN'),\n",
       "   ('drugs', 'NNS'),\n",
       "   (\"''\", \"''\"),\n",
       "   ('really', 'RB'),\n",
       "   ('has', 'VBZ'),\n",
       "   ('no', 'DT'),\n",
       "   ('impact', 'NN'),\n",
       "   ('on', 'IN'),\n",
       "   ('the', 'DT'),\n",
       "   ('legal', 'JJ'),\n",
       "   ('market', 'NN'),\n",
       "   ('in', 'IN'),\n",
       "   ('Oregon', 'NN'),\n",
       "   ('.', '.'),\n",
       "   ('I', 'PRP'),\n",
       "   ('do', 'VB'),\n",
       "   (\"n't\", 'RB'),\n",
       "   ('see', 'VB'),\n",
       "   ('any', 'DT'),\n",
       "   ('glutton', 'NN'),\n",
       "   ('of', 'IN'),\n",
       "   ('weed', 'NN'),\n",
       "   (',', ','),\n",
       "   ('whatever', 'WDT'),\n",
       "   ('that', 'IN'),\n",
       "   ('means', 'NNS'),\n",
       "   ('.', '.')],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-SPAN',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O']),\n",
       " ([('Damn', 'NN'),\n",
       "   (',', ','),\n",
       "   ('a', 'DT'),\n",
       "   ('whole', 'JJ'),\n",
       "   ('family', 'NN'),\n",
       "   ('.', '.'),\n",
       "   ('Sad', 'NN'),\n",
       "   ('indeed', 'RB'),\n",
       "   ('.', '.')],\n",
       "  ['B-SPAN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']),\n",
       " ([('What', 'WP'),\n",
       "   ('a', 'DT'),\n",
       "   ('knucklehead', 'NN'),\n",
       "   ('.', '.'),\n",
       "   ('How', 'WRB'),\n",
       "   ('can', 'MD'),\n",
       "   ('anyone', 'NN'),\n",
       "   ('not', 'RB'),\n",
       "   ('know', 'VB'),\n",
       "   ('this', 'DT'),\n",
       "   ('would', 'MD'),\n",
       "   ('be', 'VB'),\n",
       "   ('offensive', 'JJ'),\n",
       "   ('?', '.'),\n",
       "   ('?', '.')],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'B-SPAN',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O']),\n",
       " ([('``', '``'),\n",
       "   ('who', 'WP'),\n",
       "   ('do', 'VB'),\n",
       "   ('you', 'PRP'),\n",
       "   ('think', 'NN'),\n",
       "   ('should', 'MD'),\n",
       "   ('do', 'VB'),\n",
       "   ('the', 'DT'),\n",
       "   ('killing', 'VBG'),\n",
       "   ('?', '.'),\n",
       "   (\"''\", \"''\"),\n",
       "   ('Anyone', 'NN'),\n",
       "   ('and', 'CC'),\n",
       "   ('everyone', 'NN'),\n",
       "   ('.', '.'),\n",
       "   ('This', 'DT'),\n",
       "   ('is', 'VBZ'),\n",
       "   ('a', 'DT'),\n",
       "   ('community', 'NN'),\n",
       "   ('problem', 'NN'),\n",
       "   (',', ','),\n",
       "   ('so', 'RB'),\n",
       "   ('everyone', 'NN'),\n",
       "   ('who', 'WP'),\n",
       "   ('wants', 'VBZ'),\n",
       "   ('to', 'TO'),\n",
       "   ('be', 'VB'),\n",
       "   ('part', 'NN'),\n",
       "   ('of', 'IN'),\n",
       "   ('the', 'DT'),\n",
       "   ('solution', 'NN'),\n",
       "   ('should', 'MD'),\n",
       "   ('be', 'VB'),\n",
       "   ('allowed', 'VBN'),\n",
       "   ('to', 'TO'),\n",
       "   ('help', 'NN'),\n",
       "   ('.', '.'),\n",
       "   ('As', 'IN'),\n",
       "   ('I', 'PRP'),\n",
       "   ('said', 'VBD'),\n",
       "   ('above', 'IN'),\n",
       "   (':', ':'),\n",
       "   ('remove', 'VB'),\n",
       "   ('all', 'DT'),\n",
       "   ('protections', 'NNS'),\n",
       "   ('on', 'IN'),\n",
       "   ('free', 'JJ'),\n",
       "   ('ranging', 'VBG'),\n",
       "   ('cats', 'NNS'),\n",
       "   (',', ','),\n",
       "   ('and', 'CC'),\n",
       "   ('allow', 'VB'),\n",
       "   ('people', 'NNS'),\n",
       "   ('to', 'TO'),\n",
       "   ('help', 'NN'),\n",
       "   ('.', '.'),\n",
       "   ('People', 'NNS'),\n",
       "   ('who', 'WP'),\n",
       "   ('allow', 'VB'),\n",
       "   ('their', 'PRP$'),\n",
       "   ('pets', 'NNS'),\n",
       "   ('to', 'TO'),\n",
       "   ('roam', 'NN'),\n",
       "   ('outside', 'IN'),\n",
       "   ('are', 'VBP'),\n",
       "   ('part', 'NN'),\n",
       "   ('of', 'IN'),\n",
       "   ('the', 'DT'),\n",
       "   ('problem', 'NN'),\n",
       "   (',', ','),\n",
       "   ('because', 'IN'),\n",
       "   ('their', 'PRP$'),\n",
       "   ('pets', 'NNS'),\n",
       "   ('are', 'VBP'),\n",
       "   ('part', 'NN'),\n",
       "   ('of', 'IN'),\n",
       "   ('the', 'DT'),\n",
       "   ('problem', 'NN'),\n",
       "   ('.', '.'),\n",
       "   ('It', 'PRP'),\n",
       "   ('would', 'MD'),\n",
       "   ('be', 'VB'),\n",
       "   ('very', 'RB'),\n",
       "   ('sad', 'NN'),\n",
       "   ('if', 'IN'),\n",
       "   ('pets', 'NNS'),\n",
       "   ('were', 'VBD'),\n",
       "   ('killed', 'VBN'),\n",
       "   (',', ','),\n",
       "   ('but', 'CC'),\n",
       "   ('ultimately', 'RB'),\n",
       "   ('it', 'PRP'),\n",
       "   ('is', 'VBZ'),\n",
       "   ('the', 'DT'),\n",
       "   ('owner', 'NN'),\n",
       "   (\"'s\", 'POS'),\n",
       "   ('responsibility', 'NN'),\n",
       "   ('to', 'TO'),\n",
       "   ('make', 'VB'),\n",
       "   ('sure', 'NN'),\n",
       "   ('their', 'PRP$'),\n",
       "   ('pets', 'NNS'),\n",
       "   ('are', 'VBP'),\n",
       "   ('not', 'RB'),\n",
       "   ('out', 'IN'),\n",
       "   ('killing', 'VBG'),\n",
       "   ('birds', 'NNS'),\n",
       "   ('.', '.'),\n",
       "   ('Years', 'NNS'),\n",
       "   ('ago', 'RB'),\n",
       "   ('some', 'DT'),\n",
       "   ('Vermont', 'NN'),\n",
       "   ('acquaintances', 'NNS'),\n",
       "   ('got', 'VBD'),\n",
       "   ('one', 'CD'),\n",
       "   ('warning', 'VBG'),\n",
       "   ('from', 'IN'),\n",
       "   ('the', 'DT'),\n",
       "   ('Ranger', 'NN'),\n",
       "   (':', ':'),\n",
       "   ('Next', 'JJ'),\n",
       "   ('time', 'NN'),\n",
       "   ('I', 'PRP'),\n",
       "   ('see', 'VB'),\n",
       "   ('your', 'PRP$'),\n",
       "   ('dog', 'NN'),\n",
       "   ('running', 'VBG'),\n",
       "   ('deer', 'NN'),\n",
       "   (',', ','),\n",
       "   ('I', 'PRP'),\n",
       "   (\"'ll\", 'MD'),\n",
       "   ('shoot', 'NN'),\n",
       "   ('it', 'PRP'),\n",
       "   ('.', '.'),\n",
       "   ('Next', 'JJ'),\n",
       "   ('time', 'NN'),\n",
       "   ('she', 'PRP'),\n",
       "   ('saw', 'NN'),\n",
       "   ('it', 'PRP'),\n",
       "   ('running', 'VBG'),\n",
       "   ('deer', 'NN'),\n",
       "   (',', ','),\n",
       "   ('she', 'PRP'),\n",
       "   ('did', 'VBD'),\n",
       "   ('.', '.'),\n",
       "   ('Vermont', 'NN'),\n",
       "   ('does', 'VBZ'),\n",
       "   ('not', 'RB'),\n",
       "   ('pretend', 'NN'),\n",
       "   ('that', 'IN'),\n",
       "   ('trapping', 'VBG'),\n",
       "   (',', ','),\n",
       "   ('neutering', 'VBG'),\n",
       "   (',', ','),\n",
       "   ('and', 'CC'),\n",
       "   ('releasing', 'VBG'),\n",
       "   ('dogs', 'NNS'),\n",
       "   ('solves', 'NNS'),\n",
       "   ('any', 'DT'),\n",
       "   ('problems', 'NNS'),\n",
       "   ('.', '.'),\n",
       "   ('They', 'PRP'),\n",
       "   ('are', 'VBP'),\n",
       "   ('serious', 'JJ'),\n",
       "   ('about', 'IN'),\n",
       "   ('threats', 'NNS'),\n",
       "   ('.', '.'),\n",
       "   ('We', 'PRP'),\n",
       "   ('just', 'RB'),\n",
       "   ('want', 'NN'),\n",
       "   ('to', 'TO'),\n",
       "   ('feel', 'NN'),\n",
       "   ('good', 'JJ'),\n",
       "   ('about', 'IN'),\n",
       "   ('ourselves', 'NNS'),\n",
       "   ('.', '.'),\n",
       "   ('If', 'IN'),\n",
       "   ('we', 'PRP'),\n",
       "   ('were', 'VBD'),\n",
       "   ('serious', 'JJ'),\n",
       "   ('about', 'IN'),\n",
       "   ('feral', 'JJ'),\n",
       "   ('cats', 'NNS'),\n",
       "   (',', ','),\n",
       "   ('we', 'PRP'),\n",
       "   ('would', 'MD'),\n",
       "   ('kill', 'NN'),\n",
       "   ('them', 'PRP'),\n",
       "   ('.', '.'),\n",
       "   ('Neutered', 'VBN'),\n",
       "   ('cats', 'NNS'),\n",
       "   ('eat', 'NN'),\n",
       "   ('birds', 'NNS'),\n",
       "   ('.', '.'),\n",
       "   ('Do', 'VB'),\n",
       "   (\"n't\", 'RB'),\n",
       "   ('release', 'NN'),\n",
       "   ('them', 'PRP'),\n",
       "   ('.', '.'),\n",
       "   ('Kill', 'NNP'),\n",
       "   ('them', 'PRP'),\n",
       "   ('.', '.')],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-SPAN',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def word_features(sentence, i):\n",
    "    # Get the current word and POS\n",
    "    word = sentence[i][0]\n",
    "    pos = sentence[i][1]\n",
    "    \n",
    "    features = { \"bias\": 1.0,\n",
    "                 \"word.lower()\": word.lower(),\n",
    "                 \"word[-3:]\": word[-3:],\n",
    "                 \"word[-2:]\": word[-2:],\n",
    "                 \"word.isupper()\": word.isupper(),\n",
    "                 \"word.istitle()\": word.istitle(),\n",
    "                 \"word.isdigit()\": word.isdigit(),\n",
    "                 \"pos\": pos,\n",
    "                 \"pos[:2]\": pos[:2], # Generalized POS\n",
    "               }\n",
    "   \n",
    "    # If this is not the first word in the sentence...\n",
    "    if i > 0:\n",
    "        # Get the sentence's previous word and POS\n",
    "        prev_word = sentence[i-1][0] if len(sentence) > 1 else ''\n",
    "        prev_pos = sentence[i-1][1] if len(sentence) > 1 else ''\n",
    "        # Add characteristics of the sentence's previous word and POS to the feature dictionary\n",
    "        features.update({ \"-1:word.lower()\": prev_word.lower(),\n",
    "                          \"-1:word.istitle()\": prev_word.istitle(),\n",
    "                          \"-1:word.isupper()\": prev_word.isupper(),\n",
    "                          \"-1:pos\": prev_pos,\n",
    "                          \"-1:pos[:2]\": prev_pos[:2],\n",
    "                        })\n",
    "        \n",
    "    # Otherwise, add 'BOS' (beginning of sentence) to the feature dictionary\n",
    "    else:\n",
    "        features[\"BOS\"] = True\n",
    "\n",
    "    # If this is not the last word in the sentence...\n",
    "    if i < len(sentence)-1:\n",
    "        # Get the sentence's next word and POS\n",
    "        next_word = sentence[i+1][0] if len(sentence) > 1 else ''\n",
    "        next_pos = sentence[i+1][1] if len(sentence) > 1 else ''\n",
    "        # Add characteristics of the sentence's previous next and POS to the feature dictionary\n",
    "        features.update({ \"+1:word.lower()\": next_word.lower(),\n",
    "                          \"+1:word.istitle()\": next_word.istitle(),\n",
    "                          \"+1:word.isupper()\": next_word.isupper(),\n",
    "                          \"+1:pos\": next_pos,\n",
    "                          \"+1:pos[:2]\": next_pos[:2],\n",
    "                        })\n",
    "        \n",
    "    # Otherwise, add 'EOS' (end of sentence) to the feature dictionary\n",
    "    else:\n",
    "        features[\"EOS\"] = True\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def sentence_features(sentence):\n",
    "    return [word_features(sentence, i) for i in range(len(sentence))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [sentence_features(sentence[0]) for sentence in train_dataset]\n",
    "y = [sentence[1] for sentence in train_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-SPAN',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bias': 1.0,\n",
       "  'word.lower()': 'i',\n",
       "  'word[-3:]': 'I',\n",
       "  'word[-2:]': 'I',\n",
       "  'word.isupper()': True,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'PRP',\n",
       "  'pos[:2]': 'PR',\n",
       "  'BOS': True,\n",
       "  '+1:word.lower()': 'am',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VBP',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'am',\n",
       "  'word[-3:]': 'am',\n",
       "  'word[-2:]': 'am',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VBP',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'i',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': True,\n",
       "  '-1:pos': 'PRP',\n",
       "  '-1:pos[:2]': 'PR',\n",
       "  '+1:word.lower()': '56',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'CD',\n",
       "  '+1:pos[:2]': 'CD'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '56',\n",
       "  'word[-3:]': '56',\n",
       "  'word[-2:]': '56',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': True,\n",
       "  'pos': 'CD',\n",
       "  'pos[:2]': 'CD',\n",
       "  '-1:word.lower()': 'am',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VBP',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'years',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NNS',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'years',\n",
       "  'word[-3:]': 'ars',\n",
       "  'word[-2:]': 'rs',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NNS',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': '56',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'CD',\n",
       "  '-1:pos[:2]': 'CD',\n",
       "  '+1:word.lower()': 'old',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'JJ',\n",
       "  '+1:pos[:2]': 'JJ'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'old',\n",
       "  'word[-3:]': 'old',\n",
       "  'word[-2:]': 'ld',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'JJ',\n",
       "  'pos[:2]': 'JJ',\n",
       "  '-1:word.lower()': 'years',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NNS',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': ',',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': ',',\n",
       "  '+1:pos[:2]': ','},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': ',',\n",
       "  'word[-3:]': ',',\n",
       "  'word[-2:]': ',',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': ',',\n",
       "  'pos[:2]': ',',\n",
       "  '-1:word.lower()': 'old',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'JJ',\n",
       "  '-1:pos[:2]': 'JJ',\n",
       "  '+1:word.lower()': 'i',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': True,\n",
       "  '+1:pos': 'PRP',\n",
       "  '+1:pos[:2]': 'PR'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'i',\n",
       "  'word[-3:]': 'I',\n",
       "  'word[-2:]': 'I',\n",
       "  'word.isupper()': True,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'PRP',\n",
       "  'pos[:2]': 'PR',\n",
       "  '-1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': ',',\n",
       "  '-1:pos[:2]': ',',\n",
       "  '+1:word.lower()': 'am',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VBP',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'am',\n",
       "  'word[-3:]': 'am',\n",
       "  'word[-2:]': 'am',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VBP',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'i',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': True,\n",
       "  '-1:pos': 'PRP',\n",
       "  '-1:pos[:2]': 'PR',\n",
       "  '+1:word.lower()': 'not',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'RB',\n",
       "  '+1:pos[:2]': 'RB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'not',\n",
       "  'word[-3:]': 'not',\n",
       "  'word[-2:]': 'ot',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'RB',\n",
       "  'pos[:2]': 'RB',\n",
       "  '-1:word.lower()': 'am',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VBP',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'your',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'PRP$',\n",
       "  '+1:pos[:2]': 'PR'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'your',\n",
       "  'word[-3:]': 'our',\n",
       "  'word[-2:]': 'ur',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'PRP$',\n",
       "  'pos[:2]': 'PR',\n",
       "  '-1:word.lower()': 'not',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'RB',\n",
       "  '-1:pos[:2]': 'RB',\n",
       "  '+1:word.lower()': 'fucking',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VBG',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'fucking',\n",
       "  'word[-3:]': 'ing',\n",
       "  'word[-2:]': 'ng',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VBG',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'your',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'PRP$',\n",
       "  '-1:pos[:2]': 'PR',\n",
       "  '+1:word.lower()': 'junior',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'junior',\n",
       "  'word[-3:]': 'ior',\n",
       "  'word[-2:]': 'or',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'fucking',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VBG',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'pal',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'pal',\n",
       "  'word[-3:]': 'pal',\n",
       "  'word[-2:]': 'al',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'junior',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': '.',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': '.',\n",
       "  '+1:pos[:2]': '.'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '.',\n",
       "  'word[-3:]': '.',\n",
       "  'word[-2:]': '.',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': '.',\n",
       "  'pos[:2]': '.',\n",
       "  '-1:word.lower()': 'pal',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'what',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'WP',\n",
       "  '+1:pos[:2]': 'WP'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'what',\n",
       "  'word[-3:]': 'hat',\n",
       "  'word[-2:]': 'at',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'WP',\n",
       "  'pos[:2]': 'WP',\n",
       "  '-1:word.lower()': '.',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': '.',\n",
       "  '-1:pos[:2]': '.',\n",
       "  '+1:word.lower()': 'you',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'PRP',\n",
       "  '+1:pos[:2]': 'PR'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'you',\n",
       "  'word[-3:]': 'you',\n",
       "  'word[-2:]': 'ou',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'PRP',\n",
       "  'pos[:2]': 'PR',\n",
       "  '-1:word.lower()': 'what',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'WP',\n",
       "  '-1:pos[:2]': 'WP',\n",
       "  '+1:word.lower()': 'are',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VBP',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'are',\n",
       "  'word[-3:]': 'are',\n",
       "  'word[-2:]': 're',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VBP',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'you',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'PRP',\n",
       "  '-1:pos[:2]': 'PR',\n",
       "  '+1:word.lower()': 'saying',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VBG',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'saying',\n",
       "  'word[-3:]': 'ing',\n",
       "  'word[-2:]': 'ng',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VBG',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'are',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VBP',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'makes',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VBZ',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'makes',\n",
       "  'word[-3:]': 'kes',\n",
       "  'word[-2:]': 'es',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VBZ',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'saying',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VBG',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'no',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'DT',\n",
       "  '+1:pos[:2]': 'DT'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'no',\n",
       "  'word[-3:]': 'no',\n",
       "  'word[-2:]': 'no',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'DT',\n",
       "  'pos[:2]': 'DT',\n",
       "  '-1:word.lower()': 'makes',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VBZ',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'sense',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'sense',\n",
       "  'word[-3:]': 'nse',\n",
       "  'word[-2:]': 'se',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'no',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'DT',\n",
       "  '-1:pos[:2]': 'DT',\n",
       "  '+1:word.lower()': '.',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': '.',\n",
       "  '+1:pos[:2]': '.'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '.',\n",
       "  'word[-3:]': '.',\n",
       "  'word[-2:]': '.',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': '.',\n",
       "  'pos[:2]': '.',\n",
       "  '-1:word.lower()': 'sense',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'i',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': True,\n",
       "  '+1:pos': 'PRP',\n",
       "  '+1:pos[:2]': 'PR'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'i',\n",
       "  'word[-3:]': 'I',\n",
       "  'word[-2:]': 'I',\n",
       "  'word.isupper()': True,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'PRP',\n",
       "  'pos[:2]': 'PR',\n",
       "  '-1:word.lower()': '.',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': '.',\n",
       "  '-1:pos[:2]': '.',\n",
       "  '+1:word.lower()': 'do',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VB',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'do',\n",
       "  'word[-3:]': 'do',\n",
       "  'word[-2:]': 'do',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VB',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'i',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': True,\n",
       "  '-1:pos': 'PRP',\n",
       "  '-1:pos[:2]': 'PR',\n",
       "  '+1:word.lower()': \"n't\",\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'RB',\n",
       "  '+1:pos[:2]': 'RB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': \"n't\",\n",
       "  'word[-3:]': \"n't\",\n",
       "  'word[-2:]': \"'t\",\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'RB',\n",
       "  'pos[:2]': 'RB',\n",
       "  '-1:word.lower()': 'do',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VB',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'know',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VB',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'know',\n",
       "  'word[-3:]': 'now',\n",
       "  'word[-2:]': 'ow',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VB',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': \"n't\",\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'RB',\n",
       "  '-1:pos[:2]': 'RB',\n",
       "  '+1:word.lower()': 'what',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'WP',\n",
       "  '+1:pos[:2]': 'WP'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'what',\n",
       "  'word[-3:]': 'hat',\n",
       "  'word[-2:]': 'at',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'WP',\n",
       "  'pos[:2]': 'WP',\n",
       "  '-1:word.lower()': 'know',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VB',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'you',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'PRP',\n",
       "  '+1:pos[:2]': 'PR'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'you',\n",
       "  'word[-3:]': 'you',\n",
       "  'word[-2:]': 'ou',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'PRP',\n",
       "  'pos[:2]': 'PR',\n",
       "  '-1:word.lower()': 'what',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'WP',\n",
       "  '-1:pos[:2]': 'WP',\n",
       "  '+1:word.lower()': 'are',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VBP',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'are',\n",
       "  'word[-3:]': 'are',\n",
       "  'word[-2:]': 're',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VBP',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'you',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'PRP',\n",
       "  '-1:pos[:2]': 'PR',\n",
       "  '+1:word.lower()': 'basing',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VBG',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'basing',\n",
       "  'word[-3:]': 'ing',\n",
       "  'word[-2:]': 'ng',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VBG',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'are',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VBP',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'this',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'DT',\n",
       "  '+1:pos[:2]': 'DT'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'this',\n",
       "  'word[-3:]': 'his',\n",
       "  'word[-2:]': 'is',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'DT',\n",
       "  'pos[:2]': 'DT',\n",
       "  '-1:word.lower()': 'basing',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VBG',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'on',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'IN',\n",
       "  '+1:pos[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'on',\n",
       "  'word[-3:]': 'on',\n",
       "  'word[-2:]': 'on',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'IN',\n",
       "  'pos[:2]': 'IN',\n",
       "  '-1:word.lower()': 'this',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'DT',\n",
       "  '-1:pos[:2]': 'DT',\n",
       "  '+1:word.lower()': '.',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': '.',\n",
       "  '+1:pos[:2]': '.'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '.',\n",
       "  'word[-3:]': '.',\n",
       "  'word[-2:]': '.',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': '.',\n",
       "  'pos[:2]': '.',\n",
       "  '-1:word.lower()': 'on',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'IN',\n",
       "  '-1:pos[:2]': 'IN',\n",
       "  '+1:word.lower()': 'the',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'DT',\n",
       "  '+1:pos[:2]': 'DT'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'the',\n",
       "  'word[-3:]': 'The',\n",
       "  'word[-2:]': 'he',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'DT',\n",
       "  'pos[:2]': 'DT',\n",
       "  '-1:word.lower()': '.',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': '.',\n",
       "  '-1:pos[:2]': '.',\n",
       "  '+1:word.lower()': 'cheap',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'cheap',\n",
       "  'word[-3:]': 'eap',\n",
       "  'word[-2:]': 'ap',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'the',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'DT',\n",
       "  '-1:pos[:2]': 'DT',\n",
       "  '+1:word.lower()': 'black',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'JJ',\n",
       "  '+1:pos[:2]': 'JJ'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'black',\n",
       "  'word[-3:]': 'ack',\n",
       "  'word[-2:]': 'ck',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'JJ',\n",
       "  'pos[:2]': 'JJ',\n",
       "  '-1:word.lower()': 'cheap',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'market',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'market',\n",
       "  'word[-3:]': 'ket',\n",
       "  'word[-2:]': 'et',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'black',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'JJ',\n",
       "  '-1:pos[:2]': 'JJ',\n",
       "  '+1:word.lower()': 'crap',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'crap',\n",
       "  'word[-3:]': 'rap',\n",
       "  'word[-2:]': 'ap',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'market',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'is',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VBZ',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'is',\n",
       "  'word[-3:]': 'is',\n",
       "  'word[-2:]': 'is',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VBZ',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'crap',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'still',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'RB',\n",
       "  '+1:pos[:2]': 'RB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'still',\n",
       "  'word[-3:]': 'ill',\n",
       "  'word[-2:]': 'll',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'RB',\n",
       "  'pos[:2]': 'RB',\n",
       "  '-1:word.lower()': 'is',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VBZ',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'coming',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VBG',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'coming',\n",
       "  'word[-3:]': 'ing',\n",
       "  'word[-2:]': 'ng',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VBG',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'still',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'RB',\n",
       "  '-1:pos[:2]': 'RB',\n",
       "  '+1:word.lower()': 'up',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'RB',\n",
       "  '+1:pos[:2]': 'RB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'up',\n",
       "  'word[-3:]': 'up',\n",
       "  'word[-2:]': 'up',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'RB',\n",
       "  'pos[:2]': 'RB',\n",
       "  '-1:word.lower()': 'coming',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VBG',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'from',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'IN',\n",
       "  '+1:pos[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'from',\n",
       "  'word[-3:]': 'rom',\n",
       "  'word[-2:]': 'om',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'IN',\n",
       "  'pos[:2]': 'IN',\n",
       "  '-1:word.lower()': 'up',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'RB',\n",
       "  '-1:pos[:2]': 'RB',\n",
       "  '+1:word.lower()': 'mexican',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'JJ',\n",
       "  '+1:pos[:2]': 'JJ'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'mexican',\n",
       "  'word[-3:]': 'can',\n",
       "  'word[-2:]': 'an',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'JJ',\n",
       "  'pos[:2]': 'JJ',\n",
       "  '-1:word.lower()': 'from',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'IN',\n",
       "  '-1:pos[:2]': 'IN',\n",
       "  '+1:word.lower()': 'nationals',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NNS',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'nationals',\n",
       "  'word[-3:]': 'als',\n",
       "  'word[-2:]': 'ls',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NNS',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'mexican',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'JJ',\n",
       "  '-1:pos[:2]': 'JJ',\n",
       "  '+1:word.lower()': ',',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': ',',\n",
       "  '+1:pos[:2]': ','},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': ',',\n",
       "  'word[-3:]': ',',\n",
       "  'word[-2:]': ',',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': ',',\n",
       "  'pos[:2]': ',',\n",
       "  '-1:word.lower()': 'nationals',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NNS',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'and',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'CC',\n",
       "  '+1:pos[:2]': 'CC'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'and',\n",
       "  'word[-3:]': 'and',\n",
       "  'word[-2:]': 'nd',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'CC',\n",
       "  'pos[:2]': 'CC',\n",
       "  '-1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': ',',\n",
       "  '-1:pos[:2]': ',',\n",
       "  '+1:word.lower()': 'the',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'DT',\n",
       "  '+1:pos[:2]': 'DT'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'the',\n",
       "  'word[-3:]': 'the',\n",
       "  'word[-2:]': 'he',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'DT',\n",
       "  'pos[:2]': 'DT',\n",
       "  '-1:word.lower()': 'and',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'CC',\n",
       "  '-1:pos[:2]': 'CC',\n",
       "  '+1:word.lower()': 'good',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'JJ',\n",
       "  '+1:pos[:2]': 'JJ'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'good',\n",
       "  'word[-3:]': 'ood',\n",
       "  'word[-2:]': 'od',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'JJ',\n",
       "  'pos[:2]': 'JJ',\n",
       "  '-1:word.lower()': 'the',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'DT',\n",
       "  '-1:pos[:2]': 'DT',\n",
       "  '+1:word.lower()': 'stuff',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'stuff',\n",
       "  'word[-3:]': 'uff',\n",
       "  'word[-2:]': 'ff',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'good',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'JJ',\n",
       "  '-1:pos[:2]': 'JJ',\n",
       "  '+1:word.lower()': 'from',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'IN',\n",
       "  '+1:pos[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'from',\n",
       "  'word[-3:]': 'rom',\n",
       "  'word[-2:]': 'om',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'IN',\n",
       "  'pos[:2]': 'IN',\n",
       "  '-1:word.lower()': 'stuff',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'bc',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': True,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'bc',\n",
       "  'word[-3:]': 'BC',\n",
       "  'word[-2:]': 'BC',\n",
       "  'word.isupper()': True,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'from',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'IN',\n",
       "  '-1:pos[:2]': 'IN',\n",
       "  '+1:word.lower()': 'and',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'CC',\n",
       "  '+1:pos[:2]': 'CC'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'and',\n",
       "  'word[-3:]': 'and',\n",
       "  'word[-2:]': 'nd',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'CC',\n",
       "  'pos[:2]': 'CC',\n",
       "  '-1:word.lower()': 'bc',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': True,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'humbolt',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'humbolt',\n",
       "  'word[-3:]': 'olt',\n",
       "  'word[-2:]': 'lt',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'and',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'CC',\n",
       "  '-1:pos[:2]': 'CC',\n",
       "  '+1:word.lower()': '.',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': '.',\n",
       "  '+1:pos[:2]': '.'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '.',\n",
       "  'word[-3:]': '.',\n",
       "  'word[-2:]': '.',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': '.',\n",
       "  'pos[:2]': '.',\n",
       "  '-1:word.lower()': 'humbolt',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'the',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'DT',\n",
       "  '+1:pos[:2]': 'DT'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'the',\n",
       "  'word[-3:]': 'The',\n",
       "  'word[-2:]': 'he',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'DT',\n",
       "  'pos[:2]': 'DT',\n",
       "  '-1:word.lower()': '.',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': '.',\n",
       "  '-1:pos[:2]': '.',\n",
       "  '+1:word.lower()': 'i5',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': True,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'i5',\n",
       "  'word[-3:]': 'I5',\n",
       "  'word[-2:]': 'I5',\n",
       "  'word.isupper()': True,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'the',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'DT',\n",
       "  '-1:pos[:2]': 'DT',\n",
       "  '+1:word.lower()': 'corridor',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'corridor',\n",
       "  'word[-3:]': 'dor',\n",
       "  'word[-2:]': 'or',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'i5',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': True,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'is',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VBZ',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'is',\n",
       "  'word[-3:]': 'is',\n",
       "  'word[-2:]': 'is',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VBZ',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'corridor',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'still',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'RB',\n",
       "  '+1:pos[:2]': 'RB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'still',\n",
       "  'word[-3:]': 'ill',\n",
       "  'word[-2:]': 'll',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'RB',\n",
       "  'pos[:2]': 'RB',\n",
       "  '-1:word.lower()': 'is',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VBZ',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'just',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'RB',\n",
       "  '+1:pos[:2]': 'RB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'just',\n",
       "  'word[-3:]': 'ust',\n",
       "  'word[-2:]': 'st',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'RB',\n",
       "  'pos[:2]': 'RB',\n",
       "  '-1:word.lower()': 'still',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'RB',\n",
       "  '-1:pos[:2]': 'RB',\n",
       "  '+1:word.lower()': 'as',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'IN',\n",
       "  '+1:pos[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'as',\n",
       "  'word[-3:]': 'as',\n",
       "  'word[-2:]': 'as',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'IN',\n",
       "  'pos[:2]': 'IN',\n",
       "  '-1:word.lower()': 'just',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'RB',\n",
       "  '-1:pos[:2]': 'RB',\n",
       "  '+1:word.lower()': 'busy',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'JJ',\n",
       "  '+1:pos[:2]': 'JJ'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'busy',\n",
       "  'word[-3:]': 'usy',\n",
       "  'word[-2:]': 'sy',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'JJ',\n",
       "  'pos[:2]': 'JJ',\n",
       "  '-1:word.lower()': 'as',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'IN',\n",
       "  '-1:pos[:2]': 'IN',\n",
       "  '+1:word.lower()': ',',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': ',',\n",
       "  '+1:pos[:2]': ','},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': ',',\n",
       "  'word[-3:]': ',',\n",
       "  'word[-2:]': ',',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': ',',\n",
       "  'pos[:2]': ',',\n",
       "  '-1:word.lower()': 'busy',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'JJ',\n",
       "  '-1:pos[:2]': 'JJ',\n",
       "  '+1:word.lower()': 'and',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'CC',\n",
       "  '+1:pos[:2]': 'CC'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'and',\n",
       "  'word[-3:]': 'and',\n",
       "  'word[-2:]': 'nd',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'CC',\n",
       "  'pos[:2]': 'CC',\n",
       "  '-1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': ',',\n",
       "  '-1:pos[:2]': ',',\n",
       "  '+1:word.lower()': 'they',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'PRP',\n",
       "  '+1:pos[:2]': 'PR'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'they',\n",
       "  'word[-3:]': 'hey',\n",
       "  'word[-2:]': 'ey',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'PRP',\n",
       "  'pos[:2]': 'PR',\n",
       "  '-1:word.lower()': 'and',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'CC',\n",
       "  '-1:pos[:2]': 'CC',\n",
       "  '+1:word.lower()': 'are',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VBP',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'are',\n",
       "  'word[-3:]': 'are',\n",
       "  'word[-2:]': 're',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VBP',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'they',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'PRP',\n",
       "  '-1:pos[:2]': 'PR',\n",
       "  '+1:word.lower()': 'still',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'RB',\n",
       "  '+1:pos[:2]': 'RB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'still',\n",
       "  'word[-3:]': 'ill',\n",
       "  'word[-2:]': 'll',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'RB',\n",
       "  'pos[:2]': 'RB',\n",
       "  '-1:word.lower()': 'are',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VBP',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'making',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VBG',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'making',\n",
       "  'word[-3:]': 'ing',\n",
       "  'word[-2:]': 'ng',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VBG',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'still',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'RB',\n",
       "  '-1:pos[:2]': 'RB',\n",
       "  '+1:word.lower()': 'a',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'DT',\n",
       "  '+1:pos[:2]': 'DT'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'a',\n",
       "  'word[-3:]': 'a',\n",
       "  'word[-2:]': 'a',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'DT',\n",
       "  'pos[:2]': 'DT',\n",
       "  '-1:word.lower()': 'making',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VBG',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'ton',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'ton',\n",
       "  'word[-3:]': 'ton',\n",
       "  'word[-2:]': 'on',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'a',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'DT',\n",
       "  '-1:pos[:2]': 'DT',\n",
       "  '+1:word.lower()': 'of',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'IN',\n",
       "  '+1:pos[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'of',\n",
       "  'word[-3:]': 'of',\n",
       "  'word[-2:]': 'of',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'IN',\n",
       "  'pos[:2]': 'IN',\n",
       "  '-1:word.lower()': 'ton',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'money',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'money',\n",
       "  'word[-3:]': 'ney',\n",
       "  'word[-2:]': 'ey',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'of',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'IN',\n",
       "  '-1:pos[:2]': 'IN',\n",
       "  '+1:word.lower()': '.',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': '.',\n",
       "  '+1:pos[:2]': '.'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '.',\n",
       "  'word[-3:]': '.',\n",
       "  'word[-2:]': '.',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': '.',\n",
       "  'pos[:2]': '.',\n",
       "  '-1:word.lower()': 'money',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'nothing',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'nothing',\n",
       "  'word[-3:]': 'ing',\n",
       "  'word[-2:]': 'ng',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': '.',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': '.',\n",
       "  '-1:pos[:2]': '.',\n",
       "  '+1:word.lower()': 'has',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VBZ',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'has',\n",
       "  'word[-3:]': 'has',\n",
       "  'word[-2:]': 'as',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VBZ',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'nothing',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'really',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'RB',\n",
       "  '+1:pos[:2]': 'RB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'really',\n",
       "  'word[-3:]': 'lly',\n",
       "  'word[-2:]': 'ly',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'RB',\n",
       "  'pos[:2]': 'RB',\n",
       "  '-1:word.lower()': 'has',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VBZ',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'changed',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VBN',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'changed',\n",
       "  'word[-3:]': 'ged',\n",
       "  'word[-2:]': 'ed',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VBN',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'really',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'RB',\n",
       "  '-1:pos[:2]': 'RB',\n",
       "  '+1:word.lower()': 'except',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'IN',\n",
       "  '+1:pos[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'except',\n",
       "  'word[-3:]': 'ept',\n",
       "  'word[-2:]': 'pt',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'IN',\n",
       "  'pos[:2]': 'IN',\n",
       "  '-1:word.lower()': 'changed',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VBN',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'that',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'IN',\n",
       "  '+1:pos[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'that',\n",
       "  'word[-3:]': 'hat',\n",
       "  'word[-2:]': 'at',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'IN',\n",
       "  'pos[:2]': 'IN',\n",
       "  '-1:word.lower()': 'except',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'IN',\n",
       "  '-1:pos[:2]': 'IN',\n",
       "  '+1:word.lower()': 'the',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'DT',\n",
       "  '+1:pos[:2]': 'DT'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'the',\n",
       "  'word[-3:]': 'the',\n",
       "  'word[-2:]': 'he',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'DT',\n",
       "  'pos[:2]': 'DT',\n",
       "  '-1:word.lower()': 'that',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'IN',\n",
       "  '-1:pos[:2]': 'IN',\n",
       "  '+1:word.lower()': 'legal',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'JJ',\n",
       "  '+1:pos[:2]': 'JJ'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'legal',\n",
       "  'word[-3:]': 'gal',\n",
       "  'word[-2:]': 'al',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'JJ',\n",
       "  'pos[:2]': 'JJ',\n",
       "  '-1:word.lower()': 'the',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'DT',\n",
       "  '-1:pos[:2]': 'DT',\n",
       "  '+1:word.lower()': 'market',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'market',\n",
       "  'word[-3:]': 'ket',\n",
       "  'word[-2:]': 'et',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'legal',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'JJ',\n",
       "  '-1:pos[:2]': 'JJ',\n",
       "  '+1:word.lower()': 'has',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VBZ',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'has',\n",
       "  'word[-3:]': 'has',\n",
       "  'word[-2:]': 'as',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VBZ',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'market',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'made',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VBN',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'made',\n",
       "  'word[-3:]': 'ade',\n",
       "  'word[-2:]': 'de',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VBN',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'has',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VBZ',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'it',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'PRP',\n",
       "  '+1:pos[:2]': 'PR'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'it',\n",
       "  'word[-3:]': 'it',\n",
       "  'word[-2:]': 'it',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'PRP',\n",
       "  'pos[:2]': 'PR',\n",
       "  '-1:word.lower()': 'made',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VBN',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'easy',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'JJ',\n",
       "  '+1:pos[:2]': 'JJ'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'easy',\n",
       "  'word[-3:]': 'asy',\n",
       "  'word[-2:]': 'sy',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'JJ',\n",
       "  'pos[:2]': 'JJ',\n",
       "  '-1:word.lower()': 'it',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'PRP',\n",
       "  '-1:pos[:2]': 'PR',\n",
       "  '+1:word.lower()': 'for',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'IN',\n",
       "  '+1:pos[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'for',\n",
       "  'word[-3:]': 'for',\n",
       "  'word[-2:]': 'or',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'IN',\n",
       "  'pos[:2]': 'IN',\n",
       "  '-1:word.lower()': 'easy',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'JJ',\n",
       "  '-1:pos[:2]': 'JJ',\n",
       "  '+1:word.lower()': 'anyone',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'anyone',\n",
       "  'word[-3:]': 'one',\n",
       "  'word[-2:]': 'ne',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'for',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'IN',\n",
       "  '-1:pos[:2]': 'IN',\n",
       "  '+1:word.lower()': 'of',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'IN',\n",
       "  '+1:pos[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'of',\n",
       "  'word[-3:]': 'of',\n",
       "  'word[-2:]': 'of',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'IN',\n",
       "  'pos[:2]': 'IN',\n",
       "  '-1:word.lower()': 'anyone',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'legal',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'JJ',\n",
       "  '+1:pos[:2]': 'JJ'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'legal',\n",
       "  'word[-3:]': 'gal',\n",
       "  'word[-2:]': 'al',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'JJ',\n",
       "  'pos[:2]': 'JJ',\n",
       "  '-1:word.lower()': 'of',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'IN',\n",
       "  '-1:pos[:2]': 'IN',\n",
       "  '+1:word.lower()': 'age',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'age',\n",
       "  'word[-3:]': 'age',\n",
       "  'word[-2:]': 'ge',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'legal',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'JJ',\n",
       "  '-1:pos[:2]': 'JJ',\n",
       "  '+1:word.lower()': 'to',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'TO',\n",
       "  '+1:pos[:2]': 'TO'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'to',\n",
       "  'word[-3:]': 'to',\n",
       "  'word[-2:]': 'to',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'TO',\n",
       "  'pos[:2]': 'TO',\n",
       "  '-1:word.lower()': 'age',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'buy',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VB',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'buy',\n",
       "  'word[-3:]': 'buy',\n",
       "  'word[-2:]': 'uy',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VB',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'to',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'TO',\n",
       "  '-1:pos[:2]': 'TO',\n",
       "  '+1:word.lower()': 'it',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'PRP',\n",
       "  '+1:pos[:2]': 'PR'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'it',\n",
       "  'word[-3:]': 'it',\n",
       "  'word[-2:]': 'it',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'PRP',\n",
       "  'pos[:2]': 'PR',\n",
       "  '-1:word.lower()': 'buy',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VB',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': '.',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': '.',\n",
       "  '+1:pos[:2]': '.'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '.',\n",
       "  'word[-3:]': '.',\n",
       "  'word[-2:]': '.',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': '.',\n",
       "  'pos[:2]': '.',\n",
       "  '-1:word.lower()': 'it',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'PRP',\n",
       "  '-1:pos[:2]': 'PR',\n",
       "  '+1:word.lower()': 'and',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'CC',\n",
       "  '+1:pos[:2]': 'CC'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'and',\n",
       "  'word[-3:]': 'And',\n",
       "  'word[-2:]': 'nd',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'CC',\n",
       "  'pos[:2]': 'CC',\n",
       "  '-1:word.lower()': '.',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': '.',\n",
       "  '-1:pos[:2]': '.',\n",
       "  '+1:word.lower()': 'the',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'DT',\n",
       "  '+1:pos[:2]': 'DT'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'the',\n",
       "  'word[-3:]': 'the',\n",
       "  'word[-2:]': 'he',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'DT',\n",
       "  'pos[:2]': 'DT',\n",
       "  '-1:word.lower()': 'and',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'CC',\n",
       "  '-1:pos[:2]': 'CC',\n",
       "  '+1:word.lower()': 'legal',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'JJ',\n",
       "  '+1:pos[:2]': 'JJ'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'legal',\n",
       "  'word[-3:]': 'gal',\n",
       "  'word[-2:]': 'al',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'JJ',\n",
       "  'pos[:2]': 'JJ',\n",
       "  '-1:word.lower()': 'the',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'DT',\n",
       "  '-1:pos[:2]': 'DT',\n",
       "  '+1:word.lower()': 'market',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'market',\n",
       "  'word[-3:]': 'ket',\n",
       "  'word[-2:]': 'et',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'legal',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'JJ',\n",
       "  '-1:pos[:2]': 'JJ',\n",
       "  '+1:word.lower()': 'is',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VBZ',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'is',\n",
       "  'word[-3:]': 'is',\n",
       "  'word[-2:]': 'is',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VBZ',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'market',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'controlled',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VBN',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'controlled',\n",
       "  'word[-3:]': 'led',\n",
       "  'word[-2:]': 'ed',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VBN',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'is',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VBZ',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'by',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'IN',\n",
       "  '+1:pos[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'by',\n",
       "  'word[-3:]': 'by',\n",
       "  'word[-2:]': 'by',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'IN',\n",
       "  'pos[:2]': 'IN',\n",
       "  '-1:word.lower()': 'controlled',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VBN',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'corporate',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'JJ',\n",
       "  '+1:pos[:2]': 'JJ'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'corporate',\n",
       "  'word[-3:]': 'ate',\n",
       "  'word[-2:]': 'te',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'JJ',\n",
       "  'pos[:2]': 'JJ',\n",
       "  '-1:word.lower()': 'by',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'IN',\n",
       "  '-1:pos[:2]': 'IN',\n",
       "  '+1:word.lower()': 'growers',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NNS',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'growers',\n",
       "  'word[-3:]': 'ers',\n",
       "  'word[-2:]': 'rs',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NNS',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'corporate',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'JJ',\n",
       "  '-1:pos[:2]': 'JJ',\n",
       "  '+1:word.lower()': 'with',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'IN',\n",
       "  '+1:pos[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'with',\n",
       "  'word[-3:]': 'ith',\n",
       "  'word[-2:]': 'th',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'IN',\n",
       "  'pos[:2]': 'IN',\n",
       "  '-1:word.lower()': 'growers',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NNS',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'million',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'CD',\n",
       "  '+1:pos[:2]': 'CD'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'million',\n",
       "  'word[-3:]': 'ion',\n",
       "  'word[-2:]': 'on',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'CD',\n",
       "  'pos[:2]': 'CD',\n",
       "  '-1:word.lower()': 'with',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'IN',\n",
       "  '-1:pos[:2]': 'IN',\n",
       "  '+1:word.lower()': 'dollar',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'dollar',\n",
       "  'word[-3:]': 'lar',\n",
       "  'word[-2:]': 'ar',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'million',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'CD',\n",
       "  '-1:pos[:2]': 'CD',\n",
       "  '+1:word.lower()': 'facilities',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NNS',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'facilities',\n",
       "  'word[-3:]': 'ies',\n",
       "  'word[-2:]': 'es',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NNS',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'dollar',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': ',',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': ',',\n",
       "  '+1:pos[:2]': ','},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': ',',\n",
       "  'word[-3:]': ',',\n",
       "  'word[-2:]': ',',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': ',',\n",
       "  'pos[:2]': ',',\n",
       "  '-1:word.lower()': 'facilities',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NNS',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'or',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'CC',\n",
       "  '+1:pos[:2]': 'CC'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'or',\n",
       "  'word[-3:]': 'or',\n",
       "  'word[-2:]': 'or',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'CC',\n",
       "  'pos[:2]': 'CC',\n",
       "  '-1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': ',',\n",
       "  '-1:pos[:2]': ',',\n",
       "  '+1:word.lower()': 'a',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'DT',\n",
       "  '+1:pos[:2]': 'DT'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'a',\n",
       "  'word[-3:]': 'a',\n",
       "  'word[-2:]': 'a',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'DT',\n",
       "  'pos[:2]': 'DT',\n",
       "  '-1:word.lower()': 'or',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'CC',\n",
       "  '-1:pos[:2]': 'CC',\n",
       "  '+1:word.lower()': 'co-op',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'co-op',\n",
       "  'word[-3:]': '-op',\n",
       "  'word[-2:]': 'op',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'a',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'DT',\n",
       "  '-1:pos[:2]': 'DT',\n",
       "  '+1:word.lower()': 'of',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'IN',\n",
       "  '+1:pos[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'of',\n",
       "  'word[-3:]': 'of',\n",
       "  'word[-2:]': 'of',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'IN',\n",
       "  'pos[:2]': 'IN',\n",
       "  '-1:word.lower()': 'co-op',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'smaller',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'JJR',\n",
       "  '+1:pos[:2]': 'JJ'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'smaller',\n",
       "  'word[-3:]': 'ler',\n",
       "  'word[-2:]': 'er',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'JJR',\n",
       "  'pos[:2]': 'JJ',\n",
       "  '-1:word.lower()': 'of',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'IN',\n",
       "  '-1:pos[:2]': 'IN',\n",
       "  '+1:word.lower()': 'growers',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NNS',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'growers',\n",
       "  'word[-3:]': 'ers',\n",
       "  'word[-2:]': 'rs',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NNS',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'smaller',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'JJR',\n",
       "  '-1:pos[:2]': 'JJ',\n",
       "  '+1:word.lower()': '.',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': '.',\n",
       "  '+1:pos[:2]': '.'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '.',\n",
       "  'word[-3:]': '.',\n",
       "  'word[-2:]': '.',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': '.',\n",
       "  'pos[:2]': '.',\n",
       "  '-1:word.lower()': 'growers',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NNS',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'the',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'DT',\n",
       "  '+1:pos[:2]': 'DT'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'the',\n",
       "  'word[-3:]': 'The',\n",
       "  'word[-2:]': 'he',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'DT',\n",
       "  'pos[:2]': 'DT',\n",
       "  '-1:word.lower()': '.',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': '.',\n",
       "  '-1:pos[:2]': '.',\n",
       "  '+1:word.lower()': 'federal',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'JJ',\n",
       "  '+1:pos[:2]': 'JJ'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'federal',\n",
       "  'word[-3:]': 'ral',\n",
       "  'word[-2:]': 'al',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'JJ',\n",
       "  'pos[:2]': 'JJ',\n",
       "  '-1:word.lower()': 'the',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'DT',\n",
       "  '-1:pos[:2]': 'DT',\n",
       "  '+1:word.lower()': 'government',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'government',\n",
       "  'word[-3:]': 'ent',\n",
       "  'word[-2:]': 'nt',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'federal',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'JJ',\n",
       "  '-1:pos[:2]': 'JJ',\n",
       "  '+1:word.lower()': '``',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': '``',\n",
       "  '+1:pos[:2]': '``'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '``',\n",
       "  'word[-3:]': '``',\n",
       "  'word[-2:]': '``',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': '``',\n",
       "  'pos[:2]': '``',\n",
       "  '-1:word.lower()': 'government',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'war',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'war',\n",
       "  'word[-3:]': 'war',\n",
       "  'word[-2:]': 'ar',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': '``',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': '``',\n",
       "  '-1:pos[:2]': '``',\n",
       "  '+1:word.lower()': 'on',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'IN',\n",
       "  '+1:pos[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'on',\n",
       "  'word[-3:]': 'on',\n",
       "  'word[-2:]': 'on',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'IN',\n",
       "  'pos[:2]': 'IN',\n",
       "  '-1:word.lower()': 'war',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'drugs',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NNS',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'drugs',\n",
       "  'word[-3:]': 'ugs',\n",
       "  'word[-2:]': 'gs',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NNS',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'on',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'IN',\n",
       "  '-1:pos[:2]': 'IN',\n",
       "  '+1:word.lower()': \"''\",\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': \"''\",\n",
       "  '+1:pos[:2]': \"''\"},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': \"''\",\n",
       "  'word[-3:]': \"''\",\n",
       "  'word[-2:]': \"''\",\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': \"''\",\n",
       "  'pos[:2]': \"''\",\n",
       "  '-1:word.lower()': 'drugs',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NNS',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'really',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'RB',\n",
       "  '+1:pos[:2]': 'RB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'really',\n",
       "  'word[-3:]': 'lly',\n",
       "  'word[-2:]': 'ly',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'RB',\n",
       "  'pos[:2]': 'RB',\n",
       "  '-1:word.lower()': \"''\",\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': \"''\",\n",
       "  '-1:pos[:2]': \"''\",\n",
       "  '+1:word.lower()': 'has',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VBZ',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'has',\n",
       "  'word[-3:]': 'has',\n",
       "  'word[-2:]': 'as',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VBZ',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'really',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'RB',\n",
       "  '-1:pos[:2]': 'RB',\n",
       "  '+1:word.lower()': 'no',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'DT',\n",
       "  '+1:pos[:2]': 'DT'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'no',\n",
       "  'word[-3:]': 'no',\n",
       "  'word[-2:]': 'no',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'DT',\n",
       "  'pos[:2]': 'DT',\n",
       "  '-1:word.lower()': 'has',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VBZ',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'impact',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'impact',\n",
       "  'word[-3:]': 'act',\n",
       "  'word[-2:]': 'ct',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'no',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'DT',\n",
       "  '-1:pos[:2]': 'DT',\n",
       "  '+1:word.lower()': 'on',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'IN',\n",
       "  '+1:pos[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'on',\n",
       "  'word[-3:]': 'on',\n",
       "  'word[-2:]': 'on',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'IN',\n",
       "  'pos[:2]': 'IN',\n",
       "  '-1:word.lower()': 'impact',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'the',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'DT',\n",
       "  '+1:pos[:2]': 'DT'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'the',\n",
       "  'word[-3:]': 'the',\n",
       "  'word[-2:]': 'he',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'DT',\n",
       "  'pos[:2]': 'DT',\n",
       "  '-1:word.lower()': 'on',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'IN',\n",
       "  '-1:pos[:2]': 'IN',\n",
       "  '+1:word.lower()': 'legal',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'JJ',\n",
       "  '+1:pos[:2]': 'JJ'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'legal',\n",
       "  'word[-3:]': 'gal',\n",
       "  'word[-2:]': 'al',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'JJ',\n",
       "  'pos[:2]': 'JJ',\n",
       "  '-1:word.lower()': 'the',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'DT',\n",
       "  '-1:pos[:2]': 'DT',\n",
       "  '+1:word.lower()': 'market',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'market',\n",
       "  'word[-3:]': 'ket',\n",
       "  'word[-2:]': 'et',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'legal',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'JJ',\n",
       "  '-1:pos[:2]': 'JJ',\n",
       "  '+1:word.lower()': 'in',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'IN',\n",
       "  '+1:pos[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'in',\n",
       "  'word[-3:]': 'in',\n",
       "  'word[-2:]': 'in',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'IN',\n",
       "  'pos[:2]': 'IN',\n",
       "  '-1:word.lower()': 'market',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'oregon',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'oregon',\n",
       "  'word[-3:]': 'gon',\n",
       "  'word[-2:]': 'on',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'in',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'IN',\n",
       "  '-1:pos[:2]': 'IN',\n",
       "  '+1:word.lower()': '.',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': '.',\n",
       "  '+1:pos[:2]': '.'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '.',\n",
       "  'word[-3:]': '.',\n",
       "  'word[-2:]': '.',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': '.',\n",
       "  'pos[:2]': '.',\n",
       "  '-1:word.lower()': 'oregon',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'i',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': True,\n",
       "  '+1:pos': 'PRP',\n",
       "  '+1:pos[:2]': 'PR'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'i',\n",
       "  'word[-3:]': 'I',\n",
       "  'word[-2:]': 'I',\n",
       "  'word.isupper()': True,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'PRP',\n",
       "  'pos[:2]': 'PR',\n",
       "  '-1:word.lower()': '.',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': '.',\n",
       "  '-1:pos[:2]': '.',\n",
       "  '+1:word.lower()': 'do',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VB',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'do',\n",
       "  'word[-3:]': 'do',\n",
       "  'word[-2:]': 'do',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VB',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': 'i',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': True,\n",
       "  '-1:pos': 'PRP',\n",
       "  '-1:pos[:2]': 'PR',\n",
       "  '+1:word.lower()': \"n't\",\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'RB',\n",
       "  '+1:pos[:2]': 'RB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': \"n't\",\n",
       "  'word[-3:]': \"n't\",\n",
       "  'word[-2:]': \"'t\",\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'RB',\n",
       "  'pos[:2]': 'RB',\n",
       "  '-1:word.lower()': 'do',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VB',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'see',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'VB',\n",
       "  '+1:pos[:2]': 'VB'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'see',\n",
       "  'word[-3:]': 'see',\n",
       "  'word[-2:]': 'ee',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'VB',\n",
       "  'pos[:2]': 'VB',\n",
       "  '-1:word.lower()': \"n't\",\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'RB',\n",
       "  '-1:pos[:2]': 'RB',\n",
       "  '+1:word.lower()': 'any',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'DT',\n",
       "  '+1:pos[:2]': 'DT'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'any',\n",
       "  'word[-3:]': 'any',\n",
       "  'word[-2:]': 'ny',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'DT',\n",
       "  'pos[:2]': 'DT',\n",
       "  '-1:word.lower()': 'see',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'VB',\n",
       "  '-1:pos[:2]': 'VB',\n",
       "  '+1:word.lower()': 'glutton',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'glutton',\n",
       "  'word[-3:]': 'ton',\n",
       "  'word[-2:]': 'on',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'any',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'DT',\n",
       "  '-1:pos[:2]': 'DT',\n",
       "  '+1:word.lower()': 'of',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'IN',\n",
       "  '+1:pos[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'of',\n",
       "  'word[-3:]': 'of',\n",
       "  'word[-2:]': 'of',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'IN',\n",
       "  'pos[:2]': 'IN',\n",
       "  '-1:word.lower()': 'glutton',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'weed',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NN',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'weed',\n",
       "  'word[-3:]': 'eed',\n",
       "  'word[-2:]': 'ed',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NN',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'of',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'IN',\n",
       "  '-1:pos[:2]': 'IN',\n",
       "  '+1:word.lower()': ',',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': ',',\n",
       "  '+1:pos[:2]': ','},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': ',',\n",
       "  'word[-3:]': ',',\n",
       "  'word[-2:]': ',',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': ',',\n",
       "  'pos[:2]': ',',\n",
       "  '-1:word.lower()': 'weed',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NN',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  '+1:word.lower()': 'whatever',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'WDT',\n",
       "  '+1:pos[:2]': 'WD'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'whatever',\n",
       "  'word[-3:]': 'ver',\n",
       "  'word[-2:]': 'er',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'WDT',\n",
       "  'pos[:2]': 'WD',\n",
       "  '-1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': ',',\n",
       "  '-1:pos[:2]': ',',\n",
       "  '+1:word.lower()': 'that',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'IN',\n",
       "  '+1:pos[:2]': 'IN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'that',\n",
       "  'word[-3:]': 'hat',\n",
       "  'word[-2:]': 'at',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'IN',\n",
       "  'pos[:2]': 'IN',\n",
       "  '-1:word.lower()': 'whatever',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'WDT',\n",
       "  '-1:pos[:2]': 'WD',\n",
       "  '+1:word.lower()': 'means',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': 'NNS',\n",
       "  '+1:pos[:2]': 'NN'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': 'means',\n",
       "  'word[-3:]': 'ans',\n",
       "  'word[-2:]': 'ns',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': 'NNS',\n",
       "  'pos[:2]': 'NN',\n",
       "  '-1:word.lower()': 'that',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'IN',\n",
       "  '-1:pos[:2]': 'IN',\n",
       "  '+1:word.lower()': '.',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:pos': '.',\n",
       "  '+1:pos[:2]': '.'},\n",
       " {'bias': 1.0,\n",
       "  'word.lower()': '.',\n",
       "  'word[-3:]': '.',\n",
       "  'word[-2:]': '.',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'pos': '.',\n",
       "  'pos[:2]': '.',\n",
       "  '-1:word.lower()': 'means',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:pos': 'NNS',\n",
       "  '-1:pos[:2]': 'NN',\n",
       "  'EOS': True}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm=None, all_possible_states=None, all_possible_transitions=None,\n",
       "    averaging=None, c=None, c1=None, c2=None, calibration_candidates=None,\n",
       "    calibration_eta=None, calibration_max_trials=None, calibration_rate=None,\n",
       "    calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "    gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=None,\n",
       "    max_linesearch=None, min_freq=None, model_filename=None, num_memories=None,\n",
       "    pa_type=None, period=None, trainer_cls=None, variance=None, verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sklearn_crfsuite.CRF()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How', 'O'),\n",
       " ('about', 'O'),\n",
       " ('we', 'O'),\n",
       " ('stop', 'O'),\n",
       " ('protecting', 'O'),\n",
       " ('idiots', 'B-SPAN'),\n",
       " ('and', 'O'),\n",
       " ('let', 'O'),\n",
       " ('nature', 'O'),\n",
       " ('add', 'O'),\n",
       " ('some', 'O'),\n",
       " ('bleach', 'O'),\n",
       " ('to', 'O'),\n",
       " ('the', 'O'),\n",
       " ('gene', 'O'),\n",
       " ('pool', 'O'),\n",
       " ('.', 'O'),\n",
       " ('We', 'O'),\n",
       " ('can', 'O'),\n",
       " ('always', 'O'),\n",
       " ('submit', 'O'),\n",
       " ('their', 'O'),\n",
       " ('names', 'O'),\n",
       " ('for', 'O'),\n",
       " ('the', 'O'),\n",
       " ('Darwin', 'O'),\n",
       " ('awards', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_data = test_dataset[1]\n",
    "X_pred = [sentence_features(sentence[0]) for sentence in [pred_data]]\n",
    "preds = model.predict(X_pred)\n",
    "\n",
    "list(zip([e[0] for e in pred_data[0]], preds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.5230179028132992\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "X_test = [sentence_features(sentence[0]) for sentence in test_dataset]\n",
    "preds = model.predict(X_test)\n",
    "f1_score = f1_score([e[1] for e in test_dataset], preds)\n",
    "\n",
    "print(f'F1 score: {f1_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BI-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0187,  0.1713, -0.2944]],\n",
      "\n",
      "        [[-0.3521,  0.1026, -0.2971]],\n",
      "\n",
      "        [[-0.3191,  0.0781, -0.1957]],\n",
      "\n",
      "        [[-0.1634,  0.0941, -0.1637]],\n",
      "\n",
      "        [[-0.3368,  0.0959, -0.0538]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[-0.3368,  0.0959, -0.0538]]], grad_fn=<StackBackward>), tensor([[[-0.9825,  0.4715, -0.0633]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "# Example network\n",
    "\n",
    "lstm = nn.LSTM(input_size=3, hidden_size=3) \n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # for a sequence of length 5\n",
    "\n",
    "\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "for i in inputs:\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary to convert words to indexes:\n",
      "{'PAD': 0, ('Another', 'DT'): 1, ('violent', 'NN'): 2, ('and', 'CC'): 3, ('aggressive', 'JJ'): 4, ('immigrant', 'NN'): 5, ('killing', 'VBG'): 6, ('a', 'DT'): 7, ('innocent', 'NN'): 8, ('intelligent', 'NN'): 9, ('US', 'NN'): 10, ('Citizen', 'NN'): 11, ('...', ':'): 12, ('.', '.'): 13, ('Sarcasm', 'NN'): 14, ('I', 'PRP'): 15, ('am', 'VBP'): 16, ('56', 'CD'): 17, ('years', 'NNS'): 18, ('old', 'JJ'): 19, (',', ','): 20, ('not', 'RB'): 21, ('your', 'PRP$'): 22, ('fucking', 'VBG'): 23, ('junior', 'NN'): 24, ('pal', 'NN'): 25, ('What', 'WP'): 26, ('you', 'PRP'): 27, ('are', 'VBP'): 28, ('saying', 'VBG'): 29, ('makes', 'VBZ'): 30, ('no', 'DT'): 31, ('sense', 'NN'): 32, ('do', 'VB'): 33, (\"n't\", 'RB'): 34, ('know', 'VB'): 35, ('what', 'WP'): 36, ('basing', 'VBG'): 37, ('this', 'DT'): 38, ('on', 'IN'): 39, ('The', 'DT'): 40, ('cheap', 'NN'): 41, ('black', 'JJ'): 42, ('market', 'NN'): 43, ('crap', 'NN'): 44, ('is', 'VBZ'): 45, ('still', 'RB'): 46, ('coming', 'VBG'): 47, ('up', 'RB'): 48, ('from', 'IN'): 49, ('Mexican', 'JJ'): 50, ('nationals', 'NNS'): 51, ('the', 'DT'): 52, ('good', 'JJ'): 53, ('stuff', 'NN'): 54, ('BC', 'NN'): 55, ('Humbolt', 'NN'): 56, ('I5', 'NN'): 57, ('corridor', 'NN'): 58, ('just', 'RB'): 59, ('as', 'IN'): 60, ('busy', 'JJ'): 61, ('they', 'PRP'): 62, ('making', 'VBG'): 63, ('ton', 'NN'): 64, ('of', 'IN'): 65, ('money', 'NN'): 66, ('Nothing', 'NN'): 67, ('has', 'VBZ'): 68, ('really', 'RB'): 69, ('changed', 'VBN'): 70, ('except', 'IN'): 71, ('that', 'IN'): 72, ('legal', 'JJ'): 73, ('made', 'VBN'): 74, ('it', 'PRP'): 75, ('easy', 'JJ'): 76, ('for', 'IN'): 77, ('anyone', 'NN'): 78, ('age', 'NN'): 79, ('to', 'TO'): 80, ('buy', 'VB'): 81, ('And', 'CC'): 82, ('controlled', 'VBN'): 83, ('by', 'IN'): 84, ('corporate', 'JJ'): 85, ('growers', 'NNS'): 86, ('with', 'IN'): 87, ('million', 'CD'): 88, ('dollar', 'NN'): 89, ('facilities', 'NNS'): 90, ('or', 'CC'): 91, ('co-op', 'NN'): 92, ('smaller', 'JJR'): 93, ('Federal', 'JJ'): 94, ('government', 'NN'): 95, ('``', '``'): 96, ('war', 'NN'): 97, ('drugs', 'NNS'): 98, (\"''\", \"''\"): 99}\n",
      "\n",
      "Vocabulary to convert tags to indexes:\n",
      "{'PAD': 0, 'O': 1, 'B-SPAN': 2, 'I-SPAN': 3}\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {}\n",
    "tag_to_ix = {}\n",
    "\n",
    "word_to_ix['PAD'] = 0\n",
    "tag_to_ix['PAD'] = 0\n",
    "\n",
    "for sent, tags in train_dataset:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "    \n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "print('Vocabulary to convert words to indexes:')\n",
    "print(dict(list(word_to_ix.items())[:100]))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Vocabulary to convert tags to indexes:')\n",
    "print(tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class BasicLstmTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, \n",
    "                 tagset_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "\n",
    "    def forward(self, sentences, lengths):\n",
    "        embeds = self.word_embeddings(sentences)\n",
    "\n",
    "        packed_embeds = pack_padded_sequence(embeds.transpose(0, 1), lengths, enforce_sorted=False)\n",
    "        packed_lstm_out, _ = self.lstm(packed_embeds)\n",
    "        lstm_out, _ = pad_packed_sequence(packed_lstm_out)\n",
    "\n",
    "        tag_space = self.hidden2tag(self.dropout(lstm_out.transpose(0, 1)))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=-1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicLstmTagger(embedding_dim=300, \n",
    "                        hidden_dim=200, \n",
    "                        vocab_size=len(word_to_ix), \n",
    "                        tagset_size=len(tag_to_ix)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tokens(batch_tokens, word_to_ix):\n",
    "    batch_token_ids = [[word_to_ix.get(tok, 0) for tok in inst] for inst in batch_tokens]\n",
    "    batch_max_len = max([len(s) for s in batch_token_ids])\n",
    "    \n",
    "    batch_token_ids_padded = word_to_ix['PAD']*np.ones((len(batch_token_ids), batch_max_len))\n",
    "    lengths = []\n",
    "    for i in range(len(batch_token_ids)):\n",
    "        cur_len = len(batch_token_ids[i])\n",
    "        batch_token_ids_padded[i][:cur_len] = batch_token_ids[i]\n",
    "        lengths.append(cur_len)\n",
    "\n",
    "    #since all data are indices, we convert them to torch LongTensors\n",
    "    return torch.LongTensor(batch_token_ids_padded), lengths\n",
    "\n",
    "\n",
    "def prepare_tags(batch_tags, tag_to_ix):\n",
    "    batch_tag_ids = [[tag_to_ix[tag] for tag in inst] for inst in batch_tags]\n",
    "    batch_max_len = max([len(s) for s in batch_tag_ids])\n",
    "\n",
    "    batch_tag_ids_padded = tag_to_ix['PAD']*np.ones((len(batch_tag_ids), batch_max_len))\n",
    "    for i in range(len(batch_tag_ids)):\n",
    "        cur_len = len(batch_tag_ids[i])\n",
    "        batch_tag_ids_padded[i][:cur_len] = batch_tag_ids[i]\n",
    "\n",
    "    #since all data are indices, we convert them to torch LongTensors\n",
    "    return torch.LongTensor(batch_tag_ids_padded)\n",
    "\n",
    "\n",
    "def prepare_instance_for_training(batch_instances, word_to_ix, tag_to_ix):\n",
    "    tokens, lengths = prepare_tokens([inst[0] for inst in batch_instances], word_to_ix)\n",
    "    tags = prepare_tags([inst[1] for inst in batch_instances], tag_to_ix)\n",
    "\n",
    "    return tokens, lengths, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('I', 'PRP'), ('am', 'VBP'), ('56', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('I', 'PRP'), ('am', 'VBP'), ('not', 'RB'), ('your', 'PRP$'), ('fucking', 'VBG'), ('junior', 'NN'), ('pal', 'NN'), ('.', '.'), ('What', 'WP'), ('you', 'PRP'), ('are', 'VBP'), ('saying', 'VBG'), ('makes', 'VBZ'), ('no', 'DT'), ('sense', 'NN'), ('.', '.'), ('I', 'PRP'), ('do', 'VB'), (\"n't\", 'RB'), ('know', 'VB'), ('what', 'WP'), ('you', 'PRP'), ('are', 'VBP'), ('basing', 'VBG'), ('this', 'DT'), ('on', 'IN'), ('.', '.'), ('The', 'DT'), ('cheap', 'NN'), ('black', 'JJ'), ('market', 'NN'), ('crap', 'NN'), ('is', 'VBZ'), ('still', 'RB'), ('coming', 'VBG'), ('up', 'RB'), ('from', 'IN'), ('Mexican', 'JJ'), ('nationals', 'NNS'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('good', 'JJ'), ('stuff', 'NN'), ('from', 'IN'), ('BC', 'NN'), ('and', 'CC'), ('Humbolt', 'NN'), ('.', '.'), ('The', 'DT'), ('I5', 'NN'), ('corridor', 'NN'), ('is', 'VBZ'), ('still', 'RB'), ('just', 'RB'), ('as', 'IN'), ('busy', 'JJ'), (',', ','), ('and', 'CC'), ('they', 'PRP'), ('are', 'VBP'), ('still', 'RB'), ('making', 'VBG'), ('a', 'DT'), ('ton', 'NN'), ('of', 'IN'), ('money', 'NN'), ('.', '.'), ('Nothing', 'NN'), ('has', 'VBZ'), ('really', 'RB'), ('changed', 'VBN'), ('except', 'IN'), ('that', 'IN'), ('the', 'DT'), ('legal', 'JJ'), ('market', 'NN'), ('has', 'VBZ'), ('made', 'VBN'), ('it', 'PRP'), ('easy', 'JJ'), ('for', 'IN'), ('anyone', 'NN'), ('of', 'IN'), ('legal', 'JJ'), ('age', 'NN'), ('to', 'TO'), ('buy', 'VB'), ('it', 'PRP'), ('.', '.'), ('And', 'CC'), ('the', 'DT'), ('legal', 'JJ'), ('market', 'NN'), ('is', 'VBZ'), ('controlled', 'VBN'), ('by', 'IN'), ('corporate', 'JJ'), ('growers', 'NNS'), ('with', 'IN'), ('million', 'CD'), ('dollar', 'NN'), ('facilities', 'NNS'), (',', ','), ('or', 'CC'), ('a', 'DT'), ('co-op', 'NN'), ('of', 'IN'), ('smaller', 'JJR'), ('growers', 'NNS'), ('.', '.'), ('The', 'DT'), ('Federal', 'JJ'), ('government', 'NN'), ('``', '``'), ('war', 'NN'), ('on', 'IN'), ('drugs', 'NNS'), (\"''\", \"''\"), ('really', 'RB'), ('has', 'VBZ'), ('no', 'DT'), ('impact', 'NN'), ('on', 'IN'), ('the', 'DT'), ('legal', 'JJ'), ('market', 'NN'), ('in', 'IN'), ('Oregon', 'NN'), ('.', '.'), ('I', 'PRP'), ('do', 'VB'), (\"n't\", 'RB'), ('see', 'VB'), ('any', 'DT'), ('glutton', 'NN'), ('of', 'IN'), ('weed', 'NN'), (',', ','), ('whatever', 'WDT'), ('that', 'IN'), ('means', 'NNS'), ('.', '.')], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-SPAN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d384482bd51649f58b257d6a6facfc87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ee991ab133438180f3b8bcb2bf0098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3ca493ce0d4561ad4e7be921e382a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f007bbfb50e4621a3cf283aa3f3bd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f93dbb42ff46ec83260784e6ccaada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071e64d1d2fd4bea85f4cb9eb379727f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb25563a65fa4a4aafd6aa19d082afff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043c80e7d3f04ad787b1de54abecf91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8676ddc66bb143db9357ef2578819e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc5662152424210971fbef739cd0d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "N_EPOCHS = 10\n",
    "LEARNING_RATE = 0.1\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "loss_function = nn.NLLLoss(ignore_index=tag_to_ix['PAD'])\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "f_preprocess = lambda batch: prepare_instance_for_training(batch, word_to_ix, tag_to_ix)\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              num_workers=1,\n",
    "                              collate_fn=f_preprocess)\n",
    "\n",
    "try:\n",
    "    model.train()\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        print(f'Epoch #{epoch}')\n",
    "\n",
    "        for sentences, lengths, targets in tqdm(train_dataloader):\n",
    "            # Move data to the GPU.\n",
    "            sentences = sentences.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            # Pytorch accumulates gradients. We clear them out before each instance.\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Run our forward pass.\n",
    "            tag_scores = model(sentences, lengths)\n",
    "\n",
    "            # Compute the loss, gradients, and update the parameters. \n",
    "            loss = loss_function(tag_scores.reshape(-1, tag_scores.shape[-1]), targets.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "pred_dataset = [inst[0] for inst in test_dataset]\n",
    "\n",
    "f_preprocess = lambda batch: prepare_tokens(batch, word_to_ix)\n",
    "pred_dataloader = DataLoader(pred_dataset, \n",
    "                             batch_size=100, \n",
    "                             num_workers=1,\n",
    "                             collate_fn=f_preprocess)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_probas = []\n",
    "with torch.no_grad():\n",
    "    for sentences, lengths in pred_dataloader:\n",
    "        sentences = sentences.cuda()\n",
    "\n",
    "        tag_scores = model(sentences, lengths)\n",
    "        probas, pred_tags = tag_scores.max(dim=-1)\n",
    "        probas = torch.exp(probas)\n",
    "        \n",
    "        pred_tags = pred_tags.cpu().tolist()\n",
    "        probas = probas.cpu().tolist()\n",
    "        all_preds += pred_tags\n",
    "        all_probas += probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('It', 'PRP'), 'O', 0.9478151798248291),\n",
       " (('did', 'VBD'), 'O', 0.9802039265632629),\n",
       " ((\"n't\", 'RB'), 'O', 0.9680961966514587),\n",
       " (('take', 'VB'), 'O', 0.9443789720535278),\n",
       " (('much', 'JJ'), 'O', 0.9567553997039795),\n",
       " (('rope', 'NN'), 'O', 0.8854822516441345),\n",
       " (('for', 'IN'), 'O', 0.9473097920417786),\n",
       " (('the', 'DT'), 'O', 0.9666978120803833),\n",
       " (('village', 'NN'), 'O', 0.9211320281028748),\n",
       " (('idiot', 'NN'), 'B-SPAN', 0.8248081803321838),\n",
       " (('to', 'TO'), 'O', 0.9412413835525513),\n",
       " (('hang', 'NN'), 'O', 0.8933104872703552),\n",
       " (('himself', 'PRP'), 'O', 0.8529583811759949),\n",
       " (('.', '.'), 'O', 0.998956561088562)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restoring tag strings\n",
    "\n",
    "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
    "\n",
    "pred_tags = []\n",
    "pred_probas = []\n",
    "for pred_sent, preds, probas in zip(pred_dataset, all_preds, all_probas):\n",
    "    pred_tags.append([ix_to_tag[tag_idx] for tag_idx in preds[:len(pred_sent)]])\n",
    "    pred_probas.append(probas[:len(pred_sent)])\n",
    "\n",
    "idx = 21\n",
    "list(zip(pred_dataset[idx], pred_tags[idx], pred_probas[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5025252525252525"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "f1_score(pred_tags, [inst[1] for inst in test_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AllenNLP LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from allennlp.data import allennlp_collate\n",
    "from allennlp.training.util import evaluate\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b62ea5446140e6a55e6c49886fa168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', layout=Layout(width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8002ab7c52c43deb62fbef19c04f2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', layout=Layout(width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f67fc1999c43bdb128ff387cebcde1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', layout=Layout(width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.dataset_readers.sequence_tagging import SequenceTaggingDatasetReader\n",
    "from allennlp.data.dataset_readers import Conll2003DatasetReader\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "\n",
    "\n",
    "indexer = SingleIdTokenIndexer()\n",
    "#reader = Conll2003DatasetReader(token_indexers={'tokens':indexer})\n",
    "reader = SequenceTaggingDatasetReader(token_indexers={'tokens':indexer})\n",
    "train_dataset = reader.read('data/train.seq')\n",
    "dev_dataset = reader.read('data/dev.seq')\n",
    "test_dataset = reader.read('data/trial.seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance with fields:\n",
      " \t tokens: TextField of length 15 with text: \n",
      " \t\t[Another, violent, and, aggressive, immigrant, killing, a, innocent, and, intelligent, US, Citizen,\n",
      "\t\t..., ., Sarcasm]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \t metadata: MetadataField (print field.metadata to see specific information). \n",
      " \t tags: SequenceLabelField of length 15 with labels:\n",
      " \t\t['O', 'B-SPAN', 'I-SPAN', 'I-SPAN', 'I-SPAN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      " \t\tin namespace: 'labels'. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance with fields:\n",
      " \t tokens: TextField of length 8 with text: \n",
      " \t\t[Nice, come-back, to, quite, a, stupid, comment, .]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \t metadata: MetadataField (print field.metadata to see specific information). \n",
      " \t tags: SequenceLabelField of length 8 with labels:\n",
      " \t\t['O', 'O', 'O', 'O', 'O', 'B-SPAN', 'O', 'O']\n",
      " \t\tin namespace: 'labels'. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dev_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc2a21958b54d358fe6593bbd0aa2d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='building vocab', max=6351.0, style=ProgressStyle(descript"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "vocab = Vocabulary.from_instances(train_dataset.instances)\n",
    "train_dataset.index_with(vocab)\n",
    "dev_dataset.index_with(vocab)\n",
    "test_dataset.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary with namespaces:\n",
      " \tNon Padded Namespaces: {'*tags', '*labels'}\n",
      " \tNamespace: tokens, Size: 21875 \n",
      " \tNamespace: labels, Size: 3 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.seq2seq_encoders.pytorch_seq2seq_wrapper import LstmSeq2SeqEncoder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "\n",
    "\n",
    "embedder = Embedding(embedding_dim=300, vocab=vocab)\n",
    "text_field_embedder = BasicTextFieldEmbedder({'tokens': embedder})\n",
    "encoder = LstmSeq2SeqEncoder(input_size=embedder.get_output_dim(), hidden_size=200, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import SimpleTagger\n",
    "\n",
    "\n",
    "model = SimpleTagger(text_field_embedder=text_field_embedder, \n",
    "                     vocab=vocab, \n",
    "                     encoder=encoder,\n",
    "                     calculate_span_f1=True,\n",
    "                     label_encoding='IOB1').cuda(device=cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleTagger(\n",
       "  (text_field_embedder): BasicTextFieldEmbedder(\n",
       "    (token_embedder_tokens): Embedding()\n",
       "  )\n",
       "  (encoder): LstmSeq2SeqEncoder(\n",
       "    (_module): LSTM(300, 200, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (tag_projection_layer): TimeDistributed(\n",
       "    (_module): Linear(in_features=400, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625aec7007c74d728331990c617b2cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb4abe80aac4daea777cd6759a8839d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666791fc35904968a6469426b7cdb21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44990369c75436184baf614b9b48519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f741bb665741e6bba539cc94cfa2f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f165698d1144e3b9fbfc35c17f37c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05354cbe34794e36ad224ae757725376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34bc6fb011243e7b31e414a25fe7db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d989cf3b9ed48238c4a2e93b3145f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc87bc5c68b54c4ca365d833f3d549b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61caaf099a2349bf98ad3c3c52a2f8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d6491945154af6b23f7d97304dab3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa01bfee476d4e8585137775fdef5f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed22ecb5fc4456789ebc29832b51461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1849d438ea544566a149b2f1217ffb25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b2130bf79e4c6c962014a93a04a70d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bd1d101eeb445b9419918508dfd965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d8e54c1b62424ea3580d15ceaf44fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ee899d972d43f8a035be2800ddbdfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8d80733e0642f1b412b01132d8d6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "\n",
    "from allennlp.training.learning_rate_schedulers import ReduceOnPlateauLearningRateScheduler\n",
    "from allennlp.data import allennlp_collate\n",
    "from allennlp.training import GradientDescentTrainer\n",
    "\n",
    "date_time = datetime.now()\n",
    "date_str = date_time.strftime('%m/%d/%Y')\n",
    "time_str = date_time.strftime('%H:%M:%S')\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=0.0)\n",
    "train_data_loader = DataLoader(dataset=train_dataset, batch_size=8, \n",
    "                               collate_fn=allennlp_collate, shuffle=True)\n",
    "dev_data_loader = DataLoader(dataset=dev_dataset, batch_size=100, collate_fn=allennlp_collate)\n",
    "lr_scheduler = ReduceOnPlateauLearningRateScheduler(optimizer, patience=1, factor=0.5)\n",
    "\n",
    "trainer = GradientDescentTrainer(\n",
    "    model=model,\n",
    "    serialization_dir=f'./workdir/{date_str}/{time_str}',\n",
    "    data_loader=train_data_loader,\n",
    "    validation_data_loader=dev_data_loader,\n",
    "    num_epochs=10,\n",
    "    optimizer=optimizer,\n",
    "    cuda_device=cuda_device,\n",
    "    learning_rate_scheduler=lr_scheduler,\n",
    "    grad_clipping=1.\n",
    ")\n",
    "\n",
    "try:\n",
    "    metrics = trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6299a09a9f64fc99de690c9ddc0f120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9471671388101983,\n",
       " 'accuracy3': 1.0,\n",
       " 'precision-overall': 0.753246753246753,\n",
       " 'recall-overall': 0.29896907216494845,\n",
       " 'f1-measure-overall': 0.4280442804427637,\n",
       " 'loss': 0.30225372101579395}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp.training.util import evaluate\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=100, collate_fn=allennlp_collate)\n",
    "evaluate(model, test_dataloader, cuda_device=cuda_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d24e937af354f9e802322b17b98bd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "glove_file = f'https://allennlp.s3.amazonaws.com/datasets/glove/glove.6B.{embedding_dim}d.txt.gz'\n",
    "\n",
    "embedder = Embedding(embedding_dim=embedding_dim, vocab=vocab, \n",
    "                     pretrained_file=glove_file, trainable=False)\n",
    "text_field_embedder = BasicTextFieldEmbedder({'tokens': embedder})\n",
    "encoder = LstmSeq2SeqEncoder(input_size=embedder.get_output_dim(), hidden_size=200, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import SimpleTagger\n",
    "\n",
    "\n",
    "model = SimpleTagger(text_field_embedder=text_field_embedder, \n",
    "                     vocab=vocab, \n",
    "                     encoder=encoder,\n",
    "                     calculate_span_f1=True,\n",
    "                     label_encoding='IOB1').cuda(device=cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442a73b3b57d49979e3fa84a812e760e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "from allennlp.training.learning_rate_schedulers import ReduceOnPlateauLearningRateScheduler\n",
    "from allennlp.training import GradientDescentTrainer\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=0.)\n",
    "train_data_loader = DataLoader(dataset=train_dataset, batch_size=8, \n",
    "                               collate_fn=allennlp_collate, shuffle=True)\n",
    "dev_data_loader = DataLoader(dataset=dev_dataset, batch_size=100, collate_fn=allennlp_collate)\n",
    "lr_scheduler = ReduceOnPlateauLearningRateScheduler(optimizer, patience=1, factor=0.5)\n",
    "\n",
    "date_time = datetime.now()\n",
    "date_str = date_time.strftime('%m/%d/%Y')\n",
    "time_str = date_time.strftime('%H:%M:%S')\n",
    "\n",
    "trainer = GradientDescentTrainer(\n",
    "    model=model,\n",
    "    serialization_dir=f'./workdir/{date_str}/{time_str}',\n",
    "    data_loader=train_data_loader,\n",
    "    validation_data_loader=dev_data_loader,\n",
    "    num_epochs=10,\n",
    "    optimizer=optimizer,\n",
    "    cuda_device=cuda_device,\n",
    "    learning_rate_scheduler=lr_scheduler,\n",
    "    grad_clipping=1., \n",
    "    patience = 2\n",
    ")\n",
    "\n",
    "try:\n",
    "    metrics = trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7f18ee960944dcb3f26fff607f828e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9316495623972633,\n",
       " 'accuracy3': 1.0,\n",
       " 'precision-overall': 0.07377049180327863,\n",
       " 'recall-overall': 0.0029004189494038026,\n",
       " 'f1-measure-overall': 0.005581395348829929,\n",
       " 'loss': 0.3618400309767042}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=100, collate_fn=allennlp_collate)\n",
    "evaluate(model, test_dataloader, cuda_device=cuda_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With CRF output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp_models.tagging import CrfTagger\n",
    "\n",
    "\n",
    "model = CrfTagger(text_field_embedder=text_field_embedder, \n",
    "                  vocab=vocab, \n",
    "                  encoder=encoder,\n",
    "                  calculate_span_f1=True,\n",
    "                  label_encoding='IOB1').cuda(device=cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ce0804dd084281907a13a83a08b9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from allennlp.training.learning_rate_schedulers import ReduceOnPlateauLearningRateScheduler\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05, weight_decay=0.001)\n",
    "train_data_loader = DataLoader(dataset=train_dataset, batch_size=8, \n",
    "                               collate_fn=allennlp_collate, shuffle=True)\n",
    "dev_data_loader = DataLoader(dataset=dev_dataset, batch_size=100, collate_fn=allennlp_collate)\n",
    "lr_scheduler = ReduceOnPlateauLearningRateScheduler(optimizer, patience=1, factor=0.5)\n",
    "\n",
    "date_time = datetime.now()\n",
    "date_str = date_time.strftime('%m/%d/%Y')\n",
    "time_str = date_time.strftime('%H:%M:%S')\n",
    "\n",
    "trainer = GradientDescentTrainer(\n",
    "    model=model,\n",
    "    serialization_dir=f'./workdir/{date_str}/{time_str}',\n",
    "    data_loader=train_data_loader,\n",
    "    validation_data_loader=dev_data_loader,\n",
    "    num_epochs=8,\n",
    "    optimizer=optimizer,\n",
    "    cuda_device=cuda_device,\n",
    "    learning_rate_scheduler=lr_scheduler,\n",
    "    grad_clipping=1.\n",
    ")\n",
    "\n",
    "try:\n",
    "    metrics = trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d51a281f2ac48918d6e44b5f4aa84a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9307011331444759,\n",
       " 'accuracy3': 1.0,\n",
       " 'precision-overall': 0.06666666666666661,\n",
       " 'recall-overall': 0.007216494845360824,\n",
       " 'f1-measure-overall': 0.013023255813935859,\n",
       " 'loss': 773.1597551618304}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=100, collate_fn=allennlp_collate)\n",
    "evaluate(model, test_dataloader, cuda_device=cuda_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With convolutional character encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5447eb7349a94e21bfb99c37c3fa5e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', layout=Layout(width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfc1cb842b046a4973fbf193d0ae51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', layout=Layout(width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8c8e8b5f484baa8d2ac13fab55491c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', layout=Layout(width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.token_indexers import SingleIdTokenIndexer, TokenCharactersIndexer\n",
    "from allennlp.data.dataset_readers import Conll2003DatasetReader\n",
    "from allennlp.data.dataset_readers.sequence_tagging import SequenceTaggingDatasetReader\n",
    "\n",
    "token_indexer = SingleIdTokenIndexer()\n",
    "char_indexer = TokenCharactersIndexer(min_padding_length=5)\n",
    "#reader = SequenceTaggingDatasetReader(token_indexers={'tokens' : token_indexer, \n",
    "#                                                      'chars' : char_indexer})\n",
    "reader = Conll2003DatasetReader(token_indexers={'tokens': token_indexer, \n",
    "                                                'chars': char_indexer})\n",
    "train_dataset = reader.read('data/train.conll2003')\n",
    "test_dataset = reader.read('data/trial.conll2003')\n",
    "dev_dataset = reader.read('data/dev.conll2003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ae477e9d694db9b4d7a6c83b57d609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='building vocab', max=6351.0, style=ProgressStyle(descript"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "vocab = Vocabulary.from_instances(train_dataset.instances)\n",
    "train_dataset.index_with(vocab)\n",
    "dev_dataset.index_with(vocab)\n",
    "test_dataset.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3faf190b8a1a41a18873d1e9ee43df8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from allennlp.modules.token_embedders import TokenCharactersEncoder\n",
    "from allennlp.modules.seq2vec_encoders import CnnEncoder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "\n",
    "\n",
    "char_embedding_size = 300\n",
    "char_embedder = TokenCharactersEncoder(embedding=Embedding(embedding_dim=char_embedding_size, \n",
    "                                                           num_embeddings=300, \n",
    "                                                           trainable=True), \n",
    "                                       encoder=CnnEncoder(embedding_dim=char_embedding_size, \n",
    "                                                          num_filters=30, ngram_filter_sizes=(2, 3, 4, 5)))\n",
    "\n",
    "embedding_dim = 300\n",
    "glove_file = f'https://allennlp.s3.amazonaws.com/datasets/glove/glove.6B.{embedding_dim}d.txt.gz'\n",
    "token_embedder = Embedding(embedding_dim=embedding_dim, vocab=vocab, \n",
    "                           pretrained_file=glove_file, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "\n",
    "text_field_embedder = BasicTextFieldEmbedder({'tokens': token_embedder, \n",
    "                                              'chars' : char_embedder})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedder.get_output_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_embedder.get_output_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp_models.tagging import CrfTagger\n",
    "from allennlp.modules.seq2seq_encoders.pytorch_seq2seq_wrapper import LstmSeq2SeqEncoder\n",
    "\n",
    "\n",
    "input_dim = token_embedder.get_output_dim() + char_embedder.get_output_dim()\n",
    "encoder = LstmSeq2SeqEncoder(input_size=input_dim, hidden_size=300, bidirectional=True)\n",
    "\n",
    "model = CrfTagger(text_field_embedder=text_field_embedder, \n",
    "                  vocab=vocab, \n",
    "                  encoder=encoder,\n",
    "                  calculate_span_f1=True,\n",
    "                  label_encoding='IOB1').cuda(device=cuda_device)\n",
    "                  #label_encoding='IOB1').cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40bc19d77c1443759f9f44b87827e85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a93a5cc5f45463da70659f72d46ce51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541075ff9e4e471e88f38aa38dc41e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec815be189545808a2d811e9659f6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0143a91a5624d7a81a0faa04d1a78f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efb0fae116f4afb80da105601f21657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d06d7f1851a42749fbd6e04c75c28b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b340df7a528f455197c009a4c2500e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b474a64fd64bef9e78c0dbf64bea2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87aee18b8764d118276d1ef5868ad83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f53031b99f4f128752945eb6c4bc35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429ecb330031407494678a220d3007ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095ff29dda954467a9a1b35424b5e6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9034188a1fda465497154ea327e35486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6464aad639241b0b2e0fdc42ea1bb52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae0868d8d8a4d928843b0530af53316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12576acb6cde4fa88bca05e3846ec38e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ef8822f3a2411296ecf2e47518e963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1592ab465634de0bc656fd405da55a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571cd41932484ecd8061a4810adc957e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from allennlp.training.learning_rate_schedulers import ReduceOnPlateauLearningRateScheduler\n",
    "from allennlp.training import GradientDescentTrainer\n",
    "\n",
    "import torch.optim as optim\n",
    "from transformers import AdamW\n",
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "from allennlp.training.learning_rate_schedulers import LinearWithWarmup\n",
    "from torch.utils.data import DataLoader\n",
    "from allennlp.training import GradientDescentTrainer\n",
    "from allennlp.training.learning_rate_schedulers import SlantedTriangular\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.03, weight_decay=0.001)\n",
    "train_data_loader = DataLoader(dataset=train_dataset, batch_size=8, \n",
    "                               collate_fn=allennlp_collate, shuffle=True)\n",
    "dev_data_loader = DataLoader(dataset=dev_dataset, batch_size=100, collate_fn=allennlp_collate)\n",
    "lr_scheduler = ReduceOnPlateauLearningRateScheduler(optimizer, patience=1, factor=0.5)\n",
    "\n",
    "date_time = datetime.now()\n",
    "date_str = date_time.strftime('%m/%d/%Y')\n",
    "time_str = date_time.strftime('%H:%M:%S')\n",
    "\n",
    "trainer = GradientDescentTrainer(\n",
    "    model=model,\n",
    "    serialization_dir=f'./workdir/{date_str}/{time_str}',\n",
    "    data_loader=train_data_loader,\n",
    "    validation_data_loader=dev_data_loader,\n",
    "    num_epochs=10,\n",
    "    optimizer=optimizer,\n",
    "    cuda_device=cuda_device,\n",
    "    learning_rate_scheduler=lr_scheduler,\n",
    "    grad_clipping=1.,\n",
    ")\n",
    "\n",
    "try:\n",
    "    metrics = trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48fc68c16f046f1b6eb1f1ce819773a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9379603399433428,\n",
       " 'accuracy3': 1.0,\n",
       " 'precision-overall': 0.0,\n",
       " 'recall-overall': 0.0,\n",
       " 'f1-measure-overall': 0.0,\n",
       " 'loss': 889.3219517299107}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=100, collate_fn=allennlp_collate)\n",
    "evaluate(model, test_dataloader, cuda_device=cuda_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869975555ad74f76a24b50bd7dfc6003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', layout=Layout(width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ed3c3f1bca4ec1833f6b5ac3b5fcae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', layout=Layout(width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c220e94b8d2d4bd69a4fccbc065c2656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', layout=Layout(width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.dataset_readers.sequence_tagging import SequenceTaggingDatasetReader\n",
    "from allennlp.data.token_indexers import ELMoTokenCharactersIndexer\n",
    "\n",
    "token_indexer = ELMoTokenCharactersIndexer()\n",
    "reader = SequenceTaggingDatasetReader(token_indexers={'elmo_tokens': token_indexer})\n",
    "\n",
    "train_dataset = reader.read('data/train.seq')\n",
    "dev_dataset = reader.read('data/dev.seq')\n",
    "test_dataset = reader.read('data/trial.seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9e6f9e768345b680266904f0a5181c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='building vocab', max=6351.0, style=ProgressStyle(descript"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "vocab = Vocabulary.from_instances(train_dataset.instances)\n",
    "train_dataset.index_with(vocab)\n",
    "dev_dataset.index_with(vocab)\n",
    "test_dataset.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.token_embedders import ElmoTokenEmbedder\n",
    "\n",
    "elmo_embedder = ElmoTokenEmbedder()\n",
    "text_field_embedder = BasicTextFieldEmbedder({\"elmo_tokens\": elmo_embedder})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp_models.tagging import CrfTagger\n",
    "from allennlp.modules.seq2seq_encoders.pytorch_seq2seq_wrapper import LstmSeq2SeqEncoder\n",
    "\n",
    "\n",
    "encoder = LstmSeq2SeqEncoder(input_size=elmo_embedder.get_output_dim(), \n",
    "                             hidden_size=300, bidirectional=True)\n",
    "\n",
    "model = CrfTagger(text_field_embedder=text_field_embedder, \n",
    "                  vocab=vocab, \n",
    "                  encoder=encoder,\n",
    "                  calculate_span_f1=True,\n",
    "                  label_encoding='IOB1').cuda(device=cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n",
      "/home/markov/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea61fabae3c43c3b9a73942db9e0e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a9f4385ac443d2bea83d4d90e92369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/markov/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d35d2bf83c4730a4e5b2885ab6b22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb5f2cbf0fe49dfb7206b986503d2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/markov/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58cf5f5ca9a410abe933c07453821dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1612435c395242e793b1718f95c7a344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/markov/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3dd28dc17143e28c8b1179abb11206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e72d4803964cf796cc6aaf59e646ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/markov/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b4936130a145ddb49fe5934a8d577b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a72b8b8ea82451aafcd5ae50a03a35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/markov/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5571fd46994b6b8faf269b9cdff49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b6703426d74d839b33216e1a786c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/markov/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394f2b8938fb4286bf21890ed3ff1baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a5b72b8662466781ce701ed6c1c196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/markov/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c189c2c916d64392818db69758e82cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92db306373a744efbc9f1d6d9cba190b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/markov/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc245f7d08b4c34ad56baf9e98c4aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f300a73b4d0a47f89492772db855c532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/markov/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf03ff7a9694b5b97d0d22bf47ea669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=794.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc009f32303b4848999e039cf2809290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from allennlp.training.learning_rate_schedulers import ReduceOnPlateauLearningRateScheduler\n",
    "from allennlp.training import GradientDescentTrainer\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05, weight_decay=0.001)\n",
    "train_data_loader = DataLoader(dataset=train_dataset, batch_size=8, \n",
    "                               collate_fn=allennlp_collate, shuffle=True)\n",
    "dev_data_loader = DataLoader(dataset=dev_dataset, batch_size=100, collate_fn=allennlp_collate)\n",
    "lr_scheduler = ReduceOnPlateauLearningRateScheduler(optimizer, patience=1, factor=0.5)\n",
    "\n",
    "date_time = datetime.now()\n",
    "date_str = date_time.strftime('%m/%d/%Y')\n",
    "time_str = date_time.strftime('%H:%M:%S')\n",
    "\n",
    "trainer = GradientDescentTrainer(\n",
    "    model=model,\n",
    "    serialization_dir=f'./workdir/{date_str}/{time_str}',\n",
    "    data_loader=train_data_loader,\n",
    "    validation_data_loader=dev_data_loader,\n",
    "    num_epochs=10,\n",
    "    optimizer=optimizer,\n",
    "    cuda_device=cuda_device,\n",
    "    learning_rate_scheduler=lr_scheduler,\n",
    "    grad_clipping=1.\n",
    ")\n",
    "\n",
    "try:\n",
    "    metrics = trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b54613c08a4fb7bd645a45c80ade3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9069759206798866,\n",
       " 'accuracy3': 1.0,\n",
       " 'precision-overall': 0.0822281167108753,\n",
       " 'recall-overall': 0.031958762886597936,\n",
       " 'f1-measure-overall': 0.04602821083886094,\n",
       " 'loss': 1481.4861711774554}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=100, collate_fn=allennlp_collate)\n",
    "evaluate(model, test_dataloader, cuda_device=cuda_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import PretrainedTransformerMismatchedIndexer\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "\n",
    "BERT_MODEL = 'bert-base-cased'\n",
    "indexer = PretrainedTransformerMismatchedIndexer(model_name=BERT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[CLS], We, are, living, in, M, ##, ##nchen, ##g, ##lad, ##bach, [SEP]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpieces, offsets = indexer._allennlp_tokenizer.intra_word_tokenize(['We', 'are', 'living', 'in', 'Mnchengladbach'])\n",
    "wordpieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fca0acac8544fd8eed7ac903a66151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', layout=Layout(width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d3d5c1fae54004aff24a2e865f30fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', layout=Layout(width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763efc3ee4a24d3f91202f038b3249e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', layout=Layout(width"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reader = SequenceTaggingDatasetReader(token_indexers={'tokens' : indexer})\n",
    "train_dataset = reader.read('data/train.seq')\n",
    "dev_dataset = reader.read('data/dev.seq')\n",
    "test_dataset = reader.read('data/trial.seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba98a0b63c5d4b7d83d90826b165f7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='building vocab', max=6351.0, style=ProgressStyle(descript"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(train_dataset.instances)\n",
    "train_dataset.index_with(vocab)\n",
    "dev_dataset.index_with(vocab)\n",
    "test_dataset.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.token_embedders import PretrainedTransformerMismatchedEmbedder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.seq2seq_encoders import PassThroughEncoder\n",
    "\n",
    "\n",
    "embedder = PretrainedTransformerMismatchedEmbedder(model_name=BERT_MODEL)\n",
    "text_field_embedder = BasicTextFieldEmbedder({'tokens': embedder})\n",
    "seq2seq_encoder = PassThroughEncoder(input_dim=embedder.get_output_dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PretrainedTransformerMismatchedEmbedder(\n",
       "  (_matched_embedder): PretrainedTransformerEmbedder(\n",
       "    (transformer_model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import SimpleTagger\n",
    "\n",
    "\n",
    "model = SimpleTagger(text_field_embedder=text_field_embedder, \n",
    "                      vocab=vocab, \n",
    "                      encoder=seq2seq_encoder,\n",
    "                      calculate_span_f1=True,\n",
    "                      label_encoding='IOB1').cuda(device=cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6a337be34c45cea1cc559b1f945a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=636.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153cdb1432594c4b9551084b143d1b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951e44b9cb214f0ba510b6cf63358bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=636.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f3595603cf4c569d59317a04cda51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a525379b8a459c9efec80d335cc797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=636.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4bff3bfde204761b1af531b7dd9912b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718ba8ab8f3d4769bf2547fbac636e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=636.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8cd46f117845ad816fd1fa970a8df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from transformers import AdamW\n",
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "from allennlp.training.learning_rate_schedulers import LinearWithWarmup\n",
    "from torch.utils.data import DataLoader\n",
    "from allennlp.training import GradientDescentTrainer\n",
    "from allennlp.training.learning_rate_schedulers import SlantedTriangular\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "num_epochs = 4\n",
    "batch_size = 10\n",
    "steps_per_epoch = math.ceil(len(train_dataset) / batch_size)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
    "train_data_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, \n",
    "                               collate_fn=allennlp_collate, shuffle=True)\n",
    "val_data_loader = DataLoader(dataset=dev_dataset, batch_size=50, collate_fn=allennlp_collate)\n",
    "lr_scheduler = LinearWithWarmup(optimizer, \n",
    "                                num_epochs=num_epochs, \n",
    "                                warmup_steps=(steps_per_epoch*num_epochs)*0.1, \n",
    "                                num_steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "date_time = datetime.now()\n",
    "date_str = date_time.strftime('%m/%d/%Y')\n",
    "time_str = date_time.strftime('%H:%M:%S')\n",
    "\n",
    "\n",
    "trainer = GradientDescentTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_data_loader,\n",
    "    validation_data_loader=val_data_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    cuda_device=cuda_device,\n",
    "    learning_rate_scheduler=lr_scheduler,\n",
    "    num_gradient_accumulation_steps=1,\n",
    "    serialization_dir=f'./workdir/{date_str}/{time_str}',\n",
    "    grad_clipping=1.\n",
    ")\n",
    "\n",
    "try:\n",
    "    metrics = trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae920a7a191e49549af3777ba61a7cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9515226628895184,\n",
       " 'accuracy3': 1.0,\n",
       " 'precision-overall': 0.6449086161879894,\n",
       " 'recall-overall': 0.5092783505154639,\n",
       " 'f1-measure-overall': 0.5691244239630843,\n",
       " 'loss': 0.2455632154430662}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=100, collate_fn=allennlp_collate)\n",
    "evaluate(model, test_dataloader, cuda_device=cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
