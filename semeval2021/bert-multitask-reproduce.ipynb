{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* basic roberta ft: 0.6589791487657798 (thr 0.3)\n",
    "* basic roberta ft (head first): 0.6768011808573329 (thr 0.42)\n",
    "* fine tune roberta on weird clf, then only head on spans, then whole: 0.6853127403287083 (thr 0.32)\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForTokenClassification\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta-base'  #roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForTokenClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1] * inputs[\"input_ids\"].size(1)).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs, labels=labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create labels for tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((690, 2), (7939, 2), (2000, 2))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial = pd.read_csv(path + 'tsd_trial.csv')\n",
    "train = pd.read_csv(path + 'tsd_train.csv')\n",
    "# final_test = pd.read_csv(path + 'tsd_test.csv')\n",
    "final_test = pd.read_csv(path + 'tsd_test_gt.csv')\n",
    "\n",
    "train['spans'] = train.spans.apply(literal_eval)\n",
    "trial['spans'] = trial.spans.apply(literal_eval)\n",
    "final_test['spans'] = final_test.spans.apply(literal_eval)\n",
    "trial.shape, train.shape, final_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(set(trial.text).intersection(set(train.text))))\n",
    "print(len(set(final_test.text).intersection(set(train.text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06109081748331024\n",
      "0.06231884057971015\n"
     ]
    }
   ],
   "source": [
    "print((train.spans.apply(len) == 0).mean())\n",
    "print((trial.spans.apply(len) == 0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Because he's a <b>moron</b> and a <b>bigot</b>. It's not any more complicated than that."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Because he's a <b>moron</b> and a <b>bigot</b>. It's not any more complicated than that."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spans_utils\n",
    "from importlib import reload\n",
    "reload(spans_utils)\n",
    "from spans_utils import display_spans, spans2labels, labels2spans\n",
    "\n",
    "display_spans(trial.spans[0], trial.text[0])\n",
    "display_spans(trial.spans[0], trial.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02deb8286808482baa9163e9ebb8a6d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=690.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for row in tqdm([row for i, row in trial.iterrows()]):\n",
    "    break\n",
    "    labels = spans2labels(row.text, row.spans, tokenizer)\n",
    "    spans2 = labels2spans(row.text, labels, tokenizer)\n",
    "    if row.spans != spans2:\n",
    "        t = row.text.replace(' ', '+')\n",
    "        display_spans(row.spans, t)\n",
    "        display_spans(spans2, t)\n",
    "        n += 1\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f526cee7c0a4c4dbc616b3463fd4782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_labels = [spans2labels(row.text, row.spans, tokenizer) for i, row in tqdm(train.iterrows())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2611a7c694048b9b4f1c922b20fcf6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trial_labels = [spans2labels(row.text, row.spans, tokenizer) for i, row in tqdm(trial.iterrows())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['labels'] = train_labels\n",
    "trial['labels'] = trial_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpansDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpansDataset(tokenizer(train.text.tolist()), train_labels)\n",
    "eval_dataset = SpansDataset(tokenizer(trial.text.tolist()), trial_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_dataset = SpansDataset(tokenizer(final_test.text.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from semeval2021 import f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358984, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>222509</th>\n",
       "      <td>With Bannon out of the picture he's not nearly...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271397</th>\n",
       "      <td>If you want to save 15% on your car insurance ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149303</th>\n",
       "      <td>he sounds like a moron...and to solidify  that...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  label\n",
       "222509  With Bannon out of the picture he's not nearly...      0\n",
       "271397  If you want to save 15% on your car insurance ...      0\n",
       "149303  he sounds like a moron...and to solidify  that...      1"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('../data/train/train.1.tsv', sep='\\t')\n",
    "df0 = pd.read_csv('../data/train/train_small.0.tsv', sep='\\t')\n",
    "df01 = pd.concat([df1, df0], ignore_index=True)\n",
    "df01.label = df01.label.astype(int)\n",
    "print(df01.shape)\n",
    "df01.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df01, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>214726</th>\n",
       "      <td>My Dad had a saying, if you can't play, play l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275434</th>\n",
       "      <td>Awesome! I realize players cannot / will not s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315281</th>\n",
       "      <td>If it were a school, I'd fire the principal fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225573</th>\n",
       "      <td>Do you believe POTUS should stop having he and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214259</th>\n",
       "      <td>The Liberals better wake up.  \\n\\nRemember wha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274177</th>\n",
       "      <td>This legislature is violating the Alaska State...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20450</th>\n",
       "      <td>if a trip wire or sensors could be hooked up t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182019</th>\n",
       "      <td>See what Dr. Fred Baughman had to say about dr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317139</th>\n",
       "      <td>Impatient millinial. Is that something to adve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23614</th>\n",
       "      <td>So ridiculous people still call this \"Our oil\"...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  label\n",
       "214726  My Dad had a saying, if you can't play, play l...      0\n",
       "275434  Awesome! I realize players cannot / will not s...      0\n",
       "315281  If it were a school, I'd fire the principal fo...      0\n",
       "225573  Do you believe POTUS should stop having he and...      0\n",
       "214259  The Liberals better wake up.  \\n\\nRemember wha...      0\n",
       "274177  This legislature is violating the Alaska State...      0\n",
       "20450   if a trip wire or sensors could be hooked up t...      1\n",
       "182019  See what Dr. Fred Baughman had to say about dr...      0\n",
       "317139  Impatient millinial. Is that something to adve...      0\n",
       "23614   So ridiculous people still call this \"Our oil\"...      1"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpansDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_train_dataset = SpansDataset(\n",
    "    tokenizer(df_train.comment_text.tolist(), truncation=True), \n",
    "    df_train.label.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_test_dataset = SpansDataset(\n",
    "    tokenizer(df_test.comment_text.tolist(), truncation=True), \n",
    "    df_test.label.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_test_small_dataset = SpansDataset(\n",
    "    tokenizer(df_test.comment_text.iloc[:3000].tolist(), truncation=True), \n",
    "    df_test.label[:3000].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a single-task model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb\n",
    "https://huggingface.co/transformers/custom_datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers.file_utils import cached_property\n",
    "from typing import Tuple\n",
    "\n",
    "class TrAr(TrainingArguments):\n",
    "    @cached_property\n",
    "    def _setup_devices(self):\n",
    "        return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='./models2/roberta_single',   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,            # total # of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=3000,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-8,              # strength of weight decay\n",
    "    learning_rate=1e-3,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=100,\n",
    "    eval_steps=100,\n",
    "    evaluation_strategy='steps',\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=eval_dataset,           # evaluation dataset\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='3300' max='9930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3300/9930 03:53 < 07:49, 14.13 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>0.809791</td>\n",
       "      <td>2.386100</td>\n",
       "      <td>289.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.716500</td>\n",
       "      <td>0.603003</td>\n",
       "      <td>2.303700</td>\n",
       "      <td>299.515000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.502000</td>\n",
       "      <td>0.394694</td>\n",
       "      <td>2.192600</td>\n",
       "      <td>314.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.353700</td>\n",
       "      <td>0.284466</td>\n",
       "      <td>2.206600</td>\n",
       "      <td>312.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.276600</td>\n",
       "      <td>0.248568</td>\n",
       "      <td>2.185200</td>\n",
       "      <td>315.762000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.283500</td>\n",
       "      <td>0.230986</td>\n",
       "      <td>2.258300</td>\n",
       "      <td>305.541000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.284800</td>\n",
       "      <td>0.220532</td>\n",
       "      <td>2.229300</td>\n",
       "      <td>309.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.257900</td>\n",
       "      <td>0.213154</td>\n",
       "      <td>2.220600</td>\n",
       "      <td>310.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.237000</td>\n",
       "      <td>0.207164</td>\n",
       "      <td>2.245200</td>\n",
       "      <td>307.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.245900</td>\n",
       "      <td>0.203607</td>\n",
       "      <td>2.321700</td>\n",
       "      <td>297.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.201099</td>\n",
       "      <td>2.278800</td>\n",
       "      <td>302.794000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.246600</td>\n",
       "      <td>0.198631</td>\n",
       "      <td>2.293400</td>\n",
       "      <td>300.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.240800</td>\n",
       "      <td>0.197461</td>\n",
       "      <td>2.225200</td>\n",
       "      <td>310.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>0.196184</td>\n",
       "      <td>2.303900</td>\n",
       "      <td>299.488000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.233500</td>\n",
       "      <td>0.195025</td>\n",
       "      <td>2.265700</td>\n",
       "      <td>304.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.238800</td>\n",
       "      <td>0.193906</td>\n",
       "      <td>2.221500</td>\n",
       "      <td>310.597000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.261100</td>\n",
       "      <td>0.193407</td>\n",
       "      <td>2.298600</td>\n",
       "      <td>300.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.229500</td>\n",
       "      <td>0.192580</td>\n",
       "      <td>2.271800</td>\n",
       "      <td>303.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.256500</td>\n",
       "      <td>0.191718</td>\n",
       "      <td>2.177500</td>\n",
       "      <td>316.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.211600</td>\n",
       "      <td>0.190956</td>\n",
       "      <td>2.264700</td>\n",
       "      <td>304.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.236700</td>\n",
       "      <td>0.191092</td>\n",
       "      <td>2.265000</td>\n",
       "      <td>304.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.253200</td>\n",
       "      <td>0.192811</td>\n",
       "      <td>2.331200</td>\n",
       "      <td>295.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.237300</td>\n",
       "      <td>0.190672</td>\n",
       "      <td>2.219500</td>\n",
       "      <td>310.882000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.201800</td>\n",
       "      <td>0.190424</td>\n",
       "      <td>2.334400</td>\n",
       "      <td>295.574000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.215600</td>\n",
       "      <td>0.189549</td>\n",
       "      <td>2.420500</td>\n",
       "      <td>285.065000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.219900</td>\n",
       "      <td>0.189416</td>\n",
       "      <td>2.305300</td>\n",
       "      <td>299.311000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.243100</td>\n",
       "      <td>0.188780</td>\n",
       "      <td>2.192400</td>\n",
       "      <td>314.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.243800</td>\n",
       "      <td>0.189499</td>\n",
       "      <td>2.392600</td>\n",
       "      <td>288.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.252500</td>\n",
       "      <td>0.188836</td>\n",
       "      <td>2.378300</td>\n",
       "      <td>290.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.244300</td>\n",
       "      <td>0.188119</td>\n",
       "      <td>2.443800</td>\n",
       "      <td>282.349000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.235700</td>\n",
       "      <td>0.189351</td>\n",
       "      <td>2.391500</td>\n",
       "      <td>288.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.228100</td>\n",
       "      <td>0.188279</td>\n",
       "      <td>2.500300</td>\n",
       "      <td>275.969000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.235600</td>\n",
       "      <td>0.188437</td>\n",
       "      <td>2.424500</td>\n",
       "      <td>284.598000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3300, training_loss=0.2872040771715569, metrics={'train_runtime': 233.4637, 'train_samples_per_second': 42.533, 'total_flos': 2549211576987996, 'epoch': 3.32})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='./models2/roberta_single',   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,            # total # of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=3000,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-8,              # strength of weight decay\n",
    "    learning_rate=1e-5,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=eval_dataset,           # evaluation dataset\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The minimal loss of a single-task model (full) was about 28% on validation with 0.04 on train. \n",
    "* If we first train only head (batch 8, lr 1e-3 with 3K warmup and 1e-8 decline), we get minimal loss of 0.185 on validation with 0.23 on train\n",
    "* Training then the whole model (batch 8, lr 1e-5 with 3K warmup and 1e-8 decline) we get minimal loss of 0.175 on validation with 0.21 on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='3500' max='9930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3500/9930 07:41 < 14:08, 7.58 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.214300</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>2.275500</td>\n",
       "      <td>303.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.231800</td>\n",
       "      <td>0.178588</td>\n",
       "      <td>2.282300</td>\n",
       "      <td>302.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.228400</td>\n",
       "      <td>0.178426</td>\n",
       "      <td>2.267800</td>\n",
       "      <td>304.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.219600</td>\n",
       "      <td>0.174098</td>\n",
       "      <td>2.316700</td>\n",
       "      <td>297.833000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.216500</td>\n",
       "      <td>0.181011</td>\n",
       "      <td>2.336500</td>\n",
       "      <td>295.308000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.218200</td>\n",
       "      <td>0.181870</td>\n",
       "      <td>2.270400</td>\n",
       "      <td>303.906000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.206900</td>\n",
       "      <td>0.175007</td>\n",
       "      <td>2.270100</td>\n",
       "      <td>303.947000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3500, training_loss=0.2193861323765346, metrics={'train_runtime': 461.8953, 'train_samples_per_second': 21.498, 'total_flos': 2707692614817216, 'epoch': 3.52})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./models2/roberta_single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='174' max='87' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [87/87 00:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.17584852874279022}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='337' max='87' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [87/87 09:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = trainer.predict(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.21345926240649193\n",
      "0.01 0.2522426276657023\n",
      "0.03 0.39910998627052807\n",
      "0.1 0.5959594514495652\n",
      "0.3 0.6691954194658056\n",
      "0.4 0.6734060063813712\n",
      "0.5 0.6728307956631072\n",
      "0.6 0.652604453261445\n",
      "0.7 0.6215603361690663\n",
      "1 0.06231884057971015\n"
     ]
    }
   ],
   "source": [
    "for threshold in [0, 0.01, 0.03, 0.1, 0.3, 0.4, 0.5, 0.6, 0.7, 1]:\n",
    "    preds = []\n",
    "    for text, pr in zip(trial.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    print(threshold, np.mean([f1(p, y) for p, y in zip(preds, trial.spans)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 0.6691954194658056\n",
      "0.32 0.6719575974133731\n",
      "0.35 0.6718814105545576\n",
      "0.38 0.6704494596966453\n",
      "0.4 0.6734060063813712\n",
      "0.42 0.6733924519991911\n",
      "0.45 0.675073109186953\n",
      "0.5 0.6728307956631072\n",
      "0.55 0.6635405099600963\n",
      "0.6 0.652604453261445\n"
     ]
    }
   ],
   "source": [
    "for threshold in [0.3, 0.32, 0.35, 0.38, 0.4, 0.42, 0.45, 0.5, 0.55, 0.6]:\n",
    "    preds = []\n",
    "    for text, pr in zip(trial.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    print(threshold, np.mean([f1(p, y) for p, y in zip(preds, trial.spans)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(final_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.4\n",
    "preds = []\n",
    "for text, pr in zip(final_test.text, pred.predictions):\n",
    "    proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "    proba /= proba.sum(axis=1, keepdims=True)\n",
    "    labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "    preds.append(labels2spans(text, labels, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "What a <b>stupid</b> thing to say."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = final_test.sample(1).iloc[0]\n",
    "display_spans(preds[row.name], row.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "65.31% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6631491659944739\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([f1(p, y) for p, y in zip(preds, final_test.spans)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WM Classifier + tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForTokenClassification, RobertaForSequenceClassification\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "class WMean(nn.Module):\n",
    "    def __init__(self, dim=-2):\n",
    "        super(WMean, self).__init__()\n",
    "        self.pow = torch.nn.Parameter(data=torch.Tensor([1.0]), requires_grad=True)\n",
    "        self.coef = torch.nn.Parameter(data=torch.Tensor([0.0, 1.0]), requires_grad=True)\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        result = x ** self.pow[0]\n",
    "        if mask is None:\n",
    "            mp = result.mean(dim=-1)\n",
    "        else:\n",
    "            mp = (result * mask).sum(dim=self.dim) / mask.sum(dim=self.dim)\n",
    "        return torch.log(mp) * self.coef[1] + self.coef[0]\n",
    "\n",
    "\n",
    "class RobertaTaggerClassifier(RobertaForTokenClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.wmean = WMean()\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        token_logits = self.classifier(sequence_output)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            masks = attention_mask.unsqueeze(-1).repeat(1, 1, 2)\n",
    "        else:\n",
    "            masks = None\n",
    "\n",
    "        logits = self.wmean(torch.softmax(token_logits, dim=-1), mask=masks)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaTaggerClassifier: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaTaggerClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaTaggerClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaTaggerClassifier were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'wmean.pow', 'wmean.coef']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaTaggerClassifier.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-1.0804, -0.4147]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    o = model(**inputs)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda:3')\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.file_utils import cached_property\n",
    "from typing import Tuple\n",
    "\n",
    "class TrAr(TrainingArguments):\n",
    "    @cached_property\n",
    "    def _setup_devices(self):\n",
    "        return device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy: first tune the head only with large batches and LR, then tune the whole model. \n",
    "\n",
    "Head-only stops at loss 0.4185, full model - at loss 0.302685"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_MODEL_NAME = './models2/roberta_clf_wm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir=NEW_MODEL_NAME,   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,            # total # of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=3000,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-8,              # strength of weight decay\n",
    "    learning_rate=1e-3,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=100,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=clf_train_dataset,         # training dataset\n",
    "    eval_dataset=clf_test_small_dataset,           # evaluation dataset\n",
    "    #data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='5500' max='403860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5500/403860 07:04 < 8:32:37, 12.95 it/s, Epoch 0/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>0.654588</td>\n",
       "      <td>15.743200</td>\n",
       "      <td>190.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.586400</td>\n",
       "      <td>0.563334</td>\n",
       "      <td>15.741400</td>\n",
       "      <td>190.581000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.508200</td>\n",
       "      <td>0.489804</td>\n",
       "      <td>15.830300</td>\n",
       "      <td>189.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.476600</td>\n",
       "      <td>0.451627</td>\n",
       "      <td>15.850500</td>\n",
       "      <td>189.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.464200</td>\n",
       "      <td>0.438118</td>\n",
       "      <td>15.858700</td>\n",
       "      <td>189.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.462900</td>\n",
       "      <td>0.431260</td>\n",
       "      <td>15.855000</td>\n",
       "      <td>189.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.474200</td>\n",
       "      <td>0.419679</td>\n",
       "      <td>15.857100</td>\n",
       "      <td>189.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.448800</td>\n",
       "      <td>0.418521</td>\n",
       "      <td>15.859900</td>\n",
       "      <td>189.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.453500</td>\n",
       "      <td>0.431241</td>\n",
       "      <td>15.869500</td>\n",
       "      <td>189.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.452100</td>\n",
       "      <td>0.418626</td>\n",
       "      <td>15.857600</td>\n",
       "      <td>189.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.437800</td>\n",
       "      <td>0.467199</td>\n",
       "      <td>15.853600</td>\n",
       "      <td>189.232000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir=NEW_MODEL_NAME,   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,            # total # of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=3000,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-8,              # strength of weight decay\n",
    "    learning_rate=1e-5,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=clf_train_dataset,         # training dataset\n",
    "    eval_dataset=clf_test_small_dataset,           # evaluation dataset\n",
    "    #data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10, early_stopping_threshold=0)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='9000' max='403860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  9000/403860 26:23 < 19:18:02, 5.68 it/s, Epoch 0/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.469900</td>\n",
       "      <td>0.399480</td>\n",
       "      <td>12.448400</td>\n",
       "      <td>240.994000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.437300</td>\n",
       "      <td>0.384663</td>\n",
       "      <td>12.543700</td>\n",
       "      <td>239.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.385200</td>\n",
       "      <td>0.399380</td>\n",
       "      <td>12.711300</td>\n",
       "      <td>236.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.396100</td>\n",
       "      <td>0.321518</td>\n",
       "      <td>12.540100</td>\n",
       "      <td>239.233000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.377100</td>\n",
       "      <td>0.321958</td>\n",
       "      <td>12.557800</td>\n",
       "      <td>238.895000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.348600</td>\n",
       "      <td>0.506236</td>\n",
       "      <td>12.715400</td>\n",
       "      <td>235.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.364300</td>\n",
       "      <td>0.341919</td>\n",
       "      <td>12.474700</td>\n",
       "      <td>240.487000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.361300</td>\n",
       "      <td>0.302685</td>\n",
       "      <td>12.421800</td>\n",
       "      <td>241.511000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.344800</td>\n",
       "      <td>0.435121</td>\n",
       "      <td>12.562100</td>\n",
       "      <td>238.814000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.381100</td>\n",
       "      <td>0.403674</td>\n",
       "      <td>12.762600</td>\n",
       "      <td>235.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.365300</td>\n",
       "      <td>0.317092</td>\n",
       "      <td>12.415800</td>\n",
       "      <td>241.628000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.348600</td>\n",
       "      <td>0.420537</td>\n",
       "      <td>12.443300</td>\n",
       "      <td>241.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.358900</td>\n",
       "      <td>0.335339</td>\n",
       "      <td>12.447000</td>\n",
       "      <td>241.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.348200</td>\n",
       "      <td>0.308131</td>\n",
       "      <td>12.845400</td>\n",
       "      <td>233.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.350200</td>\n",
       "      <td>0.329928</td>\n",
       "      <td>12.649000</td>\n",
       "      <td>237.173000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.327300</td>\n",
       "      <td>0.335358</td>\n",
       "      <td>12.510800</td>\n",
       "      <td>239.793000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.311347</td>\n",
       "      <td>12.374100</td>\n",
       "      <td>242.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.332300</td>\n",
       "      <td>0.348165</td>\n",
       "      <td>12.624100</td>\n",
       "      <td>237.641000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9000, training_loss=0.36898494974772134, metrics={'train_runtime': 1583.6349, 'train_samples_per_second': 255.021, 'total_flos': 9531213525787008, 'epoch': 0.22})"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([1.9293], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1.0333, 1.9060], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.wmean.pow)\n",
    "print(model.wmean.coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(NEW_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune the averager classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./models2/roberta_clf_wm were not used when initializing RobertaForTokenClassification: ['wmean.pow', 'wmean.coef']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('./models2/roberta_clf_wm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_MODEL_NAME = './models2/roberta_clf_wm_ft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir=NEW_MODEL_NAME,   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,            # total # of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=3000,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-8,              # strength of weight decay\n",
    "    learning_rate=1e-3,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=100,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=eval_dataset,           # evaluation dataset\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='5500' max='9930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5500/9930 04:05 < 03:17, 22.42 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.104800</td>\n",
       "      <td>0.514359</td>\n",
       "      <td>2.983200</td>\n",
       "      <td>231.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.293600</td>\n",
       "      <td>0.223289</td>\n",
       "      <td>2.783900</td>\n",
       "      <td>247.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.228800</td>\n",
       "      <td>0.206288</td>\n",
       "      <td>2.813100</td>\n",
       "      <td>245.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.259700</td>\n",
       "      <td>0.195860</td>\n",
       "      <td>2.822900</td>\n",
       "      <td>244.431000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.252400</td>\n",
       "      <td>0.191047</td>\n",
       "      <td>2.830200</td>\n",
       "      <td>243.796000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.212200</td>\n",
       "      <td>0.189924</td>\n",
       "      <td>2.858900</td>\n",
       "      <td>241.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.250600</td>\n",
       "      <td>0.187042</td>\n",
       "      <td>2.832100</td>\n",
       "      <td>243.639000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.218500</td>\n",
       "      <td>0.185239</td>\n",
       "      <td>2.828000</td>\n",
       "      <td>243.992000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.188281</td>\n",
       "      <td>2.849200</td>\n",
       "      <td>242.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.243800</td>\n",
       "      <td>0.185856</td>\n",
       "      <td>2.855600</td>\n",
       "      <td>241.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.228400</td>\n",
       "      <td>0.185977</td>\n",
       "      <td>2.834600</td>\n",
       "      <td>243.417000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5500, training_loss=0.5763802882107821, metrics={'train_runtime': 245.3274, 'train_samples_per_second': 40.477, 'total_flos': 4253019506029356, 'epoch': 5.54})"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the raw quasi-classifier: no use in the model at all\n",
    "* fine tuned head: still no use, the best score is 0.2138\n",
    "* fine tune whole model: 0.3 0.6849391042415774"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir=NEW_MODEL_NAME,   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,            # total # of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=3000,               # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-8,              # strength of weight decay\n",
    "    learning_rate=1e-5,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=eval_dataset,           # evaluation dataset\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='3000' max='9930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/9930 06:53 < 15:56, 7.24 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.227800</td>\n",
       "      <td>0.183276</td>\n",
       "      <td>2.417000</td>\n",
       "      <td>285.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.175740</td>\n",
       "      <td>2.280000</td>\n",
       "      <td>302.626000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.228600</td>\n",
       "      <td>0.172228</td>\n",
       "      <td>2.294100</td>\n",
       "      <td>300.768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.219200</td>\n",
       "      <td>0.172719</td>\n",
       "      <td>2.332600</td>\n",
       "      <td>295.804000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.219700</td>\n",
       "      <td>0.175370</td>\n",
       "      <td>2.357100</td>\n",
       "      <td>292.728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.219000</td>\n",
       "      <td>0.181231</td>\n",
       "      <td>2.323500</td>\n",
       "      <td>296.963000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=0.22593124389648436, metrics={'train_runtime': 414.0957, 'train_samples_per_second': 23.98, 'total_flos': 2320362174532992, 'epoch': 3.02})"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The minimal loss of a single-task model (full) was about 28% on validation with 0.04 on train. \n",
    "* If we first train only head (batch 8, lr 1e-3 with 3K warmup and 1e-8 decline), we get minimal loss of 0.185 on validation with 0.23 on train\n",
    "* Training then the whole model (batch 8, lr 1e-5 with 3K warmup and 1e-8 decline) we get minimal loss of 0.175 on validation with 0.21 on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='3500' max='9930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3500/9930 07:41 < 14:08, 7.58 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.214300</td>\n",
       "      <td>0.183102</td>\n",
       "      <td>2.275500</td>\n",
       "      <td>303.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.231800</td>\n",
       "      <td>0.178588</td>\n",
       "      <td>2.282300</td>\n",
       "      <td>302.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.228400</td>\n",
       "      <td>0.178426</td>\n",
       "      <td>2.267800</td>\n",
       "      <td>304.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.219600</td>\n",
       "      <td>0.174098</td>\n",
       "      <td>2.316700</td>\n",
       "      <td>297.833000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.216500</td>\n",
       "      <td>0.181011</td>\n",
       "      <td>2.336500</td>\n",
       "      <td>295.308000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.218200</td>\n",
       "      <td>0.181870</td>\n",
       "      <td>2.270400</td>\n",
       "      <td>303.906000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.206900</td>\n",
       "      <td>0.175007</td>\n",
       "      <td>2.270100</td>\n",
       "      <td>303.947000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3500, training_loss=0.2193861323765346, metrics={'train_runtime': 461.8953, 'train_samples_per_second': 21.498, 'total_flos': 2707692614817216, 'epoch': 3.52})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models2/roberta_clf_wm_ft'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEW_MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(NEW_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='337' max='87' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [87/87 01:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.21345926240649193\n",
      "0.01 0.2409973460392372\n",
      "0.03 0.34943570917505273\n",
      "0.1 0.557400342430896\n",
      "0.25 0.66657885004928\n",
      "0.3 0.6764404567060631\n",
      "0.35 0.6782236752482207\n",
      "0.4 0.6794854989344155\n",
      "0.5 0.6699329686773023\n",
      "0.6 0.6607375516230902\n",
      "0.7 0.6396864806672651\n",
      "1 0.06231884057971015\n"
     ]
    }
   ],
   "source": [
    "pred = trainer.predict(eval_dataset)\n",
    "for threshold in [0, 0.01, 0.03, 0.1, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6, 0.7, 1]:\n",
    "    preds = []\n",
    "    for text, pr in zip(trial.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    print(threshold, np.mean([f1(p, y) for p, y in zip(preds, trial.spans)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25 0.66657885004928\n",
      "0.28 0.6746374794679898\n",
      "0.3 0.6764404567060631\n",
      "0.32 0.6784818373039875\n",
      "0.35 0.6782236752482207\n"
     ]
    }
   ],
   "source": [
    "for threshold in [ 0.25, 0.28, 0.3, 0.32, 0.35]:\n",
    "    preds = []\n",
    "    for text, pr in zip(trial.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    print(threshold, np.mean([f1(p, y) for p, y in zip(preds, trial.spans)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(final_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.4\n",
    "preds = []\n",
    "for text, pr in zip(final_test.text, pred.predictions):\n",
    "    proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "    proba /= proba.sum(axis=1, keepdims=True)\n",
    "    labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "    preds.append(labels2spans(text, labels, tokenizer))\n",
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6503622193673821\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([f1(p, y) for p, y in zip(preds, final_test.spans)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to reproduce the score of an ordinary classifier fine tuned as tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "* roberta_clf_proba  - roberta classifier with wm head\n",
    "* roberta_clf_ft_plus_pseudolabels  - roberta_clf_ft + pseudolabels fine-tuning on data/train/train.1.tsv\n",
    "* roberta_clf        - preliminary form of roberta_clf_proba\n",
    "* roberta_clf_ft     - roberta_clf_proba + tagger fine-tuning\n",
    "* roberta_selflabel  - preliminary form of roberta_clf_ft_plus_pseudolabels\n",
    "* roberta_selflabel_final   - preliminary form of roberta_clf_ft_plus_pseudolabels\n",
    "* roberta_single_v2  - just roberta tagger \n",
    "* roberta_single     - just roberta tagger, first version\n",
    "* roberta_clf_2      - roberta classic classifier\n",
    "* roberta_ft_v2      - roberta_clf_2 + tagger fine-tuning\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### roberta_ft_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForTokenClassification.from_pretrained('models/roberta_ft_v2')\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='tmp',\n",
    "    per_device_eval_batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='337' max='87' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [87/87 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = trainer.predict(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 0.6646029995260339\n",
      "0.35 0.6715131454982759\n",
      "0.4 0.6744209810749072\n",
      "0.45 0.6716742100903217\n",
      "0.5 0.673925228987182\n",
      "0.55 0.6710741889893577\n",
      "0.6 0.6717334585591548\n",
      "0.65 0.6738372257764363\n",
      "0.7 0.6682677734353392\n",
      "0.75 0.6617100031176605\n",
      "0.8 0.6475396800952727\n",
      "0.85 0.6278304231112732\n",
      "0.9 0.59010012748674\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for threshold in [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]:\n",
    "    preds = []\n",
    "    for text, pr in zip(trial.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    score = np.mean([f1(p, y) for p, y in zip(preds, trial.spans)])\n",
    "    print(threshold, score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(final_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.5545336665297276\n",
      "0.15 0.5882344110328829\n",
      "0.2 0.6077757811791716\n",
      "0.25 0.622973609210159\n",
      "0.3 0.633958561050074\n",
      "0.35 0.6443207294927615\n",
      "0.4 0.6519269346134431\n",
      "0.45 0.6589416029476445\n",
      "0.5 0.6639145447542181\n",
      "0.55 0.6676402819948017\n",
      "0.6 0.6695941122804318\n",
      "0.65 0.6756992752387653\n",
      "0.7 0.6785258499047722\n",
      "0.75 0.6851208918524581\n",
      "0.8 0.6864238573971339\n",
      "0.85 0.6771599110310472\n",
      "0.9 0.6545350848098808\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "scores = []\n",
    "for threshold in [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]:\n",
    "    preds = []\n",
    "    for text, pr in zip(final_test.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    score = np.mean([f1(p, y) for p, y in zip(preds, final_test.spans)])\n",
    "    print(threshold, score)\n",
    "    scores.append(score)\n",
    "scores_standard_clf = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### roberta_clf_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForTokenClassification.from_pretrained('models/roberta_clf_ft')\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='tmp',\n",
    "    per_device_eval_batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='337' max='87' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [87/87 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = trainer.predict(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 0.6849391042415774\n",
      "0.35 0.684180917165571\n",
      "0.4 0.6787335180780203\n",
      "0.45 0.6735204378403357\n",
      "0.5 0.6640287939316257\n",
      "0.55 0.6541785553598181\n",
      "0.6 0.6514593408593448\n",
      "0.65 0.6397508521212191\n",
      "0.7 0.6172986482655661\n",
      "0.75 0.5866965505118897\n",
      "0.8 0.548500481404008\n",
      "0.85 0.5094082267816179\n",
      "0.9 0.44324794411502716\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for threshold in [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]:\n",
    "    preds = []\n",
    "    for text, pr in zip(trial.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    score = np.mean([f1(p, y) for p, y in zip(preds, trial.spans)])\n",
    "    print(threshold, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(final_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.5752439183819529\n",
      "0.15 0.6090082887908743\n",
      "0.2 0.6286550876155881\n",
      "0.25 0.6431153729750709\n",
      "0.3 0.6569361916626016\n",
      "0.35 0.6647910078957441\n",
      "0.4 0.6682952103234534\n",
      "0.45 0.6754628567024429\n",
      "0.5 0.680619717437095\n",
      "0.55 0.6809661485870909\n",
      "0.6 0.6832488454134622\n",
      "0.65 0.6797076042739688\n",
      "0.7 0.6773883276721572\n",
      "0.75 0.6683696147429589\n",
      "0.8 0.6469023025439109\n",
      "0.85 0.6102817741854818\n",
      "0.9 0.5628998440674843\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "scores = []\n",
    "for threshold in [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]:\n",
    "    preds = []\n",
    "    for text, pr in zip(final_test.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    score = np.mean([f1(p, y) for p, y in zip(preds, final_test.spans)])\n",
    "    print(threshold, score)\n",
    "    scores.append(score)\n",
    "scores_tagging_clf = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### roberta_clf_ft_plus_pseudolabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForTokenClassification.from_pretrained('models/roberta_clf_ft_plus_pseudolabels')\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='tmp',\n",
    "    per_device_eval_batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='337' max='87' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [87/87 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = trainer.predict(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 0.6827629205595998\n",
      "0.35 0.6855501867465028\n",
      "0.4 0.6880259983225367\n",
      "0.45 0.6834992017957995\n",
      "0.5 0.6797056139144931\n",
      "0.55 0.6784005272126371\n",
      "0.6 0.6688018820701156\n",
      "0.65 0.6623198122783942\n",
      "0.7 0.6511178167749042\n",
      "0.75 0.6296963400207056\n",
      "0.8 0.6064892688014509\n",
      "0.85 0.564881928012032\n",
      "0.9 0.5030782060202328\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for threshold in [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]:\n",
    "    preds = []\n",
    "    for text, pr in zip(trial.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    score = np.mean([f1(p, y) for p, y in zip(preds, trial.spans)])\n",
    "    print(threshold, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(final_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.5602049029090154\n",
      "0.15 0.5965378234529369\n",
      "0.2 0.6161116736335367\n",
      "0.25 0.6329899472598364\n",
      "0.3 0.6458124347386611\n",
      "0.35 0.6555907203988778\n",
      "0.4 0.6620672594740299\n",
      "0.45 0.6692636447522394\n",
      "0.5 0.674174837442134\n",
      "0.55 0.6786709542304435\n",
      "0.6 0.6819797727432348\n",
      "0.65 0.6816887377159155\n",
      "0.7 0.6820785225830176\n",
      "0.75 0.6807040933415471\n",
      "0.8 0.6784134903922955\n",
      "0.85 0.6601609890118179\n",
      "0.9 0.6186889746671457\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = []\n",
    "for threshold in [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]:\n",
    "    preds = []\n",
    "    for text, pr in zip(final_test.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    score = np.mean([f1(p, y) for p, y in zip(preds, final_test.spans)])\n",
    "    print(threshold, score)\n",
    "    scores.append(score)\n",
    "scores_pseudolabel = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### roberta_single_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForTokenClassification.from_pretrained('models/roberta_single_v2')\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='tmp',\n",
    "    per_device_eval_batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='337' max='87' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [87/87 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = trainer.predict(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 0.66359735406299\n",
      "0.35 0.6700635517847642\n",
      "0.4 0.6731999549591532\n",
      "0.45 0.6729071697382535\n",
      "0.5 0.6702516698368454\n",
      "0.55 0.6634497922801099\n",
      "0.6 0.6618129018990354\n",
      "0.65 0.6534142826263554\n",
      "0.7 0.6474321946754974\n",
      "0.75 0.6306972882994829\n",
      "0.8 0.6064613727890047\n",
      "0.85 0.5650635904385258\n",
      "0.9 0.4991748439461978\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for threshold in [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]:\n",
    "    preds = []\n",
    "    for text, pr in zip(trial.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    score = np.mean([f1(p, y) for p, y in zip(preds, trial.spans)])\n",
    "    print(threshold, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(final_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.4950510627108955\n",
      "0.15 0.5492469372720504\n",
      "0.2 0.5870032860644548\n",
      "0.25 0.6115084907939428\n",
      "0.3 0.6296202613629943\n",
      "0.35 0.6435584198597509\n",
      "0.4 0.6550788084151723\n",
      "0.45 0.660826323136532\n",
      "0.5 0.6682946286098522\n",
      "0.55 0.6709144466014042\n",
      "0.6 0.677576863437034\n",
      "0.65 0.6812185597173275\n",
      "0.7 0.6818291704532222\n",
      "0.75 0.6769056066936087\n",
      "0.8 0.667081025778574\n",
      "0.85 0.6527564321439464\n",
      "0.9 0.6191291845142385\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = []\n",
    "for threshold in [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]:\n",
    "    preds = []\n",
    "    for text, pr in zip(final_test.text, pred.predictions):\n",
    "        proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "        proba /= proba.sum(axis=1, keepdims=True)\n",
    "        labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    score = np.mean([f1(p, y) for p, y in zip(preds, final_test.spans)])\n",
    "    print(threshold, score)\n",
    "    scores.append(score)\n",
    "scores_standard = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f5835449a10>"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAABhhElEQVR4nO3dd3gURR/A8e/k0nulJaGH3ntvCqIgTURAFGw0EdTXhvoqtld99VVUFEVEilRpUpUuiLQAIUAoCaGlQHpvV+b9Yw8MISSX5CCBzOd57uFub3f2twfMb3d2dkZIKVEURVEqH5vyDkBRFEUpHyoBKIqiVFIqASiKolRSKgEoiqJUUioBKIqiVFK25R1ASfj6+sratWuXdxiKoih3lcOHDydIKf0KLr+rEkDt2rUJDg4u7zAURVHuKkKIi4UtV01AiqIolZRKAIqiKJWUSgCKoiiVlEoAiqIolZRKAIqiKJWUSgCKoiiVlEoAiqIoldRd9RyAoihKmRlyITu58JewgRYjwcWnvKO8I1QCUBTl7mUyQvwZyEq8daWenQzZKf+812cWXebO/0CH8dDlBXD2viOHUV5UAlAU5e4iJcQeg+O/wvGVkHHl5nVs7MDJ65+XZyBUb2H+7Hnjd05e4OSt/ZkWDX/+F/76Eg7+CB0nQOfn79lEIO6mGcHatWsn1VAQilK8PGMeVzKvkG3Ipo5HHex19uUdUtklX9Qq/dAVkHBGq+SD+kHjh8G9xo0Vur0LCFH6fV0Ngz8/hbC1YO8GnSZqicDJy2qHcycJIQ5LKdvdtFwlAEW5++Qac4nNiCUmI4bozGhiM2KJzogmJiOGmIwY4rPjkWj/t22FLXU969LIuxFNfJrQyLsRDb0a4mrvWi6xS5MJpETodMWvnJWkVcKhK+DSPm1Zzc7QYgQ0GXL7z8yvnjQngt/AwR06TdJed1kiUAlAUe4iOYYcYjJjrlfo117XKvv47Pgb1tcJHXWFHy1SPaiXYIv/VT1eUWnYxaegx4ReGMnFQB5GTDZgtAGdnT0O9k442Dvj5OCKs6Mb9naOYKtD6GwRtjrQ2SJ0Okw2OrINJox6PabcPIwGA6a8PKTegDQYkHo90qAHvQEMBjDoEUYDwmBEGA3YXH8ZsZEmAPROLth6eeLk443OyxOdp/nl5oouLxrb5FB0SSHo7PLQVauDrt1wbNqNAq9ad/4v5MoJ+PMTOLUeHDzyJQLPOx9LKagEoCi3QUpOCvtj93M2+SwGkwG9SY/BZMAojRhMhn9e0lDoZ6PJqG2T7/tMfSZJOUk37MfWxpbqLtWp4VoDf4dq1EtzJDDOhE90Ok4X4uDcBQxX466vr/PywqFRQ+wDAgGQRiMYDeTkZpGalUxGdgqZOWlk5qajz8tFJ8HGJHHEDhcbR+xM9tgYdZj0NhjzTBj1BoQ0YbCxxWhjg0HoMNhoL6PQobfRYbTRoRfanwYbHVJni7S1BZ0ObO3A1hbs7BC2tuTpjeQlp+Cmz8LbkE11kYN3bgp2menIXOMtf2/h4PBPovD0ROflha23F3Y1amDn73/9pfP2RpSlCehWYkO1K4LTG7RE0HmylggcPay/LytSCUBRrMBoMnIi8QR7o/eyN3ovJxJPYJImbIQNdjZ22NrYohM6bG1ssbWxLXSZrY0ttsK20M86Gx3Ots7UcK1BDZfq+Oe54hudgeOFq+jDw8k5c5bcc+dArwdA2NlhX78+jg0a4NCwIQ4NGuDYsAE6X1+LKsAcvZHDl2PYeSGEY1dPcikjnAx5AWEfjxBa3aCTLvja16G2e32qOflTxak61VxqUMO1Op6ObjjY2uBop8PB1gaHa3/a2hS7//j0XE4c2YsIXUHjxC1UJZEM6cge0Yk4v/uoXrM1LT10OOVkYExJwZiSav7zxpchMRFTauoNZQtHx3xJQfvT3t//+jJLf59bij0Guz6FMxu1yr/zFOg4ERzdS1/mbaQSgKKUUlxWnFbhx+xlX8w+0vLSEAia+zanq39Xuvp3pZlPM3Q2FrRpF0EfF0d2cDDZx0LJOXuG3DNnMSb9cyVgW60aDg0b4NigIQ4NG+LYsAH2tWsj7OwsKj87z0hYbBonolM5Hp3KiehUwuMyMJq0OsDT2Y7m/h408/egQTUHXFzjSTac53TyaU4lnuJcyjlyjDk3lOnp4Im/q792ZZLvT39Xf6q7VMfZzvmflaXUumsmhMPl/RD6K8SdBBtbZP37uFp7MNtNbfgzMpN95xJJzzUgBDSr4UHX+r50D/KlbS0vHO1u/p2NGRnoo2PQR0ejjzH/me9lTEm5YX3h4PBPgsh39eDQIAjHBg0s/BsDYkK0K4Izm8DR05wIJlS4RKASgKJYKM+Yx9G4o+yN3stfMX8RnhwOgJ+TH11qdKGbfzc6Ve+Ep6Nnmfajj44mKzhYex08RN5Fbc4O4eh4/UzeoUFDc6XfAJ1nyfZ3PiGTnafjOBGjVfYRcRmY63p8XOxp5u9hrvDdaebvgb+nU5FnxVJKknKSrt9szv/ntfd5prwbtvG2caSGsKWG3oB/Vir+OZnUMBiop9dTo1pb7WZu06Hg4nvDdgajidDoVP4KT+CviASOXkpGb5Q42NrQoY43Xev70q2+L02qu2NjU/yZvCkzE31MDHnXk8KNScKYnHx93ar/fhvvxx8vwS8NxByFXZ/A2d+1G8TXEoGDW8nKuU3KlACEEP2BrwAdMFdK+Ukh64wAZgASOCalHC2E6A18mW+1RsBIKeVaIcR8oCdw7dptnJQypKg4VAJQbpdLaZfYG6M16xy8cpBsQza2Nra0rdKWLv5d6FqjKw28GpS62UBKif7iRa2yP3SIrEPB6GNiALDx8MC5bVuc27XDuX17HBs3QtiW7hGdpMw8NoTGsPpINCGXUwDwc3O4fmbfrIY7zQM8qObuWLYmECkhMwESzkJiOCSEY4o/S1JyOFGZV4jRCWJsbYm21RHj6EK0nT0xwoief+qb+2rex4QWE2js07jY3WXmGjh4Pok94QnsjUjgzNV0ALyc7ejZwI/JvevToGrpK1tTVhb6mBjivviSjB07qPrWW3g/MabkBUUf1pqGwv8AFz8Ytwn8SnBFcZuUOgEIIXTAWaAvEAUcAkZJKcPyrRMErAD6SCmThRBVpJRxBcrxBiKAACllljkBbJBSrrT0IFQCUKwlx5DDgdgD/BX9F3tj9nI5/TIAAa4BdPPvRlf/rnSo1uHGJowSkFKSFxFxQ4VviNd67uh8fLTKvl07nDu0xyEoCGFT+mG5cvRGtp+KY83RKHadicdgkjSq5sbQ1v4MaFGdAK/SHUO+g4HoI3B+l9Z8kxCuVfo5+drddQ7gUx9864NPEPg2ML+vf/0GqUmaSMhOICYjhr0xe1kctph0fTo9AnowocUEWvi1sDikuLQc9p5L4K/wRLacvEJGnoFhrQN4qW9QmY5X5uUR9fLLZGzbTtU3p+P95JOlKygqGJY8pvUSenZ7ufcWKksC6AzMkFI+YP48HUBK+XG+df4LnJVSzi2inPFATynl4+bP81EJQLnDTiWeYlX4KjZFbiJdn46TrRMdqnXQ2vJrdKWme81SlSuNRnLPnNEq++BgsoIPX29WsK1aFef27c2vdtjXqVPmHiomk+TghSTWHIlm0/FY0nMNVHV3YHArf4a29qdx9TK2QUupnc2eXANh6yD1krbcrbq5ojdX8j5BWkXvEQglvAeSnpfO0tNLWRi2kNTcVLrU6MKEFhNoU7VNicpJzszju10RLNh3ESSM6VSLKX3q4+1SuoffpF5P9Mv/In3rVqq88To+48aVqhwu/g0LBkHdnjB6RYl/H2sqSwIYDvSXUj5r/vwE0FFKOSXfOmvRrhK6ojUTzZBS/l6gnB3AF1LKDebP84HOQC6wHXhDSplbyP7HA+MBatas2fbixULnNlaUW0rLS2Nz5GZWha/iVNIp7G3s6Vu7L4PqDqJdtXalekpWSkne+fNk7v2bzH37yDp0CFO61ixhFxCgVfbmM3y7gACrdUmMiEtn9ZFofguJITolG2d7Hf2bVWNY6wA61/NBZ0F7+C2ZTBAdrD30FPYbpF7Wnrat11t76Krhg7flwatMfSbLzyxnwckFJOUk0aFaBya2nEi7qu1K9LvFpGQzc9tZVh6OwtnelvE96vJMtzq4OJS8OU3q9UT/6xXSt2yhymuv4fP0UyUuA4Dgn2HDi9B1GvR9v3RlWMHtTgAbAD0wAggAdgPNpZQp5u+rA6FADSmlPt+yK4A9MAc4J6Us8hdSVwCKpaSUBF8NZk34GrZc3EKuMZeGXg15pMEjPFTnITwcSt5vWx8XR9b+/WT+vY/MffswXL0KgF1gIC6dOuHcQav07apXt+qxxKXnsP5YLGuORnEiOg2djaB7kC9DW/vTt0lVnO3LMKSXyQRRh7SnbcN+08bC0dlDvT7/VPp3qPki25DNyrMr+fnEz8Rnx9OmShsmtJhA5xqdS5QIIuLS+eyPM/xx8iq+rva80CeIUR1qYm9bsmY2qdcT/eprpP/+O1VefQWfZ54p6SFpNrwMwT/BsB+1m97l4HY3AX0PHJBS/mz+fO2M/pD58zSgqZRy/C320Qt4RUo5sKhYVAJQipOQncBvEb+xJmINF9Mu4mrnyoC6AxgaNJQm3k1KVJGYMjPJPHSIrH37yPx7H7nhWm8gnYcHzp0749KlMy6dO2MfGGj148jKM7A17Cqrj0TzV0QCRpOkub8HQ1r7M6hlDfzcHEpfuMkEUQfh5Fqt0k+PMVf690HTIVqlX44PNuUac1kdvpqfjv/E1ayrtPBtwYSWE+ju371Ef39HLyXz6e+n2R+ZRKC3E//q25BBLWtY1GvoGmkwEPPa66Rt2oTfv17G97nnSn5ARj0sHKw1qT21GfxL1sRlDWVJALZozTv3AdFoN4FHSylP5lunP9qN4bFCCF/gKNBKSplo/n4/MF1KuTPfNtWllLFC+xv9EsiRUr5RVCwqASiFMZgM7I3ey6rwVeyO2o1RGmlbtS3DgobRt1ZfnGydLCpHGgxkhx4nc5/WrJMdcgwMBoS9Pc7t2por/S44Nm5cppu2BZlMkrj0XM4nZHIxMZODF5L448QVMvOM+Hs6MaR1DYa29qd+lTJ0KTSZtL73J9fCqXWQHqvduK1/n/lMv3+Fe5o1z5jHb+d+46fjPxGdEU1j78ZMaDGB3jV7YyMs+/2llOwOT+DTzacJi02jUTU3Xu/fiF4N/SxOJtJgIOb1N0jbuBG/l17Cd0Kh57FFy0yAOb1AmuC5neBWteRllEFZu4E+BMxEa9+fJ6X8SAjxPhAspVxnrsT/B/QHjMBHUspl5m1rA3uBQCnNg4Bw/Z6AHyCAEGCilDKjqDhUAlDyu5x2mTURa/gt4jfisuPwcfRhUP1BDK0/lDoedYrdXkpJXmTk9SadrAMHMGVmghA4NmmineF36YJT69bYODqWKVaTSXI1PcdcyWdxISGTC4nm94mZ5Oiv/9fAzcGWh5pXZ2gbfzrU9i7RGesNctO1oQvC1mo3cjOuaJV+UF+t0m/wQIV7YKkwepOeDec2MPf4XC6lXyLIK4jxLcbTt2Zfix++M5kk60Nj+N+Ws1xKyqJDHW9e79+ItrUsG9RNGgzETH+TtPXr8Zs2Fd9Jk0p+ILGhMO8BqNYcxq4H2zJcxZWQehBMuSfkGnPZdnEbq8NXc/DKQWyEDd38uzEsaBg9AnpgZ1P8U7E5Z86SsnIl6Vu23NiOb27Wce7YEVuvko/2aDJJrqTlmCt3rWK/YK7wLybdWMnb62wI9Haijq8LtXxcqO3rQm0fZ2r7uFDD06nom7kmE2TGa003aeZXeiykxZqXxWqfc9O09W0dof792gNXDR6oMA8nlZTBZOD3C78zJ3QO51PPU9ejLs+1eI7+tftja2PZfZA8g4nlhy7x1fYIEjJy6dukKq8+0NCiZwik0Ujsm2+S+ts6fF+Ygt/zz5f8IE6ugV/HQesnYNA3ZRuyugRUAlDuSvFZ8ZxOOs3ppNOcSjrFgdgDpOWl4e/qz7CgYQyuN5iqLsVfThszMknbtJGUlavICQ1F2Nnh2qsnLt2649Kl9O34kfEZLDlwiT/PxnMxKYs8Q75K3taGWt7OWgXv42yu5F2o7etMdY9bVPJGg9blMi0mX4VeoJLPuAImw43bCR24VdO6abpXB7ca2p/edbW2fYfyGfr5djCajGy9tJU5oXMITw6nrkdd5vSdY9G/g2uy8gzM++s8P/wZSWaegWFtAnjx/uKfIZBGI7FvvU3q2rX4Pv88fi9MKXL9Qm3/APZ8Dg9+Bh1L0ZxUCioBKBWaSZq4nH6ZU0mnOJ14mtPJpzmdeJrEnMTr6wS4BtCqSiuG1B9C+2rti20HllKSc+wYyStXkrZpMzIrC/v69fB69FHcBw0q1Vk+aGeRW8OusvjARf4+l4itjaBbkC8NqrpRy3wWX9vXhWrujsWcyRu1h6piQ7ShBGKOas0Ehuwb17N3NVfsNbTXtff5K3vXKuXaz7w8mKSJHZd28NZfb1HLvRbz+88v8YN7BZ8hGNulFv/q17DQ8YaukUYjsf9+h9TVq/GdPBnfF6aUrJuvyQTLRkP4FnhyLdTpUaKYS0MlAKXCyDPmEZEScf3M/nTSac4knSHLkAVoE5jU86xHI+9G118NvRviZm9Z04UhOZm0detIWbmS3PAIhLMz7g89iNfw4Ti2bFnqPvmXk7JYevASK4Ivk5CRh7+nE6M71uTRdgFUcSvmHoHJBEnnzBV9iLmyP/bP/LR2zlC9JdRoDVWbgrv/P5X8XdBOX572RO1hyo4p9PDvwczeM0s1KN+1ZwhWBEfRtIY7sx9vS02fWycTaTIR+847pK5chc+kifhNnVqyf1c5aTD3fq0pb/xO8Kpd4phLQiUApVzkGnM5Hn/8ehPO6aTTRKZEYpBaE4azrfP1Cr6xd2MaeTeinme9Ej+cJU0msvbv19r2t25D6vU4tmyB5/DhuD/4EDpXl1LFbzCa2HkmnsUHLvLn2XgE0KdRVR7vVJMeQX6Fn+FLCUmR+c7qj2mVfp72oBi2jlCthVbZX3v5BlW6M3hrWnZ6GR8d+IgxjcfweofXS13OztNxTFt2FICZI1vRp9Gtm5WkycSVd2eQ8uuv+Iwfj99LL5YsCSSegx97g3sAPLPltjbTqQSg3FFXMq+w/MxyVp1dRXKuNiSCr5PvDRV9I+9GBLoFWtylrzD6K1dIXbOGlJWr0EdHo/PwwH3wIDwfGY5jw9IPwnUlNYdlhy6x7OBlrqTlUMXNgZEdajKyfSA1PAt0K02/ok1XeO3sPjbkn3FydA5Qrdk/FX31VuDXCHRleHhLKdR/D/2XRWGLmN5hOqMbjy51OZcSs5j4y2HCYtOY2qc+0+5vcMumPGkyceW990lZvhyf557F7+WXS5YEIrbD4uHQaCA8ugCs2L04v1slAPWvULGaa0/fLj29lB2XdiCR9A7szZD6Q2jm2wxfJ9/iC7FkP3o9GX/+ScqvK8nYswdMJpw7d8Lv5Zdwu/9+bBxK173OZJLsiUhg8f6LbD8dh9Ek6R7ky4xBTbmvcRXsdOb/nHlZ2jgv53ZA5E6IM4+LaGOnNd80HfZPhV+lMegsG69fKZt/tf0XUelRfHroUwLcAugRULq29Zo+zqye3IV/rz3B1zsiOHo5ha9HtsarkLGFhI0N1d59B2wEiT/ORZpMVHnlFcuTQP37oO8HsOUt2P0Z9Cr91UtpqCsApcyyDdlsitzEktNLOJt8Fg8HD4YFDWNkw5HUcK1htf3kRkZqZ/tr1mJMSMC2ShU8hg3F85FHyvQ0bkJGLr8GR7Hk4EUuJ2Xj42LPo+0CGdUhkFo+LtrN2thjWmV/bidcPgDGPO3svmYnbaycOj2garM72rdbuVmWPoun/niK86nnWfjgQhp5Nyp1WVJKlh26zLu/ncTPzYHZY9rQIsDzlute/fAjkhcvxvupp6jy2quWJwEpYc1ECF0Gjy2GxkUOiFAqqglIsbrojGiWn17OqvBVpOWl0dCrIaMbj+ahOg/haFu2B6eu0V+5QtrGTaRu3EBu2CnQ6XDt2RPP4cNx7dG91OPmSynZH5nE4gMX+ePkFfRGScc63jzeqRYPNK2KQ0a0Vtmf2wHn/4Rs84QhVZtB3V7aWDk1O4N9GYdaVqwuLiuOxzc9jkmaWPLQkhJ1Dy1MaFQKk345Qnx6LjMGNWVUh8BCK3cpJVf/8zHJixbhPfZJqrzxhuVJQJ8DPz+oza/wzFao2qRMMRekEoBiFVJKDlw5wJJTS/gz6k8Egj41+/B448dpU6WNVUa9NKakkPbHFtI2bCArOBikxLFFCzwGDsCtf3/sqlQpVbl6o4mD55PYcvIKW8OuEpOag7ujLcPbBjKmlQd1M47+c5afdE7byK061O2tneXX7aV1t1QqvDNJZxj7+1gC3QJZ0H9Bqed1uCY5M49py0PYfTae4W0D+HBIs0K7ikopifvkE5IWLMTriSeo+uZ0y/9PpMVow0XYOWnDRVhx5FWVAJQyydJnsSFyA0tPLyUiJQIvBy+GNxjOiIYjqOZSrczlm7KySN+5k7QNG8n46y/Q67GvUwf3hwfiMWAA9rVqlarczFwDu8/GsyXsKttPXSUtx4CDrQ29grwZVeMq3WyOY3vhT20CD2kEOxeo3c1c4fcGv4Z37GlNxbr+iv6LKdun0M2/G1/1/qrMczYbTZKvtofz9fZwmlR35/sxhXcVzZ8Eas77CZcuXSzfyeVDMP8h7epyzGqrdRZQCUAplctpl1l6Zilrw9eSrk+nsXdjHm/8OP3r9MdBV7b2bqnXk/n336Ru2Ej69u3IrCxsq1bFfcAAPAYOwKFx41JdUcSn57L91FW2hl1lT0QCeQYTXs529GlUlSE1s+iUuhm748u1J2qFjXaz9tpZfkAHsC3dRCJKxbP89HI+PPAhjzd+nDc6FDnWpMUs6Spqys0los99ODZrSs0ffijZDo7+Ar89Dx0nwYM3zb5bKqoXkGIxKSX7Yvax5PQSdkftRid09K3Vl9GNR9PSr/QPUoHWbS776FFSN2wgffPvGFNSsPHwwGPgQNwHDsC5XbtSjbR5ISGTLWFX2HLyKocvJSMlBHg5MaZjLfo3cKFt+i50x/4Hv+/Xhk0I6gstHtMqfafSPRGsVHyPNXqMS+mXWBi2kEC3QB5vXMLJ3gvRu1EVNrzQnYm/HObp+cGFdhW1cXDAa/QoEr6ZRW5kJA5161q+g9Zj4MoJODBb60LcuhRzE1tIXQEoN4jLiuP13a8TfDUYb0dvHm3wKCMajqCKc+nbvqWU5J49S9qGDaRu3IghJhbh6Ihbnz64DxyIa7euCPsSPvglJcejU9ly8ipbwq5w9qo2kGyT6u70a1qVfo2r0lh/HHF0sTYapj5Lm8Kw1ePQcqQ2bo5SKRhNRl7e9TK7onbxde+v6RnY0yrl5uiN/HvtCX49HEX3IF++Gtn6hmkoDUlJRPTqjcfQoVR/b0YJgzbAL8O050vGbYLA9mWKVTUBKcX6O/pvpv81nWxDNq+0e4Uh9YeUarrEa6ReT8rKlSQvWUJueATodLh064rHwIG49emDjUvJns41GE3sj0xiS5h2Ezc2NQcbAR3qeNOvSTX6NqlKoC4Zji2Bo4sh+TzYu0GzYdpZVEB71Z5fSWXps3j6j6eJTI1kQf8FNPZpbJVyC3YV/e7xNrQM9Lz+fey//03quvXU37Wz5GNPZSVpTwrrs2H8Lm1okFJSCUC5JYPJwOxjs/kx9Efqedbjfz3/R13PElyyFiBNJtI2biL+m2/QX7qk9eAZMhj3/v2x9S5dz4bDF5N4c/UJzlxNx9HOhp4N/OjXpBp9GlXBy94EZzZpbafndgASanfXKv3GD4N96YaBUO4t8VnxjN40GpPJxOIBi63SeeGaW3UVzQ0PJ/LhQfi9OA3fiRNLXvDVMG3MIL+G2mxidqXrXq0SgFKo/E0+Q+sPZXrH6RbPoFWQlJLM3buJ+3ImuadP49CoEVVefgmX7iWbyi+/1Gw9//39NIsPXKKGhyOvP9iIfk2q4WRnoz2cFbIYQldAToo2pkqr0drLu/gJYZTK52zyWZ7c/CSBboHM7z8fFzvrnRzcqqvopefGk3P6FPW3b8emhE2dAJxaD+tegCfXQfUWpYpNJQDlJn/H/M30PVqTz9ud3mZQvUGlLivryBHivviC7ODD2AUG4jdtGu4PPVjqqROllGwIjeW99WEkZebyVNc6vNy3AS6GVDi+QmviuXpcexq38UDtbL9OTzWgmlKsvdF7eX7783T178pXvb+yeDIZS+TvKtq0hjvLJ3SGQwe4/OyzVP/4YzyHDildwTmpZZqys6xTQvYHvkKbEnKulPKmvklCiBHADEACx6SUo83LjcBx82qXpJSDzMvrAMsAH+Aw8ISUMq+oOFQCsA6jycjsY7OZEzqnzE0+OWfOEj9zJhk7d6Lz88Vv8mQ8H3mkxDd187uclMXba0/w59l4mvt78PHQZjQznIBDP2lnQya91nWz1ePQfLjqxaOU2IozK/hg/weMbjSa6R2nW738P05eYcKiw0y7L4gX7w/i/KBBoLOlzprVVnlYsqRK3Q1UCKEDvgX6AlHAISHEOillWL51goDpQFcpZbIQIn+XkWwpZatCiv4U+FJKuUwI8T3wDDC7JAellFx8Vjyv73mdQ1cOlanJJy8qioRvviF13XpsXF3xe+klvJ8Yg41z6Z+41BtNzN1znq+2n0UnBB8+WJPRjvuw+e1fEH9aOwNq/yy0eUIbdE1RSmlEwxFcTr/M/JPzqele0yrdQ/N7oGk1BjSvzpzdkTzesSbe48YR+9bbZB04gEunTlbdV1lYcu3TAYiQUkYCCCGWAYOBsHzrPAd8K6VMBpBSxhVVoHkS+T7AtTFbF6BdPagEcBvlb/L5qNtHpWryMSQkkPD9DyQvX46wscHnmafxefZZdJ6eZYrt8MVk3lpznNNX0nmmfgYve/2Fy1+rtAlTqreCQbOg2SNq7B3Fal5q+xKX0y/z6cFP8Xf1p1dgL6uW/1r/hmwJu8KX28L5aOBA4r74kqSf5991CcAfuJzvcxTQscA6DQCEEHvRmolmSCl/N3/nKIQIBgzAJ1LKtWjNPilSymsTm0aZ93MTIcR4YDxAzZo1LQhXKSh/k09dj7rMe2Ae9TzrlayM9HSSfv6ZxPkLkLm5eA4fju/kSdhVLdtAW9du8q48eI6RLkdY6r8Hr6gjcMVRq/DbPwP+bcu0D0UpjI2w4ePuH/P070/z2u7XmN9/Pk18rDcIWy0fFx7vWIuF+y7wTLfaeI0aRcKsWeRGnsehbsXopGCt2QdsgSCgFzAK+FEI4Wn+rpa57Wk0MFMIUaKaR0o5R0rZTkrZzs/Pz0rhVh7xWfE8t/U5fgj9gcH1B7NkwJISVf6m3FwSf57Pub79SPhuNm69elJ3w3qqvzejTJW/dpM3hif+9ysBh//LEZcXec/wFV4yBfp9BC+fgiHfqcpfua2cbJ345r5v8HTwZMr2KVzJvGLV8l/oUx8Xe1s+2XwGr1EjEfb2JC1cYNV9lIUlVwDRQP7B1gPMy/KLAg5IKfXAeSHEWbSEcEhKGQ0gpYwUQuwCWgOrAE8hhK35KqCwMpUyyt/k82HXDxlcf7DF20qDgdTffiP+m1kYrlzBpVs3/F56EaemZW97v5yQzq8r5tMydhVrdSEIO4Go/5B2tl+n122bFUlRCuPr5Mu3933Lk5ufZMr2KSwZsKRMD0Dm5+PqwMRe9fjsjzMc7l6HmoMHkbr2N/ymTSv5g2G3gSX/0w4BQUKIOkIIe2AksK7AOmvRzv4RQviiNQlFCiG8hBAO+ZZ3BcKk1vVoJzDcvP1Y4LeyHYpyjdFkZNbRWUzcOhEvBy+WDlhaoso/Y89fRA4aTOxbb2NbtQo158+n5twfy1z569Pj2b/w3/BNG16Oe5vOTpeg278QLx6HkYu1MfZV5a+UgyCvID7u/jFnks+wKnyVVct+umsdqrk78p/Np/F64glkTg4py5dbdR+lVez/NvMZ+hTgD+AUsEJKeVII8b4Q4tpdxD+ARCFEGFrF/qqUMhFoDAQLIY6Zl3+Sr/fQ68DLQogItHsCP1nzwCqr/E0+g+oNKlGTjykri9j33uPyc8+BlATM+obay5bh0qngLZ8SkBIuHyRx0Tjk/xrRKfJrspyqk/TQDzi/dhqb+/8NHgGlL19RrKRnQE/aVGnD3NC55BpzrVauk72Of/VrwLHLKWzLccWlWzeSFi/GlFdkr/c7Qj0Idg/ZF7OPN/a8QbYhm7c6vlWis/7s0FBiXnudvIsX8X7qKfymTS313LrXRR/GsPlNbKP2ky6d+F3Xi+r3P0+3Lt3LVq6i3CYHYw/yzJZnyjyxfEFGk2TA13vIyjOyrqMtsRPGU/2Tj/EcMsRq+yjKrZ4DUNfb9wCTNDE7ZDYTtk4ocZOP1OuJn/UtF0aNxpSXS83586n62qtlq/xTo2D1ePixD2lRp3hXP5ZZrTfw4Ou/qMpfqdDaV2tP26pt+en4T1a9CtDZCN54sBGXkrJYo/PHIag+SfMXUN4n4CoB3OVSc1OZsn0K3x37jofrPVyiJp/c8+e58PgYEmbNwmPgAOr+9hsuHTuUPpjcDNjxIfKbthhPrOF742BG2H/LoPHvMX1IO1wd1PQTSsUmhOD5Vs8Tlx3HyrMrrVp2zwZ+dK3vw9c7InAaPYbc06fJOnDQqvsoKZUA7mJnks4wcsNI9sXu4+2Ob/Nh1w8tmvtUSkny0qWcHzoM/cWL+M/8khqfforOza10gZiMcGQhfNMGdn/GYacu9Mj6jAN1p/DrtAdoW6v8ezsoiqXaV2tP+2rtmXt8LjmGHKuVK4Rg+oONSc7S84trY3Te3iTNn2+18ktDJYC71Ppz6xmzaQx5xjx+fuBnHmv0mEVjjOjj4rg8YQJX3nsf57ZtqbNuHe79+5c+kMhd8EMPWPcCWS4BTHb6L48lPseY/t35aWx7vFzU9IrK3WdSy0kkZCdY/Sqgmb8HQ1v7M/dANLZDh5Oxaxe5keetuo+SUAngLqM36vnPgf/w5l9v0sy3GcsfXk6rKq0s2jZtyxbODxpM1oGDVP332wTO/RG7qqWc6Sv+LCx5DBYORuam8Xfrz2gd/QrBhnosebYjk3rVw8ZGTb6i3J3aV2tPh2od+OnET1a9CgD4V78GSAnzfNtoD4YtWmjV8ktCJYC7SFxWHE//8TRLTy/lySZPMqffHHydfIvdzpieTswb04meOg27gADqrFmN9+OPl25UwsxE2PQqfNcJLuwlr/e7vF5tLqP3+dO+tg+bpnWnY12fUhydolQsk1tNJiE7gRVnVli13AAvZ8Z1rc3isxnI+/uTumYthuRkq+7DUioB3CUOXz3MYxse40zyGT7r8Rmvtn8VOxu7YrfLOnSI84OHkLpuHb6TJ1F76ZKSTVB9jSEX/v4Gvm4Nh+ZC23FEjtrDgMNt+fVYAi/eH8SCpzvg61rGrqOKUkG0rdqWjtU7Mu/EPLIN2VYt+/le9XF3tGNu1Y7mB8Osm2QspRJABSel5JewX3j2j2dxsXNhyUNL6F+n+DZ7U14eVz/7jItPjgVbW2ovWYzf1KkIu+KTRoEAIOw3+LYDbHkbAjvApL9Z4/8yA+adITkrj1+e6ciL9zdAp5p8lHvM5JaTScxJtPpVgIezHVN612dVsgN5rduTvHgxshweDFMJoALL0mfxxp43+PTQp3QP6M7SAUup71W/2O1yzpzlwqMjSPppHp6PPkrdNatxatWq5AFEH4afH4QVT4KtE4xZRc5jy5n+l56Xlh+jeYAHG6d2p2v94puhFOVu1KZqGzpV78S8E/PI0mdZtewnu9QiwMuJ+TW6YIiPJ23zZquWbwmVACqoS2mXGLN5DJvPb2Zq66nM7D0TN/uiu2lKk4nEeT9zYfhwDAkJBMz+jurvv4eNSwnnPU2Nvv4gFwnhMPBLmPgX5z07M/S7v1l68DKTe2k3e6u6l26SakW5Wzzf6nmScpKsfhXgYKvj1QcassY2gFz/WtpQ63f4wTCVACqgPy//ycgNI4nLiuP7+7/nuRbPYSOK/qvSx8ZyadxTxP33v7j06EHd9etw6927ZDs25MKeL2BWOzi5Frq9BFOPQrun2Xgynoe/+YvY1Gx+Htee1/o3wlan/vko975WVVrRpUYXfj75s9WvAh5uUYNmAR4sq9WV3FOnyDp4yKrlF0f9D65Aro3iOWXHFALcAlg+cDld/LsUu13OmbNcGPEYOSdOUP2jDwmY9Q223t4l23nENpjdBba/B3V7w5SDcP8Mcm1dePe3Ezy/5AgNqrqyaWp3ejcqZddRRblLTWo5iaScJJadWWbVcm1sBG8+2JjVPs3Jc/O44w+GqWfzK4jU3FRe3/M6e6P3MqT+EN7q+BaOtsU3r2QdPszlSZOxcXSk1rKlODZoULIdp1yC36fD6Q3gXRceXwlBfQFtcvbnlxwhNCqV57rX4bX+jbBTZ/1KJdSqSiu61ujK/BPzGdlwpEVP3FuqS31fujSpwbrITgzf+Qe558/jUOfOzBim/jdXAKcST/HYhsc4EHuAf3f6N+93ed+iyj991y4uPf0Mtt7e1F66pGSVvz4H/vwMZnWAiO3Q598wef/1yn/LySsM+HoP5xMy+eGJtrw1oImq/JVKbXKrySTnJrP09FKrlz39wcasqdkJo86WpIV37sEw9T+6nK07t44nNj+B3qRnQf8FjGg4wqIHtFLWriXq+Sk41K9PrSWLsfMvdErlwp39Q3uQa+eH0KAfTDkEPV4BWweklHy59SzjFx2mlo8LG1/ozgNNq5XhCBXl3tDCrwXd/Lsx/+R8MvWZVi27YTU37u/amB2BbUhZvQZjSopVy78VlQDKiZSSLw9/yVt/vUULvxasGLiCFn4tLNo28ef5xL4xHef27am5YIHl7f1J57XhG5aMABtbeGINjFgInoHXY/r09zN8tT2cR9sGsHJSZ2r6WO9SV1HudpNbTiYlN+W2XAW81LcBGxr0gNxcku/Qg2EqAZSTH0J/YN6JeYxoMII5fefg41T88AlSSuL+9wVxn36KW79+BM75AZ2rBV089dmw8z/wbUc4vwf6vg+T/tamYMxX9n82neL7P88xplNNPn2kBQ62urIcoqLcc5r7Nae7f/fbchVQ3cOJvgO6csSvAXELF92RB8MsSgBCiP5CiDNCiAghxBu3WGeEECJMCHFSCLHEvKyVEGKfeVmoEOKxfOvPF0KcF0KEmF+trHJEd4HFpxbzbci3DKo3iLc6vYWtTfH34qXBwJV33iHxxx/xfOwx/L/8Ahv7YkbalBJObdCe4v3zU2g8EF4Ihq7TwNY+32qS9zeE8eOe84zrUpsPBjdTA7kpyi1MbjWZ1NxUlpxaYvWyJ/Ssx9ZmfRCJCaTeiQfDpJRFvgAdcA6oC9gDx4AmBdYJAo4CXubPVcx/NgCCzO9rALGAp/nzfGB4cfvP/2rbtq28260NXyubzW8mp+2YJvVGvUXbGHNy5OUpU2RYw0by6syZ0mQyFb9RQoSUi4ZJ+a67lLM6Shm5u9DVTCaT/Pfa47LW6xvk++tPWla2olRyk7dNll2WdJHpuelWL3v+X5Hy93Y95LEHH7ba/0cgWBZSp1pyBdABiJBSRkop84BlQMH5Bp8DvpVSJpuTSpz5z7NSynDz+xggDvArYY66Z2y7uI13/n6HztU7898e/7XozN+YkcHl58aTvnUbVd98kyrTphV9kzgvE7a/r93kvXQAHvgPTNwDdW6eitFkkry19gQL911kQo+6vD2gcelGCFWUSmZyy8mk5aWx+NRiq5c9qmMtdre4H7vIcDJu84xhliQAf+Byvs9R5mX5NQAaCCH2CiH2CyFuGq1MCNEB7QriXL7FH5mbhr4UQhQ6jKQQYrwQIlgIERwfH29BuBXT3zF/89ru12ju25yZvWdiryt+ohRDQgIXn3ySrCNHqPHZf/F+8olbryyl9vTurA6w53/QdBi8cBg6Pw+6mweAM5kk01cfZ8mBS0zuVY83HmykKn9FsVBT36b0CujFwrCFpOelW7Vse1sbuk16glR7F8K+/sGqZRdkrZvAtmjNQL2AUcCPQgjPa18KIaoDi4CnpJQm8+LpQCOgPeANvF5YwVLKOVLKdlLKdn5+d+fFQ0hcCC/ufJE6HnX49r5vLXqIJC8qiguPP05e5HkCv/sWj4cfvvXKuRmw4gn4dSw4ecJTv8OwH8CtaqGrG02S11aFsjz4MlP71OfVBxqqyl9RSmhSq0m37Sqgf9taBLfsjeuR/aSejbB6+ddYkgCigcB8nwPMy/KLAtZJKfVSyvPAWbSEgBDCHdgIvCWl3H9tAyllrLl5Khf4Ga2p6Z5zJukMk7dPxs/Jjx/6/oCHg0ex2+ScOcvFUaMxpqRS8+d5uPboceuVky/CT/3g9Eatd8/4P6FW51uubjCaeOXXY6w8HMVL9zfg5X6q8leU0mji04RegdpVQFpemlXLFkLQ8cXxGG1sOPj5bKuWnZ8lCeAQECSEqCOEsAdGAusKrLMW7ewfIYQvWpNQpHn9NcBCKeUNk2uarwoQWu0zBDhR6qOooC6mXWT81vE42zrzY78fLZq9K+vwYS4+8QTY2FD7l0U4t25965Uv7IUfe0NqlDaEQ9dpoLv1fQWD0cRLK46x5mg0rz7QkGn3B5XmsBRFMZvccjLpeeksDrP+VUD7tkGcadYV373biIuOs3r5YEECkFIagCnAH8ApYIWU8qQQ4n0hxCDzan8AiUKIMGAn8KqUMhEYAfQAxhXS3XOxEOI4cBzwBT605oGVtyuZV3huy3MAzOk3hxquNYrdJn3nzn+GdliyGIegIirowwtg4WBw8oLntkP9+4osW280MW1ZCOuPxfDGg414vnfx8wooilK0xj6N6RPYh0Vhi6x+FQDQ6qVJOBrz2PHZ7bkXIOQdHn+6LNq1ayeDg4PLO4xiJWYnMu73cSRkJzDvgXk09mlc7DYpa9cS+9bbODZqROCPc279dK/RAFveggPfaw9yDZ+nJYEi5BlMvLD0CH+cvMrbAxrzbPdSTAmpKEqhTied5tH1jzKp5SQmt5ps9fJ3DByBfdRFamz+g7rVPUtVhhDisJSyXcHl6klgK0vLS2PitolcybzCrPtmWVT5Wzy0Q3YyLB6uVf6dJsPoX4ut/HMNRiYvPswfJ68y4+EmqvJXFCtr5N2I+2rex6KwRaTmplq9/AZTxuOOAbeYi1YvWyUAK8o2ZDNl+xQiUiL4sveXtK3atsj1Zf6hHR54oOihHRLC4cf74MJfMGgW9P+4yPZ+gBy9kYmLDrPtVBwfDG7KuK53ZohZRalsJrWcRIY+g0Vhi6xetn+/PjT9azd+bVtavWyVAKxEb9Tz0s6XOBZ/jE+6f0I3/25Fri+l5OoHH/wztMMX/7v10A4R27TKPycVxq6HNkU8D2CWozcyftFhdp6J5z9Dm/NE59qlOCpFUSzR0LshfWv1ZfGpxVa/ChA2NpaN+VUKKgFYgdFk1CZzidnLu53f5YHaDxS5vpSS+P/9j+QlS/F+5mmqzXgXoStk4DUpYd93sPhRbcTO8TuL7OJ5TXaekWcXBLMnPJ7/PtKC0R1rlvbQFEWx0IQWE8jQZ7Aw7M6N519WKgGUkZSS9/a9x9aLW3ml3SsMCxpW7DaJP8whce5PeI4aSZVXXim8H74hF9ZNgT+mQ8OH4Ok/wLP4ijwrz8DT8w+x91wCnw9vyYj2gcVuoyhK2d3Oq4DbRSWAMpBS8lnwZ6yJWMOEFhMY23Rssdsk/bKY+JkzcX/4Yar9+9+FV/4Z8bBgEBz9BXq8BiMWgYNrsWVn5BoY9/MhDpxP5MsRrXikbUBpDktRlFKa1HISWfosFpxcUN6hWEQlgDL4IfQHFoUtYnSj0Tzf6vli109Zu5arH36I6333UeM/HyFsCvn5rxzXHu6KDdG6ePZ5Cwpbr4D0HD3j5h3k8MVkvhrZmiGtSzBDmKIoVhHkFUS/2v1YfGoxKTkp5R1OsVQCKKX8Y/q/3uH1YodTSNu6ldg338K5cyf8v/gfwu7mAdo4tV4b1sFkhKd/h2aPWBSL0SSZvPgIIZdTmDWqNQ+3LP6hM0VRbo+JLSaSbchmQVjFvwpQCaAUfov4jU8OfkKfwD681+U9bETRP2PG3r3EvPwvnJo3J3DWLGwcCgx8KiX8+V9YPgaqNNZu9tYoYgiIAr7bGcGe8AQ+GNKMB5tXL80hKYpiJfW96vNA7QfuinsBKgGU0M5LO3nn73foVL0Tn/X8rNgx/bOOHCFqygvY161L4JwfsHEp0J0rLwtWPgU7P4IWj8G4TeBm+STsf59L4MttZxna2p+R6oavolQIzzR/hmxDNhsjN5Z3KEVSCaAEsvRZvLfvPRp6NeSr3l8VO6Z/TlgYlydMxK5KFWr+NBedR4GRQFOj4ef+2jj+978HQ38AO0eL44lPz2XashBq+7rw4ZBmalRPRakgGnk3oolPE1aHr6YiD7ejEkAJLDi5gMScRN7q9FaxY/rnRp7n0rPPYePqSs2f52HrW2Ak0LRYmP8QJJ6DUcug24tQggrcaJK8uPwo6Tl6vnu8DS4Oxc8upijKnTOs/jDOJJ8hLCmsvEO5JZUALJSQncDPJ3+mb62+tPQr+pFsfXQ0l55+GoSg5ryfsKtR4KZsZiIsGgKZCfDkOmh40wRqxZq1I4K9EYm8P6gZjaq5l3h7RVFurwfrPoijzpHVZ1eXdyi3pBKAhWaHzEZv1DO19dQi1zPEx3Px6acxZWZS86e5ONQpMP5OThr8MgySzmtn/gFFjxdUmL8jEpi5/SzDWvvzaDvV119RKiJ3e3f61urLpvObyDZkl3c4hVIJwALnU8+zKnwVwxsMp7ZH7VuuZ0xJ4dIzz2KIiyfwhx9wbNToxhXysmDJY3D1BIxYWOhE7cWJS89h6rIQ6vq68IFq91eUCm1Y0DAy9Blsvbi1vEMplEoAFvjqyFc46ByY2HLiLdcxZmRyacIE8s6fJ/DbWTi3KdCN05Cnzdt7aR8Mm1OqZh+jSfLishAycvV893hb1e6vKBVc26ptqeVei9XhFbMZyKIEIIToL4Q4I4SIEEK8cYt1RgghwoQQJ4UQS/ItHyuECDe/xuZb3lYIcdxc5teigp7KhsSFsP3Sdp5q9hQ+Tj6FrmPKzSVqyhRyTpzE/8svcOnSpcAKRlj9nDaq58MzLX7Aq6Cvt4fz97lE3h/cjIbV3EpVhqIod44QgqH1h3L46mEupF4o73BuUmwCEELogG+BB4EmwCghRJMC6wQB04GuUsqmwIvm5d7Au0BHtEnf3xVCXJvBZDbwHNrk8UFAyU+JbzMpJf8L/h++Tr482eTJwtfR64l+8SWy9u+nxn8+wu3++29cwWSCdVMhbC30+wjajitVLHsjEvh6RziPtAlgRDvV319R7haD6w9GJ3SsiVhT3qHcxJIrgA5AhJQyUkqZBywDBhdY5zngWyllMoCU8toMxg8AW6WUSebvtgL9zRPCu0sp90utk+xCtInhK5Qdl3YQEh/C5FaTC+32KU0mYqa/ScbOnVR95994DC7ws0gJf7wJIb9Az9ehy5RSxRGXnsO0ZSHU83PlgyFNS1WGoijlw9fJlx4BPfgt4jf0Jn15h3MDSxKAP3A53+co87L8GgANhBB7hRD7hRD9i9nW3/y+qDLLld6kZ+aRmdTxqMPQ+kNv+l5KyZX33ydtwwb8XnoJ79Gjby5k18dwYDZ0nAS9ppcqDqNJMm1pCJm5Br57vA3O9qrdX1HuNsOChpGYk8ieqD3lHcoNrHUT2BatGacXMAr4UQjhaY2ChRDjhRDBQojg+Ph4axRpkTXha7iQdoEX27xY6HAP8V98Qcqy5fg89yy+E8bfXMDfs+DPT6HVGHjgPyV6yCu/r7aHsy8ykQ+GNKNBVdXuryh3o27+3fBz8qtwN4MtSQDRQP5G5wDzsvyigHVSSr2U8jxwFi0h3GrbaPP7osoEQEo5R0rZTkrZzs/Pz4Jwyy5Ln8V3Id/Rpkobegf2vun7hB/mkPjjXDxHjcTv5ZdvLuDwfNjyFjQZAoO+tmg458LsCY/nmx3hDG8bwHA1tr+i3LVsbWwZXH8we6L3cDXzanmHc50lNdMhIEgIUUcIYQ+MBNYVWGct2tk/QghftCahSOAPoJ8Qwst887cf8IeUMhZIE0J0Mvf+eRL4zQrHYxXXhnx4ud3LN/WzT1m5kvgvv8R94MDCJ3Q5vhLWvwj1+8KwH8GmkKkeLXA1LYcXl4VQ38+V9werdn9FudsNrT8UkzSx7lzB6rP8FJsApJQGYApaZX4KWCGlPCmEeF8IMci82h9AohAiDNgJvCqlTJRSJgEfoCWRQ8D75mUAk4G5QARwDthsxeMqtaKGfDCmp3P1s89x7tiRGh//5+YJXc7+AWsmQM3O2oNetkUPFncrBqOJqUuPkpVnVO3+inKPqOlek/bV2rMmYg0maSrvcACt7b5YUspNwKYCy97J914CL5tfBbedB8wrZHkw0KyE8d523x/7/pZDPiQtXIgpNZUqr71684Qu5/fAiiehajMYvRzsix4srihfbQ/nwPkk/vdoS4JUu7+i3DOGBQ1j+p7pBF8JpkP1DuUdjnoSOL/zqedZeXZloUM+GNPSSJq/ANf778OpaYEmmajDsHQkeNWGMavBsfSDs+0+G8+snRGMaBeg5vRVlHvM/TXvx83OjdURFeNmsEoA+Xx95OtbDvmQNH8BpvR0/KYU6Mt/9aQ2uJuLLzyxFlwKf1rYElfTcnhpeQhBVVx5b1CFuzhSFKWMHG0dGVB3AFsvbK0Qs4WpBGAWEhfCtkvbCh3ywZiSQtKCBbj163fjAG+J52DhELBzgid/A/fST8doMJp4YelRsvVau7+TfeluHiuKUrENCxpGnimPTec3Fb/ybaYSAMUP+ZD483xMWVn4Tnn+n4WpUVrlbzJoZ/5etcsUw8xt4Rw8n8RHQ5tRv4pq91eUe1Vjn8Y09m7MqrOryn22MJUAgB2Xbz3kgyE5meRFi3B/sD+ODRpoCzPitco/JwWeWA1VGt1UZkn8eTaeb3dF8Fi7QIa2Vu3+inKvGxZUMWYLq/QJQG/SM/PwrYd8SJo3D1N2Nr7Pm8/+s1Pgl6HaFcDoFVCj9U3blMSVVK3dv0EVN2YMUv39FaUyeKjuQzjoHFgTXr4DxFX6BFDUkA+GxESSflmM+4ABONSrpy1cOwniTsPIX6BW5zLt+1p//xy9kW9Vu7+iVBrXZgvbGLmxXGcLq9QJoLghHxLn/oTMzcV38mRtwfndcGYT9H4T6t9/0/olNWtnBAcvJPGfoc2pX8W1zOUpinL3uDZb2LaL28othkqdAIoa8sEQH0/y0qV4PPwwDnXraOP6b30H3P2h06Qy7zs5M485uyMZ0Lw6Q1pXqIFQFUW5A9pVbUdNt5qsCl9VbjFU2gRQ1JAPAIlz5yL1enwnmyv7k6sh5ij0eVvr9llG8/aeJyvPyLT7g8pclqIodx8hBEODyne2sEqbAIoa8kF/NY7kpcvwGDwY+1q1wJAL29/Xhnlo8ViZ952arWf+3gs82KyaGuJZUSqxwfXKd7awSpkAihryASBxzhykyYTvJPMTwcHzIOUi9H2v1KN75rfg7wuk5xqY0qd+mctSFOXu5efsR/eA7qw7t65cZgurlAmgqCEf9LGxpKxYgefQodgHBmrdPv/8L9TtBfXuK/O+M3INzNt7nvsaVaFpDY8yl6coyt1tWP1hJGQnlMtsYZUuARQ15ANAwpw5SMB34gRtwd6ZkJ0Efd8v9axe+S3ad5GULD0v3Kfa/hVFge4B3fFz8iuXZwIqVQKQUvLF4S9uOeSDPjqalJWr8HxkGHb+/trDXvtna+3+1W++UVxS2XlG5u6JpHuQL60CPctcnqIodz9bG1sG1RvE7ujdxGXF3dF9V6oEsOPyDo7GHS10yAeAhO9/QAC+E8xn/zv/A9Kk9fyxgiUHL5GYmcdUdfavKEo+Q4PKZ7awSpMADCZDkUM+5F2+TMqaNXiOGIFd9epw5QSELIEO48GzZpn3n6M38sOf5+hU15v2tb3LXJ6iKPeOWu61aFe1HavDV9/R2cIsSgBCiP5CiDNCiAghxBuFfD9OCBEvhAgxv541L++db1mIECJHCDHE/N18IcT5fN+1suaBFbQ6fPUth3wASJj9PcLGBp/x47UF297VJnbp/i+r7P/X4MvEpecytY86+1cU5WbDgoZxOf0yh68evmP7LDYBCCF0wLfAg0ATYJQQokkhqy6XUrYyv+YCSCl3XlsG9AGygC35tnk13zYhZTyWWypuyIe8ixdJ/e03vEaNxK5qFTi3EyK2QfdXwLnsZ+t5BhOzd52jbS0vOtcr/YQxiqLcu/rW6oubndsdfTLYkiuADkCElDJSSpkHLAMGl2Jfw4HNUsqsUmxbJgvCbj3kA0DCd7MRdnb4PPvsP0M+eARqzT9WsPpIFDGpObzQp36h+1cURXG0deShug+x7eK2OzZbmCUJwB+4nO9zlHlZQY8IIUKFECuFEIGFfD8SWFpg2Ufmbb4UQjgUtnMhxHghRLAQIjg+Pt6CcG8WnxVPv1r9Ch3yITfyPKnr1+M1ahS2fn5wYhVcCYU+/wY7x1LtLz+D0cR3u87RIsCDng38ylyeoij3rmFBw8g15t6x2cKsdRN4PVBbStkC2AosyP+lEKI60Bz4I9/i6UAjoD3gDbxeWMFSyjlSynZSynZ+fqWrQN/p/A6f9vi00O8SvvsO4eCAz7PP/DPkQ7Xm0PzRUu2roHXHYriUlMWU3ursX1GUojXxaUJj78Z37JkASxJANJD/jD7AvOw6KWWilDLX/HEu0LZAGSOANVJKfb5tYqUmF/gZranptinsxm/uuXOkbdyI95jHsfXxgYM/Quol6PsB2JQ9NxpNklk7I2hUzY2+TaqWuTxFUe59Q4OGcirpFGGJt3+2MEtquUNAkBCijhDCHq0p54bOquYz/GsGAacKlDGKAs0/17YR2mnxEOBEiSK3goRvv8XGyQnvp5+G7GTY/Zk23EO9m28Ul8am47FExmfyQp8gdfavKIpFHqqjzRa2Onz1bd9XsQlASmkApqA135wCVkgpTwoh3hdCDDKvNlUIcVIIcQyYCoy7tr0QojbaFcSfBYpeLIQ4DhwHfIEPy3gsJZJz5ixpm3/H64knsPXygj1fQE6qNuCbFZhMklk7IqhfxZUHm1WzSpmKotz7PBw8uL/W/WyK3ESOIee27uvmdpFCSCk3AZsKLHsn3/vpaG36hW17gUJuGksp+5QkUGtL+PZbbJyd8XlqHKRcggM/QMtRWvu/FWwJu8qZq+nMfKwVNjbq7F9RFMs9EvQIGyM3svXiVh6u9/Bt20+leRI4v5zTp0nfsgXvsWPReXrCjo+0L/q8ZZXypZR8syOc2j7ODGxRvfgNFEVR8mlXtR2BboG3vRmoUiaA+FmzsHFzw3vcWIg9BqHLtWkePQKsUv7OM3GcjEljcq/62Ooq5U+sKEoZCCEYFjSM4KvBXEy7eNv2U+lqp+wTJ8nYth3vcWPRubvD1nfByRO6vWSV8qWUfL09An9PJ4a2UXP9KopSOoPqDcJG2NzWLqGVLgEkzJqFjYcH3k8+CRHbIXIn9HhNSwJWsDcikZDLKUzqVQ87dfavKEopVXGuQg//Hvx27jcMJsNt2UelqqGyjx8nY9cufJ56Cp2Li3b271kT2j9jtX18vSOcau6OPNrOOs1JiqJUXkODht7W2cIqVQKI/+YbdJ6eeI0ZA8dXwNXjcN+7YFvoKBQldiAykYPnk5jQsy4OtmWfO1hRlMqte0B3fJ18WR1xe24GV5oEkHX0KJm79+D9zNPoHHSw/QOo3gqaDrPaPr7ZEYGvqwOjOpR9/gBFURQ7GzsG1RvEnqg9xGeVbiy0olj0HMC9IOGbWei8vfEePRoO/gBpUTB0tlWGfAA4cimZvyISePOhRjjaqbN/RVGsY1jQMFJzUzFKo9XLrhQJIOvwYTL//psqr72GjciF3f+DoH5Qp4fV9vHN9nC8nO14vGMtq5WpKIpSy70WM7rMuC1lV4omoPhZs9D5+uI1aiTs+R/kpcP91hnyAeB4VCo7z8TzTLc6uDhUipyqKMo9oFLUVlVffRX9lavY5FyFg3Og1WioWtikZqXzzY5w3B1tebJLbauVqSiKcrtVigTg2KQJjk2awKpnQeig15tWK/v0lTS2hF1l6n1BuDvaWa1cRVGU261SNAEBEHMUjv8KnSeDh/We0J21IwIXex1Pd61ttTIVRVHuhMqRAKTU5vl19oGu06xWbERcBhuPx/Jkl9p4OttbrVxFUZQ7oXIkgIjtcH439HwdHD2sVux3OyNwtNXxbLc6VitTURTlTqkcCWD3f8GrDrR9ympFXkzM5LdjMTzesSY+rtZ5klhRFOVOqhQ3gXlsMaReBlvrNdN8t/McOhvB+B51rVamoijKnWTRFYAQor8Q4owQIkII8UYh348TQsQLIULMr2fzfWfMt3xdvuV1hBAHzGUuN883fHu4+oF/G6sVF5WcxaojUYxsH0gVd0erlasoinInFZsAhBA64FvgQaAJMEoIUVgn+uVSylbm19x8y7PzLR+Ub/mnwJdSyvpAMmC9ITlvs+//PIcQMLFnvfIORVEUpdQsaQLqAERIKSMBhBDLgMFAWGl3KoQQQB9gtHnRAmAGMLu0Zd4pV9NyWHEoiuFtA6jh6VTe4SjKHaXX64mKiiIn5/ZOVq6UjqOjIwEBAdjZWfZMkiUJwB+4nO9zFNCxkPUeEUL0AM4CL0kpr23jKIQIBgzAJ1LKtYAPkCKlvDbLQRSFTBwPIIQYD4wHqFmz/EfZ/HF3JEYpmdSzfnmHoih3XFRUFG5ubtSuXRvtPE6pKKSUJCYmEhUVRZ06lvVMtFYvoPVAbSllC2Ar2hn9NbWklO3QzvZnCiFK1G4ipZwjpWwnpWzn5+dnpXBLx2A0sTYkmn5NqlLTx7lcY1GU8pCTk4OPj4+q/CsgIQQ+Pj4lujqzJAFEA4H5PgeYl10npUyUUuaaP84F2ub7Ltr8ZySwC2gNJAKeQohrVyA3lVkRHTifREJGHg+3rFHeoShKuVGVf8VV0r8bSxLAISDI3GvHHhgJrMu/ghCier6Pg4BT5uVeQggH83tfoCsQJqWUwE5guHmbscBvJYq8HGwIjcHZXkfvhlXKOxRFUZQyKzYBmNvppwB/oFXsK6SUJ4UQ7wshrvXqmSqEOCmEOAZMBcaZlzcGgs3Ld6LdA7h28/h14GUhRATaPYGfrHVQt4PeaGLziSvc37gqTvZqwhdFqShmzpxJVlaW1cqrXbs2CQkJpd5+/vz5TJkyxWrx3E4WPQgmpdwEbCqw7J1876cD0wvZ7m+g+S3KjETrYXRX2BuRQEqWnoEtqhe/sqIod8zMmTMZM2YMzs7lc1/OaDSi092dJ4WV40lgK9gQGoubgy09G5bvjWhFqSjeW3+SsJg0q5bZpIY77z7c9JbfZ2ZmMmLECKKiojAajTz66KPExMTQu3dvfH192blzJ5MmTeLQoUNkZ2czfPhw3ntPm/ypdu3ajB07lvXr16PX6/n1119p1KgRiYmJjBo1iujoaDp37ozWQq0ZMmQIly9fJicnh2nTpjF+/HgAXF1dmTBhAtu2bePbb78lPDycjz/+GE9PT1q2bImDw90xPEzlGAuojHINRv44eYW+TaviYHt3ZnpFuRf8/vvv1KhRg2PHjnHixAlefPFFatSowc6dO9m5cycAH330EcHBwYSGhvLnn38SGhp6fXtfX1+OHDnCpEmT+PzzzwF477336NatGydPnmTo0KFcunTp+vrz5s3j8OHDBAcH8/XXX5OYmAhoiahjx44cO3aMevXq8e6777J3717++usvwsJK/YjUHaeuACyw52wC6TkGHm6hev8oyjVFnanfLs2bN+df//oXr7/+OgMHDqR79+43rbNixQrmzJmDwWAgNjaWsLAwWrRoAcCwYcMAaNu2LatXrwZg9+7d198PGDAALy+v62V9/fXXrFmzBoDLly8THh6Oj48POp2ORx55BIADBw7Qq1cvrnVTf+yxxzh79uxt+gWsSyUAC2wIjcHDyY6u9X3LOxRFqdQaNGjAkSNH2LRpE2+//Tb33XffDd+fP3+ezz//nEOHDuHl5cW4ceNu6Bd/rWlGp9NhMBgoyq5du9i2bRv79u3D2dmZXr16XS/L0dHxrm33z081ARUjR29ka9hV+jethr2t+rkUpTzFxMTg7OzMmDFjePXVVzly5Ahubm6kp6cDkJaWhouLCx4eHly9epXNmzcXW2aPHj1YsmQJAJs3byY5ORmA1NRUvLy8cHZ25vTp0+zfv7/Q7Tt27Miff/5JYmLi9XsLdwt1BVCMXWfiyMwzMrCl6v2jKOXt+PHjvPrqq9jY2GBnZ8fs2bPZt28f/fv3v34voHXr1jRq1IjAwEC6du1abJnvvvsuo0aNomnTpnTp0uX6kDP9+/fn+++/p3HjxjRs2JBOnToVun316tWZMWMGnTt3xtPTk1atWlnzkG8rkf+Od0XXrl07GRwcfEf3+fySI+w/l8iBN+/DVqeuAJTK7dSpUzRu3Li8w1CKUNjfkRDisHlInhuoGq0IWXkGdpyKo3+zaqryVxTlnqNqtSJsPxVHtt7IQNX7R1GUe5BKAEXYEBqDn5sDHep4l3coiqIoVqcSwC2k5+jZeSaeAc2ro7NRox8qinLvUQngFradukqewaTG/lEU5Z6lEsAtbDgWS3UPR9rU9Cp+ZUVRlLuQSgCFSM3Ssztca/6xUc0/ilKhzZgx4/q4PqdPn6ZVq1a0bt2ac+fOlXNkFZ9KAIX4I+wKeqNkoJr5S1HuKmvXrmX48OEcPXqUevVKNPtspaSeBC7EhtBYAr2daBngUd6hKErFtfkNuHLcumVWaw4PflLkKgsXLuTzzz9HCEGLFi2uV/SbNm1i5syZ6HQ6tm/ffn10UOXWVAIoICkzj70RCTzXva6a+1RRKpiTJ0/y4Ycf8vfff+Pr60tSUhJff/01AA899BATJ07E1dWVV155pZwjvTtYlACEEP2BrwAdMFdK+UmB78cBn/HPxO6zpJRzhRCtgNmAO2AEPpJSLjdvMx/oCaSatxknpQwpw7FYxe8nrmA0SdX7R1GKU8yZ+u2wY8cOHn30UXx9tZF5vb3VMzplUWwCEELogG+BvkAUcEgIsS7f3L7XLJdSFpwIMwt4UkoZLoSoARwWQvwhpUwxf/+qlHJl2Q7BujaExlDH14WmNdzLOxRFUZTbypKbwB2ACCllpJQyD1gGDLakcCnlWSlluPl9DBAHVNg5FePTc9kfmcjAFtVV84+iVEB9+vTh119/vT4zV1JSUjlHdHezJAH4A5fzfY4yLyvoESFEqBBipRAisOCXQogOgD2Qv2/WR+ZtvhRCFDqJphBivBAiWAgRHB8fb0G4pbf5RCwmiRr7R1EqqKZNm/LWW2/Rs2dPWrZsycsvv1zeId3VrHUTeD2wVEqZK4SYACwA+lz7UghRHVgEjJVSmsyLpwNX0JLCHOB14P2CBUsp55i/p127drd17OoNx2IJquJKw2put3M3iqKUwdixYxk7dmyh382YMePOBnOXs+QKIBrIf0YfwD83ewGQUiZKKXPNH+cCba99J4RwBzYCb0kp9+fbJlZqcoGf0Zqays2V1BwOXUxSZ/+KolQaliSAQ0CQEKKOEMIeGAmsy7+C+Qz/mkHAKfNye2ANsLDgzd5r2witsX0IcKKUx2AVG4/HIiVq5i9FUSqNYpuApJQGIcQU4A+0bqDzpJQnhRDvA8FSynXAVCHEIMAAJAHjzJuPAHoAPuauovBPd8/FQgg/QAAhwERrHVRpbAiNoXF1d+r5uZZnGIqiKHeMRfcApJSbgE0Flr2T7/10tDb9gtv9AvxyizL7FLa8PEQlZ3H0UgqvPtCwvENRFEW5Y9RYQMDG0FgAHlbt/4qiVCIqAaCN/dMiwIOaPs7lHYqiKModU+kTwIWETI5Hp6qhHxTlLpCSksJ33313W8oODg5m6tSpt6Xsa1xd/7nH+Oqrr9K0aVNeffXV27rPolT6weA2Hteafwao5h9FqfCuJYDJkydbvex27drRrl07q5d7K3PmzCEpKQmdTnfH9llQpU8A64/F0KamJ/6eTuUdiqLcVT49+Cmnk05btcxG3o14vcPrt/z+jTfe4Ny5c7Rq1Yq+ffvy7rvvMnjwYJKTk9Hr9Xz44YcMHqyNVPPBBx/wyy+/4OfnR2BgIG3btuWVV17h0KFDPPPMM9jY2NC3b182b97MiRMn2LVrF59//jkbNmxgxowZXLp0icjISC5dusSLL754/ergVuXmd/XqVSZOnEhkZCQAs2fPpkuXLte/HzRoEBkZGbRt25bp06fz2GOPWfV3tFSlTgARcRmcvpLOOwOblHcoiqJY4JNPPuHEiROEhIQAYDAYWLNmDe7u7iQkJNCpUycGDRpEcHAwq1at4tixY+j1etq0aUPbttrzqU899RQ//vgjnTt35o033rjlvk6fPs3OnTtJT0+nYcOGTJo0iZCQkFuWm9/UqVPp2bMna9aswWg0kpGRccP369atw9XV9fpxlJdKnQA2hMYgBAxQ7f+KUmJFnanfKVJK3nzzTXbv3o2NjQ3R0dFcvXqVvXv3MnjwYBwdHXF0dOThhx8GtCak9PR0OnfuDMDo0aPZsGFDoWUPGDAABwcHHBwcqFKlSpHlFrRjxw4WLlwIgE6nw8OjYk4uVWkTgJSSDaGxtK/tTVV3x/IOR1GUUli8eDHx8fEcPnwYOzs7ateuTU5OjlXKdnD4Z3xKnU6HwWCwSrkVSaXtBXTmajoRcRk8rM7+FeWu4ebmRnp6+vXPqampVKlSBTs7O3bu3MnFixcB6Nq1K+vXrycnJ4eMjIzrZ/menp64ublx4MABAJYtW1ai/d+q3ILuu+8+Zs+eDYDRaCQ1NbXQ9cpbpU0AG47FYiOgfzOVABTlbuHj40PXrl1p1qwZr776Ko8//jjBwcE0b96chQsX0qhRIwDat2/PoEGDaNGiBQ8++CDNmze/3gzz008/8dxzz9GqVSsyMzNL1DxTVLn5ffXVV+zcuZPmzZvTtm1bwsIKzp9VMQgpb+sIy1bVrl07GRwcXOZypJT0/nwX/l5OLH62kxUiU5TK4dSpUzRu3Li8w7BIRkYGrq6uZGVl0aNHD+bMmUObNm2uLwftpnJsbCxfffVVmcutKAr7OxJCHJZS3tTHtVLeAzgZk8aFxCwm9KxX3qEoinKbjB8/nrCwMHJychg7duz1Snrjxo18/PHHGAwGatWqxfz5861S7t2oUiaA9aEx2NoI+jetVt6hKIpymyxZsqTQ5Y899liZ+t3fqty7UaW7ByClZGNoLF3r++LlYl/e4SiKopSbSpcAQi6nEJWcrcb+URSl0qt0CWBDaCz2Ohv6qeYfRVEquUqVAEwmrfmnRwNfPJzsyjscRVGUcmVRAhBC9BdCnBFCRAghbho8QwgxTggRL4QIMb+ezffdWCFEuPk1Nt/ytkKI4+YyvzbPDXxbHb6UzJW0HDXxu6IoFpkxYwaff/65VdcfN24cK1euLHKd/C5cuECzZs0sXr8kik0AQggd8C3wINAEGCWEKGz0tOVSylbm11zztt7Au0BHoAPwrhDCy7z+bOA5IMj86l/WgynOhmMxONjacH+Tqrd7V4qiKBWeJd1AOwARUspIACHEMmAwYMmjbQ8AW6WUSeZttwL9hRC7AHcp5X7z8oXAEGBzSQ/AUkaTZNOJK/RuWAVXh0rZ+1VRrOrKf/5D7inrDgft0LgR1d5885bfX7hwgf79+9O2bVuOHDlC06ZNWbhwIc7OzrzxxhusW7cOW1tb+vXrx+eff058fDwTJ07k0qVLAMycOZOuXbsyY8YMXF1drw/j3KxZMzZs2EDt2rX56KOPWLBgAVWqVLk+3DNASEgIEydOJCsri3r16jFv3jy8vLxuGeuPP/7InDlzyMvLo379+ixatAhnZ23WwW3btvHJJ5+QlpbGF198wcCBAzEajbzxxhvs2rWL3Nxcnn/+eSZMmHBDmSdPnuSpp54iLy8Pk8nEqlWrCAoKKvXvbUkTkD9wOd/nKPOygh4RQoQKIVYKIQKL2dbf/L64MhFCjBdCBAshguPj4y0It3AHzicSn57LwJaq94+i3M3OnDnD5MmTOXXqFO7u7nz33XckJiayZs0aTp48SWhoKG+//TYA06ZN46WXXuLQoUOsWrWKZ599tsiyDx8+zLJlywgJCWHTpk0cOnTo+ndPPvkkn376KaGhoTRv3pz33nuvyLKGDRvGoUOHOHbsGI0bN+ann366/t2FCxc4ePAgGzduZOLEieTk5PDTTz/h4eHBoUOHOHToED/++CPnz5+/oczvv/+eadOmERISQnBwMAEBASX9+W5grVPh9cBSKWWuEGICsADoY42CpZRzgDmgDQVR2nI2hMbiZKejT6Mq1ghLUSq9os7Ub6fAwEC6du0KwJgxY/j666958cUXcXR05JlnnmHgwIEMHDgQ0M6084/Dk5aWdtPY/Pnt2bOHoUOHXj9THzRoEKANOpeSkkLPnj0BGDt2LI8++miRcZ44cYK3336blJQUMjIyeOCBB65/N2LECGxsbAgKCqJu3bqcPn2aLVu2EBoaev3+QGpqKuHh4TRo0OD6dp07d+ajjz4iKiqKYcOGlensHyy7AogGAvN9DjAvu05KmSilzDV/nAu0LWbbaPP7W5ZpTQajid9PXOG+xlVwtlfNP4pyNyvYX0QIga2tLQcPHmT48OFs2LCB/v21W4omk4n9+/cTEhJCSEgI0dHRuLq6Ymtri8lkul6GtYaQzm/cuHHMmjWL48eP8+67796wj8KOQUrJN998cz3W8+fP069fvxvWGz16NOvWrcPJyYmHHnqIHTt2lClGSxLAISBICFFHCGEPjATWFQg+f7vKIOCU+f0fQD8hhJf55m8/4A8pZSyQJoToZO798yTwW5mOpAh/n0skKTNP9f5RlHvApUuX2LdvH6ANy9CtWzcyMjJITU3loYce4ssvv+TYsWMA9OvXj2+++eb6ttdm4KpduzZHjhwB4MiRI9ebWnr06MHatWvJzs4mPT2d9evXA+Dh4YGXlxd79uwBYNGiRdevBm4lPT2d6tWro9frWbx48Q3f/frrr5hMJs6dO0dkZCQNGzbkgQceYPbs2ej1egDOnj1LZmbmDdtFRkZSt25dpk6dyuDBgwkNDS3x75dfsafDUkqDEGIKWmWuA+ZJKU8KId4HgqWU64CpQohBgAFIAsaZt00SQnyAlkQA3r92QxiYDMwHnNBu/t62G8AbQmNwdbClV0O/27ULRVHukIYNG/Ltt9/y9NNP06RJEyZNmkRqaiqDBw8mJycHKSVffPEFAF9//TXPP/88LVq0wGAw0KNHD77//nseeeQRFi5cSNOmTenYseP1ZpY2bdrw2GOP0bJlS6pUqUL79u2v73fBggXXbwLXrVuXn3/+ucg4P/jgAzp27Iifnx8dO3a8YR6DmjVr0qFDB9LS0vj+++9xdHTk2Wef5cKFC7Rp0wYpJX5+fqxdu/aGMlesWMGiRYuws7OjWrVqvFnGZrhKMRz0d7siSM8x8Hr/RrchKkWpPMp7OOgLFy4wcOBATpw4UW4xVHRqOOgCJveqX94hKIqiVDiVaigIRVHubrVr11Zn/1akEoCiKCVyNzUbVzYl/btRCUBRFIs5OjqSmJiokkAFJKUkMTERR0dHi7epFPcAFEWxjoCAAKKioijLU/nK7ePo6Fiip4NVAlAUxWJ2dnbUqVOnvMNQrEQ1ASmKolRSKgEoiqJUUioBKIqiVFJ31ZPAQoh44GIpN/cFEqwYjrWouEpGxVUyKq6SuVfjqiWlvGksnLsqAZSFECK4sEehy5uKq2RUXCWj4iqZyhaXagJSFEWppFQCUBRFqaQqUwKYU94B3IKKq2RUXCWj4iqZShVXpbkHoCiKotyoMl0BKIqiKPmoBKAoilJJ3XMJQAjRXwhxRggRIYR4o5DvewghjgghDEKI4RUorpeFEGFCiFAhxHYhRK0KEtdEIcRxIUSIEOIvIUSTihBXvvUeEUJIIcQd6bpnwe81TggRb/69QoQQz1aEuMzrjDD/GzsphFhSEeISQnyZ77c6K4RIqSBx1RRC7BRCHDX/n3yogsRVy1w/hAohdgkhLB/5rTBSynvmhTZn8TmgLmAPHAOaFFinNtACWAgMr0Bx9Qacze8nAcsrSFzu+d4PAn6vCHGZ13MDdgP7gXYVIS60+bBn3Yl/VyWMKwg4CniZP1epCHEVWP8FtDnHyz0utJuuk8zvmwAXKkhcvwJjze/7AIvKss977QqgAxAhpYyUUuYBy4DB+VeQUl6QUoYCpgoW104pZZb5436gbJndenGl5fvoAtyJXgPFxmX2AfApkHMHYipJXHeaJXE9B3wrpUwGkFLGVZC48hsFLK0gcUnA3fzeA4ipIHE1AXaY3+8s5PsSudcSgD9wOd/nKPOy8lbSuJ4BNt/WiDQWxSWEeF4IcQ74LzC1IsQlhGgDBEopN96BeCyOy+wR8yX6SiFEYAWJqwHQQAixVwixXwjRv4LEBWhNG0Ad/qncyjuuGcAYIUQUsAnt6qQixHUMGGZ+PxRwE0L4lHaH91oCuOsJIcYA7YDPyjuWa6SU30op6wGvA2+XdzxCCBvgC+Bf5R1LIdYDtaWULYCtwIJyjucaW7RmoF5oZ9o/CiE8yzOgAkYCK6WUxvIOxGwUMF9KGQA8BCwy/7srb68APYUQR4GeQDRQ6t+sIhyQNUUD+c+4AszLyptFcQkh7gfeAgZJKXMrSlz5LAOG3M6AzIqLyw1oBuwSQlwAOgHr7sCN4GJ/LyllYr6/u7lA29sck0VxoZ1NrpNS6qWU54GzaAmhvOO6ZiR3pvkHLIvrGWAFgJRyH+CINiBbucYlpYyRUg6TUrZGqyuQUqaUeo+3+8bGnXyhneVEol1KXruJ0vQW687nzt0ELjYuoDXaDaCgivR75Y8HeBgIrghxFVh/F3fmJrAlv1f1fO+HAvsrSFz9gQXm975oTQ0+5R2Xeb1GwAXMD6ZWkN9rMzDO/L4x2j2A2xqfhXH5Ajbm9x8B75dpn3fiB7+TL7TLtbPmyvQt87L30c6qAdqjnQ1lAonAyQoS1zbgKhBifq2rIHF9BZw0x7SzqIr4TsZVYN07kgAs/L0+Nv9ex8y/V6MKEpdAazYLA44DIytCXObPM4BP7kQ8Jfi9mgB7zX+PIUC/ChLXcCDcvM5cwKEs+1NDQSiKolRS99o9AEVRFMVCKgEoiqJUUioBKIqiVFIqASiKolRSKgEoiqJUUioBKIqiVFIqASiKolRS/wfLnvsJ6+345AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]\n",
    "plt.plot(xx, scores_standard)\n",
    "plt.plot(xx, scores_standard_clf)\n",
    "plt.plot(xx, scores_tagging_clf)\n",
    "plt.plot(xx, scores_pseudolabel)\n",
    "plt.legend(['standard', 'clf', 'tagging clf', 'pseudo labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.682, 0.7, 0.668, 0.678\n",
      "0.682, 0.7, 0.674, 0.682\n",
      "0.686, 0.8, 0.664, 0.670\n",
      "0.683, 0.6, 0.681, 0.683\n"
     ]
    }
   ],
   "source": [
    "ss = [scores_standard, scores_pseudolabel, scores_standard_clf, scores_tagging_clf]\n",
    "for sss in ss:\n",
    "    print(f'{np.max(sss):.3f}, {xx[np.argmax(sss)]}, {sss[8]:.3f}, {sss[10]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standard deviation of score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6682946286098522\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "preds = []\n",
    "for text, pr in zip(final_test.text, pred.predictions):\n",
    "    proba = np.exp(pr[pr[:, 0]!=-100])\n",
    "    proba /= proba.sum(axis=1, keepdims=True)\n",
    "    labels = (proba[:, 1] >= threshold).astype(int).tolist()\n",
    "    preds.append(labels2spans(text, labels, tokenizer))\n",
    "ff = [f1(p, y) for p, y in zip(preds, final_test.spans)]\n",
    "score = np.mean(ff)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6506454922132865 0.685943765006418\n"
     ]
    }
   ],
   "source": [
    "se = np.std(ff) / np.sqrt(len(ff)) * 1.96\n",
    "print(score - se, score + se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009004661426819228"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(ff) / np.sqrt(len(ff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3k",
   "language": "python",
   "name": "p3k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
