{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal: fine-tune gpt2 with prefixes to discriminate between toxic and nontoxic tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForTokenClassification\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'\n",
    "model = AutoModelForCausalLM.from_pretrained('models/gpt2-cond')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358984, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>352703</th>\n",
       "      <td>Tsk,tsk. This is what happens when you:\\n 1) D...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143960</th>\n",
       "      <td>Ignorant and greedy farmers grow food. Stop ea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185136</th>\n",
       "      <td>If you prefer, I can use sarcasm next time? Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  label\n",
       "352703  Tsk,tsk. This is what happens when you:\\n 1) D...      0\n",
       "143960  Ignorant and greedy farmers grow food. Stop ea...      1\n",
       "185136  If you prefer, I can use sarcasm next time? Th...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('../data/train/train.1.tsv', sep='\\t')\n",
    "df0 = pd.read_csv('../data/train/train_small.0.tsv', sep='\\t')\n",
    "df01 = pd.concat([df1, df0], ignore_index=True)\n",
    "df01.label = df01.label.astype(int)\n",
    "print(df01.shape)\n",
    "df01.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_prefix = lambda x: '{} {}'.format(' clean' if x.label == 0 else ' toxic', x.comment_text)\n",
    "\n",
    "df01['text'] = df01.apply(add_prefix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df01, test_size=0.01, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpansDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_train_dataset = SpansDataset(\n",
    "    tokenizer(df_train.sample(frac=1.0).text.tolist(), truncation=True, max_length=MAX_LENGTH), \n",
    "    labels=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_test_dataset = SpansDataset(\n",
    "    tokenizer(df_test.text.tolist(), truncation=True, max_length=MAX_LENGTH), \n",
    "    labels=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:4')\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.file_utils import cached_property\n",
    "from typing import Tuple\n",
    "\n",
    "class TrAr(TrainingArguments):\n",
    "    @cached_property\n",
    "    def _setup_devices(self) -> \"torch.device\":\n",
    "        return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrAr(\n",
    "    output_dir='./models/gpt2-cond',   # output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,             # total # of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    #gradient_accumulation_steps=16,\n",
    "    per_device_eval_batch_size=16,  # batch size for evaluation\n",
    "    warmup_steps=5000,              # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=1e-9,              # strength of weight decay\n",
    "    learning_rate=5e-5,\n",
    "    logging_dir='./logs',           # directory for storing logs\n",
    "    logging_steps=50,\n",
    "    eval_steps=300,\n",
    "    evaluation_strategy='steps',\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                             # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                      # training arguments, defined above\n",
    "    train_dataset=clf_train_dataset,         # training dataset\n",
    "    eval_dataset=clf_test_dataset,           # evaluation dataset\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "with torch.cuda.device('cuda:4'):\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='33242' max='66639' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33242/66639 2:29:14 < 2:29:56, 3.71 it/s, Epoch 1.50/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.685000</td>\n",
       "      <td>3.731149</td>\n",
       "      <td>14.226800</td>\n",
       "      <td>252.341000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.645000</td>\n",
       "      <td>3.735057</td>\n",
       "      <td>14.223100</td>\n",
       "      <td>252.406000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.571000</td>\n",
       "      <td>3.740547</td>\n",
       "      <td>14.209100</td>\n",
       "      <td>252.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.573300</td>\n",
       "      <td>3.746353</td>\n",
       "      <td>14.192200</td>\n",
       "      <td>252.956000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.505500</td>\n",
       "      <td>3.755964</td>\n",
       "      <td>14.212500</td>\n",
       "      <td>252.595000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.454200</td>\n",
       "      <td>3.767750</td>\n",
       "      <td>14.202100</td>\n",
       "      <td>252.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>3.346200</td>\n",
       "      <td>3.778579</td>\n",
       "      <td>14.192800</td>\n",
       "      <td>252.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.384200</td>\n",
       "      <td>3.783519</td>\n",
       "      <td>14.201100</td>\n",
       "      <td>252.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>3.378200</td>\n",
       "      <td>3.787323</td>\n",
       "      <td>14.224400</td>\n",
       "      <td>252.383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.393400</td>\n",
       "      <td>3.786280</td>\n",
       "      <td>14.216200</td>\n",
       "      <td>252.529000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>3.356700</td>\n",
       "      <td>3.790648</td>\n",
       "      <td>14.217900</td>\n",
       "      <td>252.499000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>3.394100</td>\n",
       "      <td>3.793473</td>\n",
       "      <td>14.233600</td>\n",
       "      <td>252.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>3.375600</td>\n",
       "      <td>3.793752</td>\n",
       "      <td>14.215800</td>\n",
       "      <td>252.537000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>3.440000</td>\n",
       "      <td>3.798742</td>\n",
       "      <td>14.238900</td>\n",
       "      <td>252.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.397500</td>\n",
       "      <td>3.804601</td>\n",
       "      <td>14.274200</td>\n",
       "      <td>251.503000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>3.423700</td>\n",
       "      <td>3.807409</td>\n",
       "      <td>14.289900</td>\n",
       "      <td>251.227000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>3.429000</td>\n",
       "      <td>3.811414</td>\n",
       "      <td>14.207900</td>\n",
       "      <td>252.676000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>3.379000</td>\n",
       "      <td>3.797910</td>\n",
       "      <td>14.210300</td>\n",
       "      <td>252.634000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>3.459600</td>\n",
       "      <td>3.810544</td>\n",
       "      <td>14.219100</td>\n",
       "      <td>252.477000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.426800</td>\n",
       "      <td>3.807022</td>\n",
       "      <td>14.247300</td>\n",
       "      <td>251.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>3.417400</td>\n",
       "      <td>3.809171</td>\n",
       "      <td>14.223800</td>\n",
       "      <td>252.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>3.483400</td>\n",
       "      <td>3.801391</td>\n",
       "      <td>14.216100</td>\n",
       "      <td>252.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>3.470500</td>\n",
       "      <td>3.802896</td>\n",
       "      <td>14.213800</td>\n",
       "      <td>252.571000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>3.446900</td>\n",
       "      <td>3.804061</td>\n",
       "      <td>14.203800</td>\n",
       "      <td>252.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.457900</td>\n",
       "      <td>3.802408</td>\n",
       "      <td>14.255300</td>\n",
       "      <td>251.836000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>3.515100</td>\n",
       "      <td>3.801797</td>\n",
       "      <td>14.243600</td>\n",
       "      <td>252.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>3.497500</td>\n",
       "      <td>3.802843</td>\n",
       "      <td>14.216100</td>\n",
       "      <td>252.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>3.482500</td>\n",
       "      <td>3.801349</td>\n",
       "      <td>14.222300</td>\n",
       "      <td>252.421000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>3.478100</td>\n",
       "      <td>3.801023</td>\n",
       "      <td>14.213300</td>\n",
       "      <td>252.579000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.483700</td>\n",
       "      <td>3.802325</td>\n",
       "      <td>14.205400</td>\n",
       "      <td>252.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>3.463900</td>\n",
       "      <td>3.804508</td>\n",
       "      <td>14.207500</td>\n",
       "      <td>252.684000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>3.479100</td>\n",
       "      <td>3.796668</td>\n",
       "      <td>14.214400</td>\n",
       "      <td>252.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>3.509200</td>\n",
       "      <td>3.798038</td>\n",
       "      <td>14.220700</td>\n",
       "      <td>252.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>3.520800</td>\n",
       "      <td>3.793558</td>\n",
       "      <td>14.211500</td>\n",
       "      <td>252.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.527300</td>\n",
       "      <td>3.792806</td>\n",
       "      <td>14.237600</td>\n",
       "      <td>252.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>3.541100</td>\n",
       "      <td>3.793004</td>\n",
       "      <td>14.206600</td>\n",
       "      <td>252.699000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>3.527100</td>\n",
       "      <td>3.790807</td>\n",
       "      <td>14.226700</td>\n",
       "      <td>252.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>3.488400</td>\n",
       "      <td>3.791153</td>\n",
       "      <td>14.225600</td>\n",
       "      <td>252.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>3.586000</td>\n",
       "      <td>3.783742</td>\n",
       "      <td>14.213600</td>\n",
       "      <td>252.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.554100</td>\n",
       "      <td>3.785349</td>\n",
       "      <td>14.223000</td>\n",
       "      <td>252.409000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>3.586200</td>\n",
       "      <td>3.785024</td>\n",
       "      <td>14.212700</td>\n",
       "      <td>252.591000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>3.556900</td>\n",
       "      <td>3.784376</td>\n",
       "      <td>14.206300</td>\n",
       "      <td>252.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>3.574600</td>\n",
       "      <td>3.785825</td>\n",
       "      <td>14.244600</td>\n",
       "      <td>252.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>3.586400</td>\n",
       "      <td>3.778354</td>\n",
       "      <td>14.215700</td>\n",
       "      <td>252.537000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.559900</td>\n",
       "      <td>3.779718</td>\n",
       "      <td>14.344900</td>\n",
       "      <td>250.263000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>3.574000</td>\n",
       "      <td>3.777621</td>\n",
       "      <td>14.205200</td>\n",
       "      <td>252.724000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>3.591500</td>\n",
       "      <td>3.774124</td>\n",
       "      <td>14.219600</td>\n",
       "      <td>252.469000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>3.633700</td>\n",
       "      <td>3.769228</td>\n",
       "      <td>14.221400</td>\n",
       "      <td>252.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>3.595900</td>\n",
       "      <td>3.772296</td>\n",
       "      <td>14.232700</td>\n",
       "      <td>252.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.610700</td>\n",
       "      <td>3.771486</td>\n",
       "      <td>14.217900</td>\n",
       "      <td>252.499000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>3.601300</td>\n",
       "      <td>3.772125</td>\n",
       "      <td>14.231800</td>\n",
       "      <td>252.253000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>3.614400</td>\n",
       "      <td>3.766258</td>\n",
       "      <td>14.221400</td>\n",
       "      <td>252.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>3.641900</td>\n",
       "      <td>3.765703</td>\n",
       "      <td>14.230100</td>\n",
       "      <td>252.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>3.673200</td>\n",
       "      <td>3.765109</td>\n",
       "      <td>14.217100</td>\n",
       "      <td>252.513000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.699000</td>\n",
       "      <td>3.762740</td>\n",
       "      <td>14.244100</td>\n",
       "      <td>252.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>3.694700</td>\n",
       "      <td>3.761775</td>\n",
       "      <td>14.507800</td>\n",
       "      <td>247.452000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>3.631200</td>\n",
       "      <td>3.760198</td>\n",
       "      <td>14.204300</td>\n",
       "      <td>252.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>3.629700</td>\n",
       "      <td>3.760972</td>\n",
       "      <td>14.366700</td>\n",
       "      <td>249.883000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>3.688200</td>\n",
       "      <td>3.758370</td>\n",
       "      <td>14.205900</td>\n",
       "      <td>252.711000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.649600</td>\n",
       "      <td>3.756207</td>\n",
       "      <td>14.248100</td>\n",
       "      <td>251.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>3.658200</td>\n",
       "      <td>3.754217</td>\n",
       "      <td>14.211100</td>\n",
       "      <td>252.619000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>3.646300</td>\n",
       "      <td>3.752496</td>\n",
       "      <td>14.204600</td>\n",
       "      <td>252.735000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>3.669700</td>\n",
       "      <td>3.749202</td>\n",
       "      <td>14.234600</td>\n",
       "      <td>252.202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>3.665500</td>\n",
       "      <td>3.750119</td>\n",
       "      <td>14.228200</td>\n",
       "      <td>252.317000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.697800</td>\n",
       "      <td>3.747457</td>\n",
       "      <td>14.224500</td>\n",
       "      <td>252.382000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>3.681000</td>\n",
       "      <td>3.748383</td>\n",
       "      <td>14.256000</td>\n",
       "      <td>251.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>3.719800</td>\n",
       "      <td>3.748007</td>\n",
       "      <td>14.264800</td>\n",
       "      <td>251.668000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>3.697300</td>\n",
       "      <td>3.743630</td>\n",
       "      <td>14.234100</td>\n",
       "      <td>252.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>3.685900</td>\n",
       "      <td>3.745297</td>\n",
       "      <td>14.239500</td>\n",
       "      <td>252.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.726600</td>\n",
       "      <td>3.741676</td>\n",
       "      <td>14.250100</td>\n",
       "      <td>251.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>3.757300</td>\n",
       "      <td>3.740124</td>\n",
       "      <td>14.243300</td>\n",
       "      <td>252.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>3.722100</td>\n",
       "      <td>3.741097</td>\n",
       "      <td>14.261300</td>\n",
       "      <td>251.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>3.779400</td>\n",
       "      <td>3.740200</td>\n",
       "      <td>14.230300</td>\n",
       "      <td>252.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>3.751700</td>\n",
       "      <td>3.736632</td>\n",
       "      <td>14.233500</td>\n",
       "      <td>252.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.568300</td>\n",
       "      <td>3.753378</td>\n",
       "      <td>14.243200</td>\n",
       "      <td>252.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>3.582100</td>\n",
       "      <td>3.751866</td>\n",
       "      <td>14.245000</td>\n",
       "      <td>252.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23100</td>\n",
       "      <td>3.629400</td>\n",
       "      <td>3.754475</td>\n",
       "      <td>14.226800</td>\n",
       "      <td>252.341000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>3.588600</td>\n",
       "      <td>3.753534</td>\n",
       "      <td>14.217600</td>\n",
       "      <td>252.504000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23700</td>\n",
       "      <td>3.538800</td>\n",
       "      <td>3.751455</td>\n",
       "      <td>14.234700</td>\n",
       "      <td>252.201000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.600800</td>\n",
       "      <td>3.750719</td>\n",
       "      <td>14.232100</td>\n",
       "      <td>252.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24300</td>\n",
       "      <td>3.514000</td>\n",
       "      <td>3.751391</td>\n",
       "      <td>14.206000</td>\n",
       "      <td>252.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>3.579100</td>\n",
       "      <td>3.752147</td>\n",
       "      <td>14.231400</td>\n",
       "      <td>252.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24900</td>\n",
       "      <td>3.636000</td>\n",
       "      <td>3.749698</td>\n",
       "      <td>14.356600</td>\n",
       "      <td>250.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>3.616900</td>\n",
       "      <td>3.749108</td>\n",
       "      <td>14.221900</td>\n",
       "      <td>252.427000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.541400</td>\n",
       "      <td>3.752404</td>\n",
       "      <td>14.211200</td>\n",
       "      <td>252.618000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>3.556700</td>\n",
       "      <td>3.750909</td>\n",
       "      <td>14.216700</td>\n",
       "      <td>252.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26100</td>\n",
       "      <td>3.570400</td>\n",
       "      <td>3.747804</td>\n",
       "      <td>14.209300</td>\n",
       "      <td>252.652000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>3.572500</td>\n",
       "      <td>3.748400</td>\n",
       "      <td>14.208900</td>\n",
       "      <td>252.659000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26700</td>\n",
       "      <td>3.559600</td>\n",
       "      <td>3.751654</td>\n",
       "      <td>14.193800</td>\n",
       "      <td>252.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.579400</td>\n",
       "      <td>3.750415</td>\n",
       "      <td>14.277200</td>\n",
       "      <td>251.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27300</td>\n",
       "      <td>3.605900</td>\n",
       "      <td>3.747588</td>\n",
       "      <td>14.243300</td>\n",
       "      <td>252.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27600</td>\n",
       "      <td>3.573900</td>\n",
       "      <td>3.750953</td>\n",
       "      <td>14.238500</td>\n",
       "      <td>252.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27900</td>\n",
       "      <td>3.553700</td>\n",
       "      <td>3.748304</td>\n",
       "      <td>14.256600</td>\n",
       "      <td>251.813000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28200</td>\n",
       "      <td>3.609100</td>\n",
       "      <td>3.747387</td>\n",
       "      <td>14.232000</td>\n",
       "      <td>252.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>3.593200</td>\n",
       "      <td>3.747140</td>\n",
       "      <td>14.222500</td>\n",
       "      <td>252.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>3.603100</td>\n",
       "      <td>3.746685</td>\n",
       "      <td>14.218400</td>\n",
       "      <td>252.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29100</td>\n",
       "      <td>3.587200</td>\n",
       "      <td>3.747930</td>\n",
       "      <td>14.223900</td>\n",
       "      <td>252.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29400</td>\n",
       "      <td>3.558200</td>\n",
       "      <td>3.746035</td>\n",
       "      <td>14.251100</td>\n",
       "      <td>251.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29700</td>\n",
       "      <td>3.608300</td>\n",
       "      <td>3.744758</td>\n",
       "      <td>14.230300</td>\n",
       "      <td>252.278000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.632700</td>\n",
       "      <td>3.745494</td>\n",
       "      <td>14.206300</td>\n",
       "      <td>252.704000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30300</td>\n",
       "      <td>3.574100</td>\n",
       "      <td>3.744495</td>\n",
       "      <td>14.237700</td>\n",
       "      <td>252.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30600</td>\n",
       "      <td>3.602400</td>\n",
       "      <td>3.747117</td>\n",
       "      <td>14.254200</td>\n",
       "      <td>251.855000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30900</td>\n",
       "      <td>3.590500</td>\n",
       "      <td>3.744886</td>\n",
       "      <td>14.231800</td>\n",
       "      <td>252.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>3.623400</td>\n",
       "      <td>3.744125</td>\n",
       "      <td>14.224900</td>\n",
       "      <td>252.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>3.586500</td>\n",
       "      <td>3.743282</td>\n",
       "      <td>14.290000</td>\n",
       "      <td>251.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31800</td>\n",
       "      <td>3.593600</td>\n",
       "      <td>3.743644</td>\n",
       "      <td>14.239500</td>\n",
       "      <td>252.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32100</td>\n",
       "      <td>3.594600</td>\n",
       "      <td>3.743168</td>\n",
       "      <td>14.166700</td>\n",
       "      <td>253.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32400</td>\n",
       "      <td>3.604000</td>\n",
       "      <td>3.742045</td>\n",
       "      <td>14.221100</td>\n",
       "      <td>252.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32700</td>\n",
       "      <td>3.586600</td>\n",
       "      <td>3.743222</td>\n",
       "      <td>14.215400</td>\n",
       "      <td>252.542000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.608400</td>\n",
       "      <td>3.740840</td>\n",
       "      <td>14.218700</td>\n",
       "      <td>252.485000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После одной эпохи лосс такой 3.820700\t3.739524 -> ROC AUC 0.7826694435255079\n",
    "\n",
    "Потом немножко поломал, потом ещё эпоха: 3.724600\t3.729579 -> 0.7949317627038786\n",
    "\n",
    "Ещё несколько эпох, в финале 3.52 / 3.728 -> 0.798 (на максимуме 0.7750)\n",
    "\n",
    "Потом оказалось, что я вместо toxic пишу dirty, и после подгона токена roc auc сразу вырос до 0.855 / 0.835"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('models/gpt2-cond')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate as classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_toxicity(text, model=model, pos_text=' clean ', neg_text=' toxic ', raw=False):\n",
    "    scores = []\n",
    "    for prefix in [pos_text, neg_text]:\n",
    "        x = tokenizer(prefix + text, return_tensors='pt', truncation=True)['input_ids'].to(model.device)\n",
    "        with torch.no_grad():\n",
    "            r = model(x, labels=x)\n",
    "        scores.append(r.loss.item())\n",
    "    if raw:\n",
    "        return scores\n",
    "    return scores[0] - scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.10336971282958984\n",
      "-0.04397249221801758\n",
      "0.3848733901977539\n",
      "0.3750596046447754\n"
     ]
    }
   ],
   "source": [
    "print(get_text_toxicity('The internal policy of Trump is not quite adequate.'))\n",
    "print(get_text_toxicity('The internal policy of Trump is not right.'))\n",
    "print(get_text_toxicity('The internal policy of Trump is stupid.'))\n",
    "print(get_text_toxicity('The internal policy of the fucking Trump is stupid.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25143b5dd3d04c6c8aeaa880975f2ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3590.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for text in tqdm(df_test.comment_text):\n",
    "    scores.append(get_text_toxicity(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8550687385341795\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(df_test.label, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "def max_text_toxicity(text, raw=False, **kwargs):\n",
    "    toxicities = [get_text_toxicity(s, **kwargs) for s in sent_tokenize(text)]\n",
    "    if raw:\n",
    "        return toxicities\n",
    "    return max(toxicities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7508ded40f634d9c9a97cf8554c2e4b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3590.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.8352543281906307\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for text in tqdm(df_test.comment_text):\n",
    "    scores.append(max_text_toxicity(text))\n",
    "print(roc_auc_score(df_test.label, scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate as tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from importlib import reload\n",
    "\n",
    "def add_sys_path(p):\n",
    "    p = os.path.abspath(p)\n",
    "    if p not in os.sys.path:\n",
    "        sys.path.append(p)\n",
    "\n",
    "add_sys_path('../../semeval2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((690, 2), (7939, 2), (2000, 1))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial = pd.read_csv(path + 'tsd_trial.csv')\n",
    "train = pd.read_csv(path + 'tsd_train.csv')\n",
    "final_test = pd.read_csv(path + 'tsd_test.csv')\n",
    "\n",
    "train['spans'] = train.spans.apply(literal_eval)\n",
    "trial['spans'] = trial.spans.apply(literal_eval)\n",
    "trial.shape, train.shape, final_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spans_utils\n",
    "reload(spans_utils)\n",
    "from spans_utils import display_spans, spans2labels, labels2spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babccc4f7a424a3d9762ec4abf41429f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=690.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "61 / 690\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for row in tqdm([row for i, row in trial.iterrows()]):\n",
    "    labels = spans2labels(row.text, row.spans, tokenizer, bos=False, left_space=False)\n",
    "    spans2 = labels2spans(row.text, labels, tokenizer, space='Ġ',  bos=False, left_space=False)\n",
    "    if row.spans != spans2:\n",
    "        t = row.text.replace(' ', '+')\n",
    "        #display_spans(row.spans, t)\n",
    "        #display_spans(spans2, t)\n",
    "        n += 1\n",
    "        #print(row.name, n)\n",
    "print(n, '/', trial.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_toxicity(text, model=model, tokenizer=tokenizer, pos_text=' clean ', neg_text=' toxic ', raw=False):\n",
    "    scores = []\n",
    "    for prefix in [pos_text, neg_text]:\n",
    "        x = tokenizer(prefix + text, return_tensors='pt')['input_ids'].to(model.device).flatten()\n",
    "        with torch.no_grad():\n",
    "            r = model(x)\n",
    "        ce = torch.nn.functional.cross_entropy(r.logits, x, reduction='none').cpu().numpy()\n",
    "        scores.append(ce)\n",
    "    if raw:\n",
    "        return scores\n",
    "    return scores[0] - scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer(' toxic ' + text, return_tensors='pt')['input_ids'].to(model.device).flatten()\n",
    "with torch.no_grad():\n",
    "    r = model(x)\n",
    "ce = torch.nn.functional.cross_entropy(r.logits, x, reduction='none').cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3424]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3424,  314, 1101,  407, 1654,  644,  345, 1612,  416,  366, 1169, 1364,\n",
       "         1911,  220,  314,  892,  345, 1612,  262, 1364]], device='cuda:4')"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(tokenizer.encode(' clean', return_tensors='pt').to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " clean There will always be those in the business world that would not accept a sales tax because most Al\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model.generate(\n",
    "    tokenizer.encode(' clean', return_tensors='pt').to(model.device),\n",
    "    do_sample=True, \n",
    "    temperature=1,\n",
    ")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.67003345,  0.3655281 , -0.19273281,  0.32772732,  0.45638418,\n",
       "       -0.3000698 , -0.1049881 ,  0.67400837,  2.5905404 ,  0.8584938 ,\n",
       "        1.4604149 , -0.21906662], dtype=float32)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Never think you man are a bleeding stupid tall idiot.'\n",
    "tt = get_tokens_toxicity(text)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (tt>1).astype(int).tolist()[1:] + [0]\n",
    "spans2 = labels2spans(text, labels, tokenizer, space='Ġ',  bos=False, left_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Never+think+you+man+are+a+bleeding+<b>stupid</b>+tall+<b>idiot</b>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_spans(spans2, text.replace(' ', '+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e22eadd0604a83b21512d521fc6c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=690.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "toxicities = [get_tokens_toxicity(text) for text in tqdm(trial.text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series([tt for t in toxicities for tt in t[1:]]).hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semeval2021 import f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат: мы получаем чуть более высокое качество, чем рандом, но существенно ниже всех нормальных бейзлайнов: 22% вместо 70%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2 \t 0.19312386083147567\n",
      "-1 \t 0.1922833064435526\n",
      "-0.5 \t 0.1917400004375064\n",
      "0 \t 0.18883939442793682\n",
      "0.25 \t 0.2130155080781475\n",
      "0.3 \t 0.21697875359144275\n",
      "0.4 \t 0.2183846590943954\n",
      "0.45 \t 0.22148712444404112\n",
      "0.5 \t 0.21827046418759644\n",
      "0.75 \t 0.20960342261525283\n",
      "1 \t 0.19962231627375884\n",
      "2 \t 0.0887408311526229\n"
     ]
    }
   ],
   "source": [
    "for threshold in [-2, -1, -0.5, 0, 0.25, 0.3, 0.4, 0.45, 0.5, 0.75, 1, 2]:\n",
    "    preds = []\n",
    "    for text, tt in zip(trial.text, toxicities):\n",
    "        labels = (tt>threshold).astype(int).tolist()[1:] + [0]\n",
    "        preds.append(labels2spans(text, labels, tokenizer))\n",
    "    print(threshold, '\\t', np.mean([f1(p, y) for p, y in zip(preds, trial.spans)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_token_score(spans, text):\n",
    "    # todo: use style=\"background-color: #Oxffffff\"\n",
    "    result = []\n",
    "    spans = set(spans)\n",
    "    toxic, prev_toxic = False, False\n",
    "    for i, c in enumerate(text):\n",
    "        if i in spans:\n",
    "            toxic = True\n",
    "            if not prev_toxic:\n",
    "                result.append('<b>')\n",
    "        else:\n",
    "            toxic = False\n",
    "            if prev_toxic:\n",
    "                result.append('</b>')\n",
    "        result.append(c)\n",
    "        prev_toxic = toxic\n",
    "    try:\n",
    "        display(HTML(''.join(result)))\n",
    "    except InvalidURL:\n",
    "        print(''.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tt[1:].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "def display_token_scores(tokens, scores, b=None, cmap=None):\n",
    "    if cmap is None:\n",
    "        cmap = matplotlib.cm.get_cmap('bwr')\n",
    "    if b is None:\n",
    "        b = [0] * len(tokens)\n",
    "\n",
    "    spans  = []\n",
    "    for token, score, boldness in zip(tokens, scores, b):\n",
    "        text = token.replace(\"Ġ\", \" \")\n",
    "        if boldness:\n",
    "            text = '<b>{}</b>'.format(text)\n",
    "        spans.append(\n",
    "            '<span style=\"background-color: {}\">{}</span>'.format(\n",
    "                \"#{0:02x}{1:02x}{2:02x}\".format(*cmap(score, bytes=True)[:3]),\n",
    "                text\n",
    "            )\n",
    "        )\n",
    "\n",
    "    display(HTML(''.join(spans)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: #9e9eff\">Be</span><span style=\"background-color: #c6c6ff\">ijing</span><span style=\"background-color: #ececff\"> millionaires</span><span style=\"background-color: #e6e6ff\"> and</span><span style=\"background-color: #ffeeee\"> wanna</span><span style=\"background-color: #e2e2ff\"> be</span><span style=\"background-color: #d8d8ff\"> Beijing</span><span style=\"background-color: #8c8cff\"> million</span><span style=\"background-color: #c2c2ff\">ai</span><span style=\"background-color: #e6e6ff\">resses</span><span style=\"background-color: #fcfcff\"> who</span><span style=\"background-color: #e6e6ff\"> squeeze</span><span style=\"background-color: #f3f3ff\"> money</span><span style=\"background-color: #ffeeee\"> from</span><span style=\"background-color: #c3c3ff\"> the</span><span style=\"background-color: #f6f6ff\"> rest</span><span style=\"background-color: #f6f6ff\"> of</span><span style=\"background-color: #fffefe\"> us</span><span style=\"background-color: #fafaff\">!</span><span style=\"background-color: #ff3939\"><b> And</b></span><span style=\"background-color: #ffc3c3\"><b> stupid</b></span><span style=\"background-color: #ff9a9a\"><b> blind</b></span><span style=\"background-color: #ffe4e4\"><b> white</b></span><span style=\"background-color: #fffefe\"> men</span><span style=\"background-color: #ffd2d2\"> with</span><span style=\"background-color: #f6f6ff\"> blind</span><span style=\"background-color: #ececff\">ers</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = trial.sample(1).iloc[0]\n",
    "text = row.text\n",
    "tt = get_tokens_toxicity(text)\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "scores = (1 / (1 + np.exp(-tt[1:]))).tolist()\n",
    "gt_spans = spans2labels(text=text, spans=row.spans, tokenizer=tokenizer)[1:]\n",
    "\n",
    "display_token_scores(tokens, scores, b=gt_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: #f2f2ff\">The</span><span style=\"background-color: #dedeff\"> media</span><span style=\"background-color: #ffbebe\"> really</span><span style=\"background-color: #fcfcff\"> should</span><span style=\"background-color: #f6f6ff\"> stop</span><span style=\"background-color: #f3f3ff\"> giving</span><span style=\"background-color: #b0b0ff\"> attention</span><span style=\"background-color: #fff4f4\"> to</span><span style=\"background-color: #e6e6ff\"> this</span><span style=\"background-color: #f8f8ff\"><b> lun</b></span><span style=\"background-color: #fff4f4\"><b>atic</b></span><span style=\"background-color: #fefeff\"><b>,</b></span><span style=\"background-color: #fffcfc\"> self</span><span style=\"background-color: #ffe2e2\">-</span><span style=\"background-color: #d0d0ff\">serving</span><span style=\"background-color: #ffc6c6\"> criminal</span><span style=\"background-color: #c8c8ff\"><b> sc</b></span><span style=\"background-color: #ff8c8c\"><b>umb</b></span><span style=\"background-color: #fff6f6\">ag</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: #ff9696\"><b>H</b></span><span style=\"background-color: #e0e0ff\"><b>ang</b></span><span style=\"background-color: #fff0f0\"><b> this</b></span><span style=\"background-color: #fffefe\"><b> guy</b></span><span style=\"background-color: #fff6f6\"><b> and</b></span><span style=\"background-color: #ffe6e6\"><b> his</b></span><span style=\"background-color: #fff2f2\"><b> pil</b></span><span style=\"background-color: #ffeeee\"><b>au</b></span><span style=\"background-color: #fefeff\"> wife</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: #d6d6ff\">Did</span><span style=\"background-color: #fff0f0\"> none</span><span style=\"background-color: #d0d0ff\"> of</span><span style=\"background-color: #ffecec\"><b> you</b></span><span style=\"background-color: #ff8080\"><b> fools</b></span><span style=\"background-color: #f8f8ff\"> that</span><span style=\"background-color: #fffcfc\"> strategically</span><span style=\"background-color: #ffecec\"> voted</span><span style=\"background-color: #ffd8d8\"> Trudeau</span><span style=\"background-color: #ffc2c2\"> a</span><span style=\"background-color: #d6d6ff\"> majority</span><span style=\"background-color: #d6d6ff\"> government</span><span style=\"background-color: #f0f0ff\"> not</span><span style=\"background-color: #d6d6ff\"> foresee</span><span style=\"background-color: #fff6f6\"> this</span><span style=\"background-color: #ffe0e0\"> kind</span><span style=\"background-color: #eaeaff\"> of</span><span style=\"background-color: #ffb8b8\"> incompetence</span><span style=\"background-color: #fff0f0\"> happening</span><span style=\"background-color: #f3f3ff\">?</span><span style=\"background-color: #fcfcff\"> </span><span style=\"background-color: #d0d0ff\"> Un</span><span style=\"background-color: #ffeeee\">bel</span><span style=\"background-color: #e6e6ff\">iev</span><span style=\"background-color: #ffd6d6\">able</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    row = trial.sample(1).iloc[0]\n",
    "    text = row.text\n",
    "    tt = get_tokens_toxicity(text)\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    scores = (1 / (1 + np.exp(-tt[1:]))).tolist()\n",
    "    gt_spans = spans2labels(text=text, spans=row.spans, tokenizer=tokenizer)[1:]\n",
    "\n",
    "    display_token_scores(tokens, scores, b=gt_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3k",
   "language": "python",
   "name": "p3k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
