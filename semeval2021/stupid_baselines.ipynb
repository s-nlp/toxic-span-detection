{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хочется сделать максимально тупые бейзлайны без нейронок и без учёта контекста слов - чтобы понять, а сколько мы вообще выбили за счёт контекста. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create labels for tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((690, 2), (7939, 2), (2000, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial = pd.read_csv(path + 'tsd_trial.csv')\n",
    "train = pd.read_csv(path + 'tsd_train.csv')\n",
    "# final_test = pd.read_csv(path + 'tsd_test.csv')\n",
    "final_test = pd.read_csv(path + 'tsd_test_gt.csv')\n",
    "\n",
    "train['spans'] = train.spans.apply(literal_eval)\n",
    "trial['spans'] = trial.spans.apply(literal_eval)\n",
    "final_test['spans'] = final_test.spans.apply(literal_eval)\n",
    "trial.shape, train.shape, final_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(set(trial.text).intersection(set(train.text))))\n",
    "print(len(set(final_test.text).intersection(set(train.text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06109081748331024\n",
      "0.06231884057971015\n"
     ]
    }
   ],
   "source": [
    "print((train.spans.apply(len) == 0).mean())\n",
    "print((trial.spans.apply(len) == 0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Because he's a <b>moron</b> and a <b>bigot</b>. It's not any more complicated than that."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Because he's a <b>moron</b> and a <b>bigot</b>. It's not any more complicated than that."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spans_utils\n",
    "from importlib import reload\n",
    "reload(spans_utils)\n",
    "from spans_utils import display_spans, spans2labels, labels2spans\n",
    "\n",
    "display_spans(trial.spans[0], trial.text[0])\n",
    "display_spans(trial.spans[0], trial.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from semeval2021 import f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Словарь (naive bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytextspan in /home/dale/p3/lib/python3.7/site-packages (0.5.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/dale/p3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!source ~/p3/bin/activate; pip install pytextspan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textspan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Another violent and aggressive immigrant killing a innocent and intelligent US Citizen.... Sarcasm'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = train.text[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d18f3558dd245ac984c563f52d0238b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "toxic_vocab = Counter()\n",
    "normal_vocab = Counter()\n",
    "\n",
    "for i, row in tqdm(train.iterrows()):\n",
    "    text = row.text.lower()\n",
    "    toks = wordpunct_tokenize(text)\n",
    "    spans = textspan.get_original_spans(toks, text)  # [[(0, 7)],[(8, 15)], ...]\n",
    "    idxs = set(row.spans)\n",
    "    for word, wordspan in zip(toks, spans):\n",
    "        if idxs.intersection(set(range(*wordspan[0]))):\n",
    "            toxic_vocab[word] += 1\n",
    "        else:\n",
    "            normal_vocab[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4516, 18613)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toxic_vocab), len(normal_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Because he's a moron and a bigot. It's not any more complicated than that.\n",
      "[15, 16, 17, 18, 19, 27, 28, 29, 30, 31]\n"
     ]
    }
   ],
   "source": [
    "text = trial.text[0]\n",
    "print(text)\n",
    "print(trial.spans[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_pos = 0.1\n",
    "alpha_neg = 1\n",
    "threshold = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 16, 17, 18, 19, 27, 28, 29, 30, 31]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "def get_toxic_spans(text, alpha_pos=0.1, alpha_neg=1, threshold=0.3):\n",
    "    toks = wordpunct_tokenize(text)\n",
    "    spans = textspan.get_original_spans(toks, text)\n",
    "    result = []\n",
    "    for word, wordspan in zip(toks, spans):\n",
    "        score_pos = toxic_vocab.get(word, 0) + alpha_pos\n",
    "        score_neg = normal_vocab.get(word, 0) + alpha_neg\n",
    "        score = score_pos / (score_pos + score_neg)\n",
    "        if score >= threshold:\n",
    "            result.extend(range(*wordspan[0]))\n",
    "    return result\n",
    "\n",
    "get_toxic_spans(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.48543603315613565\n",
      "0.3 0.5611794534009353\n",
      "0.5 0.5425581534714119\n",
      "0.7 0.46653109132088505\n",
      "0.9 0.1661587545991842\n"
     ]
    }
   ],
   "source": [
    "for thr in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "    preds = [get_toxic_spans(text, threshold=thr) for text in trial.text]\n",
    "    print(thr,  np.mean([f1(p, y) for p, y in zip(preds, trial.spans)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 0.5439401681820818\n",
      "0.25 0.5526757386674521\n",
      "0.3 0.5611794534009353\n",
      "0.35 0.5589048654175035\n",
      "0.4 0.5556682237445129\n",
      "0.5 0.5425581534714119\n"
     ]
    }
   ],
   "source": [
    "for thr in [0.2, 0.25, 0.3, 0.35, 0.4, 0.5]:\n",
    "    preds = [get_toxic_spans(text, threshold=thr) for text in trial.text]\n",
    "    print(thr,  np.mean([f1(p, y) for p, y in zip(preds, trial.spans)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 \t 0.5540741406627043\n",
      "0.1 \t 0.5540741406627043\n",
      "0.3 \t 0.5540741406627043\n",
      "1 \t 0.5611794534009353\n",
      "3 \t 0.5607638742400142\n",
      "10 \t 0.554278235652248\n"
     ]
    }
   ],
   "source": [
    "for a in [0.01, 0.1, 0.3, 1, 3, 10]:\n",
    "    preds = [get_toxic_spans(text, alpha_pos=a * 0.1, alpha_neg=a * 1) for text in trial.text]\n",
    "    print(a, '\\t',  np.mean([f1(p, y) for p, y in zip(preds, trial.spans)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находка Игоря подтвердилась: тупой vocabulary-based подход даёт 56% точности. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 0.5267249460360243\n",
      "0.25 0.5461798868031679\n",
      "0.3 0.5689286860717168\n",
      "0.35 0.579336301344286\n",
      "0.4 0.5849523360901367\n",
      "0.5 0.5902058076633944\n",
      "0.6 0.5889286415675676\n",
      "0.7 0.5585884191995162\n"
     ]
    }
   ],
   "source": [
    "for thr in [0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6, 0.7, ]:\n",
    "    preds = [get_toxic_spans(text, threshold=thr) for text in final_test.text]\n",
    "    print(thr,  np.mean([f1(p, y) for p, y in zip(preds, final_test.spans)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import FastTextEmbeddings\n",
    "\n",
    "# init embedding\n",
    "embedding = FastTextEmbeddings('/home/dale/models/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.FastTextKeyedVectors at 0x7f2b1ae170d0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft = embedding.precomputed_word_embeddings.wv\n",
    "ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=32_000)\n",
    "def embed(w):\n",
    "    return ft[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FT(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.stack([embed(w) for w in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "weights = []\n",
    "for voc, label in [(toxic_vocab, 1), (normal_vocab, 0)]:\n",
    "    for w, c in voc.items():\n",
    "        weights.append(c)\n",
    "        y.append(label)\n",
    "        X.append(w)\n",
    "np.random.seed(1)\n",
    "perm = np.random.permutation(len(X))\n",
    "X = np.array([X[i] for i in perm])\n",
    "y = np.array([y[i] for i in perm])\n",
    "weights = np.array([weights[i] for i in perm]) ** 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('ft', FT()),\n",
       "                ('logisticregression', LogisticRegression(max_iter=1000))])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(FT(), LogisticRegression(max_iter=1_000))\n",
    "pipe.fit(X, y, logisticregression__sample_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=32_000)\n",
    "def get_word_toxicity(w):\n",
    "    return pipe.predict_proba([w])[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toxic_spans(text, threshold=0.3, model_weight=30.0):\n",
    "    toks = wordpunct_tokenize(text)\n",
    "    spans = textspan.get_original_spans(toks, text)\n",
    "    result = []\n",
    "    for word, wordspan in zip(toks, spans):\n",
    "        mscore = get_word_toxicity(word)\n",
    "        \n",
    "        score_pos = toxic_vocab.get(word, 0) + alpha_pos + mscore * model_weight\n",
    "        score_neg = normal_vocab.get(word, 0) + alpha_neg + (1-mscore) * model_weight\n",
    "        \n",
    "        score = (score_pos ) / (score_pos + score_neg)\n",
    "        \n",
    "        if score >= threshold:\n",
    "            result.extend(range(*wordspan[0]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* вес=корень - 50%\n",
    "подбор весов примеров для обучения\n",
    "* вес=корень счётчика - фигня\n",
    "* вес=счётчик - 58%\n",
    "\n",
    "подбор веса модели \n",
    "* вес модели - 1 (относительно словаря) - 60.38%\n",
    "* вес модели - 3 - 62.17%\n",
    "* вес модели - 10 - 62.31%\n",
    "* вес модели - 30 - 61.64%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.4888441281425792\n",
      "0.2 0.6009287884814687\n",
      "0.3 0.5999739062936595\n",
      "0.4 0.5826932918487115\n",
      "0.5 0.5502984282946477\n",
      "0.6 0.5087527525417305\n",
      "0.7 0.41828741844146045\n",
      "0.8 0.35338536914100604\n",
      "0.9 0.16325601546562668\n"
     ]
    }
   ],
   "source": [
    "for thr in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    preds = [get_toxic_spans(text, threshold=thr) for text in trial.text]\n",
    "    print(thr,  np.mean([f1(p, y) for p, y in zip(preds, trial.spans)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.42374356540593644\n",
      "0.2 0.5787110208319759\n",
      "0.3 0.615733746342461\n",
      "0.4 0.6110432484024746\n",
      "0.5 0.6164045187730383\n",
      "0.6 0.5925740101243221\n",
      "0.7 0.5293337819949625\n",
      "0.8 0.4616237834867994\n",
      "0.9 0.2955879117231959\n"
     ]
    }
   ],
   "source": [
    "for thr in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    preds = [get_toxic_spans(text, threshold=thr) for text in final_test.text]\n",
    "    print(thr,  np.mean([f1(p, y) for p, y in zip(preds, final_test.spans)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нейросетка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for voc, label in [(toxic_vocab, 1), (normal_vocab, 0)]:\n",
    "    for w, c in voc.items():\n",
    "        for _ in range(c):\n",
    "            y.append(label)\n",
    "            X.append(w)\n",
    "np.random.seed(1)\n",
    "perm = np.random.permutation(len(X))\n",
    "X = np.array([X[i] for i in perm])\n",
    "y = np.array([y[i] for i in perm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342965"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.22402599\n",
      "Validation score: 0.937781\n",
      "Iteration 2, loss = 0.21325462\n",
      "Validation score: 0.939588\n",
      "Iteration 3, loss = 0.21155067\n",
      "Validation score: 0.938655\n",
      "Iteration 4, loss = 0.21047662\n",
      "Validation score: 0.938597\n",
      "Iteration 5, loss = 0.20965603\n",
      "Validation score: 0.940288\n",
      "Iteration 6, loss = 0.20881203\n",
      "Validation score: 0.939997\n",
      "Iteration 7, loss = 0.20814020\n",
      "Validation score: 0.938480\n",
      "Iteration 8, loss = 0.20757158\n",
      "Validation score: 0.939530\n",
      "Iteration 9, loss = 0.20707349\n",
      "Validation score: 0.939647\n",
      "Iteration 10, loss = 0.20651087\n",
      "Validation score: 0.939588\n",
      "Iteration 11, loss = 0.20606586\n",
      "Validation score: 0.939122\n",
      "Iteration 12, loss = 0.20562595\n",
      "Validation score: 0.940055\n",
      "Iteration 13, loss = 0.20521489\n",
      "Validation score: 0.938947\n",
      "Iteration 14, loss = 0.20480584\n",
      "Validation score: 0.939647\n",
      "Iteration 15, loss = 0.20452820\n",
      "Validation score: 0.939647\n",
      "Iteration 16, loss = 0.20403018\n",
      "Validation score: 0.939530\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('ft', FT()),\n",
       "                ('mlpclassifier',\n",
       "                 MLPClassifier(early_stopping=True, hidden_layer_sizes=(300,),\n",
       "                               validation_fraction=0.05, verbose=1))])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "pipe = make_pipeline(FT(), MLPClassifier(hidden_layer_sizes=(300,), activation='relu', verbose=1, early_stopping=True, validation_fraction=0.05))\n",
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=32_000)\n",
    "def get_word_toxicity(w):\n",
    "    return pipe.predict_proba([w])[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toxic_spans(text, threshold=0.3, model_weight=3.0):\n",
    "    toks = wordpunct_tokenize(text)\n",
    "    spans = textspan.get_original_spans(toks, text)\n",
    "    result = []\n",
    "    prev = False\n",
    "    prev_end = 0\n",
    "    for word, wordspan in zip(toks, spans):\n",
    "        mscore = get_word_toxicity(word)\n",
    "        \n",
    "        score_pos = toxic_vocab.get(word, 0) + alpha_pos + mscore * model_weight\n",
    "        score_neg = normal_vocab.get(word, 0) + alpha_neg + (1-mscore) * model_weight\n",
    "        \n",
    "        score = (score_pos ) / (score_pos + score_neg)\n",
    "        \n",
    "        toxic = score >= threshold\n",
    "        if toxic:\n",
    "            if prev:\n",
    "                result.extend(range(prev_end, wordspan[0][0]))\n",
    "            result.extend(range(*wordspan[0]))\n",
    "        \n",
    "        if len(word) > 0:\n",
    "            prev = toxic\n",
    "            prev_end = wordspan[0][1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подбор веса модели (против веса словаря):\n",
    "* вес модели - 1 - 0.6322\n",
    "* вес модели - 3 - 0.6398 (выбрал его)\n",
    "* вес модели - 10 - 0.6378\n",
    "* вес модели - 30 - 0.6360\n",
    "\n",
    "Другие параметры:\n",
    "* сделал токсичными пропуски между токсичными словами - 0.6397 (не помогает)\n",
    "* увеличил размер скрытого слоя с 30 до 100 - и выбил уже 0.6454\n",
    "* увеличил размер скрытого слоя до 300 - вроде особо пользы нет, 0.6443\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.4957608668206724\n",
      "0.2 0.6083819087224832\n",
      "0.3 0.625489948993125\n",
      "0.4 0.6166048481994797\n",
      "0.5 0.5879459188133708\n",
      "0.6 0.5457713234165323\n",
      "0.7 0.48764345238387613\n",
      "0.8 0.3575961534803883\n",
      "0.9 0.1661587545991842\n"
     ]
    }
   ],
   "source": [
    "for thr in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    preds = [get_toxic_spans(text, threshold=thr) for text in trial.text]\n",
    "    print(thr,  np.mean([f1(p, y) for p, y in zip(preds, trial.spans)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.42538572416947895\n",
      "0.2 0.5775798901676763\n",
      "0.3 0.6262081107598961\n",
      "0.4 0.6443802662357849\n",
      "0.5 0.6435602124499669\n",
      "0.6 0.6261727952332393\n",
      "0.7 0.5775737073659616\n",
      "0.8 0.4772399665569934\n",
      "0.9 0.3052372126999968\n"
     ]
    }
   ],
   "source": [
    "for thr in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    preds = [get_toxic_spans(text, threshold=thr) for text in final_test.text]\n",
    "    print(thr,  np.mean([f1(p, y) for p, y in zip(preds, final_test.spans)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем, я выбил 64.5% на тесте, пользуясь ТОЛЬКО отдельно взятыми словами, и оценивая их ансамблем из простого счётчика и MLP над fasttext-эмбеддингом. \n",
    "\n",
    "ВООБЩЕ САМАЯ НАИЛУЧШАЯ модель во всей дорожке выбила 70.8%, наилучшая из наших - 68.6% (уже после дедлайна). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3k",
   "language": "python",
   "name": "p3k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
